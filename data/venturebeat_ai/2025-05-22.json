[
  {
    "id": "deda7f9ce10b3f1f00f7f2f708878576",
    "title": "OpenAI updates its new Responses API rapidly with MCP support, GPT-4o native image gen, and more enterprise features",
    "url": "https://venturebeat.com/programming-development/openai-updates-its-new-responses-api-rapidly-with-mcp-support-gpt-4o-native-image-gen-and-more-enterprise-features/",
    "authors": "Carl Franzen",
    "published_date": "2025-05-21T15:30:22+00:00",
    "source": "VentureBeat AI",
    "summary": "OpenAI 迅速更新了新的 Responses API，加入了 MCP 支援、GPT-4o 原生圖像生成等功能，並提供更多企業級功能。這些更新讓開發者和企業更容易建立智能應用程式，讓應用更具行動性。Responses API 自去年三月推出以來，已處理了數以兆計的 tokens，支援各種應用案例，從市場研究到教育和金融分析。企業可以整合 OpenAI 的技術，如 ChatGPT，到他們的產品和服務中。",
    "content": "OpenAI updates its new Responses API rapidly with MCP support, GPT-4o native image gen, and more enterprise features | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nOpenAI updates its new Responses API rapidly with MCP support, GPT-4o native image gen, and more enterprise features\nCarl Franzen\n@carlfranzen\nMay 21, 2025 8:30 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with ChatGPT\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nOpenAI is\nrolling out a set of significant updates\nto its\nnewish Responses API\n, aiming to make it easier for developers and enterprises to build intelligent, action-oriented agentic applications.\nThese enhancements include support for remote\nModel Context Protocol (MCP) servers\n, integration of image generation and Code Interpreter tools, and upgrades to file search capabilities—all available as of today, May 21.\nFirst\nlaunched in March 2025\n, the Responses API serves as OpenAI’s toolbox for third-party developers to build agentic applications atop some of the core functionalities of its hit services ChatGPT and its first-party AI agents\nDeep Research\nand\nOperator\n.\nIn the months since its debut, it has processed trillions of tokens and supported a broad range of use cases, from market research and education to software development and financial analysis.\nPopular applications built with the API include Zencoder’s coding agent, Revi’s market intelligence assistant, and MagicSchool’s educational platform.\nThe basis and purpose of the Responses API\nThe Responses API debuted alongside OpenAI’s open-source Agents SDK in March 2025, as part of an initiative to provide third-party developer access to the same technologies powering OpenAI’s own AI agents like Deep Research and Operator.\nThis way, startups and companies outside of OpenAI could integrate the same tech as it offers through ChatGPT into their own products and services, be they internal for employee usage or external for customers and partners.\nInitially, the API combined elements from Chat Completions and the Assistants API—delivering built-in tools for web and file search, as well as computer use—enabling developers to build autonomous workflows without complex orchestration logic. OpenAI said at that time that the Chat Completions API would be deprecated by mid 2026.\nThe Responses API provides visibility into model decisions, access to real-time data, and integration capabilities that allowed agents to retrieve, reason over, and act on information.\nThis launch marked a shift toward giving developers a unified toolkit for creating production-ready, domain-specific AI agents with minimal friction.\nRemote MCP server support broadens integration potential\nA key addition in this update is support for remote MCP servers. Developers can now connect OpenAI’s models to external tools and services such as Stripe, Shopify, and Twilio using only a few lines of code. This capability enables the creation of agents that can take actions and interact with systems users already depend on. To support this evolving ecosystem, OpenAI has joined the MCP steering committee.\nAccess to new tools: native image gen and Code Interpreter\nThe update brings new built-in tools to the Responses API that enhance what agents can do within a single API call.\nA variant of OpenAI’s hit GPT-4o native image generation model — which inspired a wave of “Studio Ghibli” style anime memes around the web and buckled OpenAI’s servers with its popularity, but can obviously create many other image styles — is now available through the API under the model name “gpt-image-1.” It includes potentially helpful and fairly impressive new features like real-time streaming previews and multi-turn refinement.\nThis enables developers to build applications that can produce and edit images dynamically in response to user input.\nAdditionally, the Code Interpreter tool is now integrated into the Responses API, allowing models to handle data analysis, complex math, and logic-based tasks within their reasoning processes.\nThe tool helps improve model performance across various technical benchmarks and allows for more sophisticated agent behavior.\nImproved file search and context handling\nThe file search functionality has also been upgraded. Developers can now perform searches across multiple vector stores and apply attribute-based filtering to retrieve only the most relevant content.\nThis improves the precision of information agents use, enhancing their ability to answer complex questions and operate within large knowledge domains.\nNew enterprises reliability, transparency features\nSeveral features are designed specifically to meet enterprise needs. Background mode allows for long-running asynchronous tasks, addressing issues of timeouts or network interruptions during intensive reasoning.\nReasoning summaries, a new addition, offer natural-language explanations of the model’s internal thought process, helping with debugging and transparency.\nEncrypted reasoning items provide an additional privacy layer for Zero Data Retention customers.\nThese allow models to reuse previous reasoning steps without storing any data on OpenAI servers, improving both security and efficiency.\nThe latest capabilities are supported across OpenAI’s GPT-4o series, GPT-4.1 series, and the o-series models, including o3 and o4-mini. These models now maintain reasoning state across multiple tool calls and requests, which leads to more accurate responses at lower cost and latency.\nYesterday’s price IS today’s price!\nDespite the expanded feature set, OpenAI has confirmed that pricing for the new tools and capabilities within the Responses API will remain consistent with existing rates.\nFor example, the Code Interpreter tool is priced at $0.03 per session, and file search usage is billed at $2.50 per 1,000 calls, with storage costs of $0.10 per GB per day after the first free gigabyte.\nWeb search pricing varies based on the model and search context size, ranging from $25 to $50 per 1,000 calls. Image generation through the gpt-image-1 tool is also charged according to resolution and quality tier, starting at $0.011 per image.\nAll tool usage is billed at the chosen model’s per-token rates, with no additional markup for the newly added capabilities.\nWhat’s next for the Responses API?\nWith these updates, OpenAI continues to expand what is possible with the Responses API. Developers gain access to a richer set of tools and enterprise-ready features, while enterprises can now build more integrated, capable, and secure AI-driven applications.\nAll features are live as of May 21, with pricing and implementation details available through OpenAI’s documentation.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Series Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Series. See if we're coming to your area!\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-05-22T04:36:45.062492",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-22T04:37:51.200488",
    "audio_file": "deda7f9ce10b3f1f00f7f2f708878576.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/backend/data/audio/deda7f9ce10b3f1f00f7f2f708878576.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-22T04:38:11.791857"
  },
  {
    "id": "374c509f2f7310e79270e7c014ec167c",
    "title": "Mistral AI launches Devstral, powerful new open source SWE agent model that runs on laptops",
    "url": "https://venturebeat.com/ai/mistral-ai-launches-devstral-powerful-new-open-source-swe-agent-model-that-runs-on-laptops/",
    "authors": "Carl Franzen",
    "published_date": "2025-05-21T14:57:15+00:00",
    "source": "VentureBeat AI",
    "summary": "Mistral AI推出全新的開源SWE代理模型Devstral，可在筆記型電腦上運行，專為軟體工程開發而設計。這個模型相較於其他競爭對手的模型更小，只有2400萬個參數，因此需要更少的運算能力，適合在筆記型電腦上運行。Devstral不同於傳統的模型，能夠全面理解程式碼、導覽大型程式庫並解決實際問題。這個模型現在以寬鬆的Apache 2.0許可證免費提供給開發者和組織使用。",
    "content": "Mistral AI launches Devstral, powerful new open source SWE agent model that runs on laptops | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nExclusive\nMistral AI launches Devstral, powerful new open source SWE agent model that runs on laptops\nCarl Franzen\n@carlfranzen\nMay 21, 2025 7:57 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nWell-funded French AI model maker\nMistral\nhas consistently punched above its weight since its debut of its\nown powerful open source foundation model in fall 2023\n— but it took some criticism among developers on X recently for its last release of a proprietary large language model (LLM) called\nMedium 3\n, which some viewed as betraying its open source roots and commitment.\n(Recall that open source models can be taken and adapted freely by anyone, while proprietary models must be paid for and their customization options are more limited and controlled by the model maker.)\nBut today, Mistral is back and recommitting to the open source AI community, and AI-powered software development in particular, in a big way. The company has teamed up with open source startup\nAll Hands AI\n, creators of Open Devin to release\nDevstral\n, a new open-source language model with 24-million parameters — much smaller than many rivals whose models are in the multibillions, and thus, requiring far less computing power such that it can be run on a laptop — purpose-built for agentic AI development.\nUnlike traditional LLMs designed for short-form code completions or isolated function generation, Devstral is optimized to act as a full software engineering agent—capable of understanding context across files, navigating large codebases, and resolving real-world issues.\nThe model is now freely available under the\npermissive Apache 2.0 license\n, allowing developers and organizations to deploy, modify, and commercialize it without restriction.\n“We wanted to release something open for the developer and enthusiast community—something they can run locally, privately, and modify as they want,” said Baptiste Rozière, research scientist at Mistral AI. “It’s released under Apache 2.0, so people can do basically whatever they want with it.”\nBuilding upon Codestral\nDevstral represents the next step in Mistral’s growing portfolio of code-focused models, following its earlier success with the Codestral series.\nFirst launched in\nMay 2024, Codestral\nwas Mistral’s initial foray into specialized coding LLMs. It was a 22-billion-parameter model trained to handle over 80 programming languages and became well-regarded for its performance in code generation and completion tasks.\nThe model’s popularity and technical strengths led to rapid iterations, including the launch of Codestral-Mamba—an enhanced version built on Mamba architecture—and most recently, Codestral 25.01, which has found adoption among IDE plugin developers and enterprise users looking for high-frequency, low-latency models.\nThe momentum around Codestral helped establish Mistral as a key player in the coding-model ecosystem and laid the foundation for the development of Devstral—extending from fast completions to full-agent task execution.\nOutperforms larger models on top SWE benchmarks\nDevstral achieves a score of 46.8% on the SWE-Bench Verified benchmark, a dataset of 500 real-world GitHub issues manually validated for correctness.\nThis places it ahead of all previously released open-source models and ahead of several closed models, including GPT-4.1-mini, which it surpasses by over 20 percentage points.\n“Right now, it’s by pretty far the best open model for SWE-bench verified and for code agents,” said Rozière. “And it’s also a very small model—only 24 billion parameters—that you can run locally, even on a MacBook.”\n“Compare Devstral to closed and open models evaluated under any scaffold—we find that Devstral achieves substantially better performance than a number of closed-source alternatives,” wrote Sophia Yang, Ph.D., Head of Developer Relations at Mistral AI, on\nthe social network X\n. “For example, Devstral surpasses the recent GPT-4.1-mini by over 20%.”\nThe model is finetuned from Mistral Small 3.1 using reinforcement learning and safety alignment techniques.\n“We started from a very good base model with Mistral’s small tree control, which already performs well,” Rozière said. “Then we specialized it using safety and reinforcement learning techniques to improve its performance on SWE-bench.”\nBuilt for the agentic era\nDevstral is not just a code generation model — it is optimized for integration into agentic frameworks like OpenHands, SWE-Agent, and OpenDevin.\nThese scaffolds allow Devstral to interact with test cases, navigate source files, and execute multi-step tasks across projects.\n“We’re releasing it with OpenDevin, which is a scaffolding for code agents,” said Rozière. “We build the model, and they build the scaffolding — a set of prompts and tools that the model can use, like a backend for the developer model.”\nTo ensure robustness, the model was tested across diverse repositories and internal workflows.\n“We were very careful not to overfit to SWE-bench,” Rozière explained. “We trained only on data from repositories that are not cloned from the SWE-bench set and validated the model across different frameworks.”\nHe added that Mistral dogfooded Devstral internally to ensure it generalizes well to new, unseen tasks.\nEfficient deployment with permissive open license — even for enterprise and commercial projects\nDevstral’s compact 24B architecture makes it practical for developers to run locally, whether on a single RTX 4090 GPU or a Mac with 32GB of RAM. This makes it appealing for privacy-sensitive use cases and edge deployments.\n“This model is targeted toward enthusiasts and people who care about running something locally and privately—something they can use even on a plane with no internet,” Rozière said.\nBeyond performance and portability, its Apache 2.0 license offers a compelling proposition for commercial applications. The license permits unrestricted use, adaptation, and distribution—even for proprietary products—making Devstral a low-friction option for enterprise adoption.\nDetailed specifications and usage instructions are available on the\nDevstral-Small-2505 model card on Hugging Face\n.\nThe model features a 128,000 token context window and uses the Tekken tokenizer with a 131,000\nvocabulary.\nIt supports deployment through all major open source platforms including Hugging Face, Ollama, Kaggle, LM Studio, and Unsloth, and works well with libraries such as vLLM, Transformers, and Mistral Inference.\nAvailable via API or locally\nDevstral is accessible via\nMistral’s Le Platforme API\n(application programming interface) under the model name devstral-small-2505, with pricing set at $0.10 per million input tokens and $0.30 per million output tokens.\nFor those deploying locally, support for frameworks like OpenHands enables integration with codebases and agentic workflows out of the box.\nRozière shared how he incorporates Devstral in his own development flow: “I use it myself. You can ask it to do small tasks, like updating the version of a package or modifying a tokenization script. It finds the right place in your code and makes the changes. It’s really nice to use.”\nMore to come\nWhile Devstral is currently released as a research preview, Mistral and All Hands AI are already working on a larger follow-up model with expanded capabilities. “There will always be a gap between smaller and larger models,” Rozière noted, “but we’ve gone a long way in bridging that. These models already perform very strongly, even compared to some larger competitors.”\nWith its performance benchmarks, permissive license, and agentic design, Devstral positions itself not just as a code generation tool—but as a foundational model for building autonomous software engineering systems.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Series Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Series. See if we're coming to your area!\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-05-22T04:36:45.324969",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-22T04:37:55.948926",
    "audio_file": "374c509f2f7310e79270e7c014ec167c.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/backend/data/audio/374c509f2f7310e79270e7c014ec167c.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-22T04:38:12.962435"
  },
  {
    "id": "eb3a91ddb2687cc1d6185d1ce361cb9f",
    "title": "AMD unveils new Threadripper CPUs and Radeon GPUs for gamers at Computex 2025",
    "url": "https://venturebeat.com/games/amd-unveils-new-threadripper-cpus-and-radeon-gpus-for-gamers-at-computex-2025/",
    "authors": "Dean Takahashi",
    "published_date": "2025-05-21T03:00:00+00:00",
    "source": "VentureBeat AI",
    "summary": "AMD在2025年的Computex展示了新款Threadripper CPU和Radeon GPU，針對遊戲玩家推出。這些新硬體提供更高效能和功能，適用於遊戲、內容創作、專業產業和AI開發等領域。其中，Radeon RX 9060 XT GPU具備新功能，提升遊戲表現。同時，AMD也加強與ASUS的合作，推出搭載AI加速功能的商用PC。這些產品將帶來更快速、更高效的AI增強生產力，並提供企業級安全和管理功能。",
    "content": "AMD unveils new Threadripper CPUs and Radeon GPUs for gamers at Computex 2025 | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nAMD unveils new Threadripper CPUs and Radeon GPUs for gamers at Computex 2025\nDean Takahashi\n@deantak\nMay 20, 2025 8:00 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nAMD Radeon is getting an upgrade at Computex 2025.\nImage Credit: AMD\nDuring Computex 2025,\nAdvanced Micro Devices\nheld a press event to introduce its Radeon RX 9060 XT graphics cards and Threadripper CPUs for next-gen gaming.\nWith up to 16 GB GDDR6 memory, the new GPUs unlock new levels of performance while delivering a suite of new and enhanced features for next-gen gaming, the company said.\nAMD said it is extending its leadership in high-performance computing by unveiling new graphics and workstation products engineered to address the toughest workloads in gaming, content creation, professional industries, and AI development.\nThe company unveiled the AMD Ryzen Threadripper PRO 9000 WX‑Series and AMD Ryzen Threadripper 9000 Series processors built on “Zen 5” architecture—led by the Ryzen Threadripper PRO 9995WX with 96 cores / 192 threads—deliver unmatched multi‑threaded performance, leadership efficiency, and enterprise‑grade AMD PRO Technologies, empowering professionals to bring complex visions to life faster.\nThe Radeon AI PRO R9700 is built for AI-powered workstations. It delivers up to 4X more throughput than the previous generation, as well as expanded AMD ROCm on Radeon support, bringing high‑performance GPU acceleration to a broader range of AI and compute workloads for advanced AI development.AMD also strengthens its partnership with ASUS and will introduce the new ASUS Expert P Series Copilot+ PCs, the next-generation commercial PCs bringing AI acceleration to the enterprise.\nThe PCs are powered by up to AMD Ryzen AI PRO 300 Series processors and features AMD PRO Technologies, offering 50+ TOPS of NPU performance for faster and more efficient AI-enhanced productivity as well as enterprise-grade security and manageability for the modern IT environment.\nGaming: Radeon RX 9060 XT\nAMD Radeon RX 9060 XT\nAMD said the Radeon RX 9060 XT GPUs unlock new levels of performance while delivering a suite of new and enhanced features for next-gen gaming.The Radeon RX 9060 XT features 32 AMD RDNA 4 compute\nunits and doubles raytracing throughput compared to the previous generation. With up to 16GB of GDDR6 memory, these GPUs allow gamers to render the most exciting games of today and tomorrow at max settings.Additionally, the Radeon RX 9060 XT supports FP8 data types and structured sparsity, making it ready for the next-generation of AI-assisted gameplay, creative tools, and generative\nexperiences.\nThe Radeon RX 9060 XT 16GB variant makes a great upgrade for gamers looking to future-proof their systems with a suite of next-gen features that will keep their experiences feeling fresh for years to come.\nThreadrippers for workstations: Ryzen Threadripper PRO 9000 WX-Series and Ryzen Threadripper 9000 Series\nThe AMD Ryzen Threadripper PRO 9000 WX-Series and AMD Ryzen Threadripper 9000 Series processors set new standards for high-end workstations and enthusiast desktops and empower professionals to bring their complex visions to life faster than ever.Built on the advanced “Zen 5” architecture, both processor families deliver unmatched multi-threaded performance, leading energy efficiency, and extensive platform capabilities, including expansive memory bandwidth.\nAt the top of stack, the AMD Ryzen Threadripper PRO 9995WX offers 96 cores and 192 threads, providing extraordinary compute capacity for the most demanding AEC, M&E, and AI workloads. Every AMD Ryzen Threadripper PRO 9000 WX-Series processor comes equipped with AMD PRO Technologies, offering a robust suite of enterprise grade features including multilayered security, advanced remote manageability,\nand long-term platform stability—helping professional users and IT teams achieve new levels of productivity.\nFor enthusiasts, the AMD Ryzen Threadripper 9980X comes equipped with 64 cores and 128 threads, offering DIY customers maximum performance for the most intensive workloads including content creation, software compiling, and local AI training.\nRadeon AI PRO R9700\nThe AMD Radeon AI PRO R9700 brings next-generation on-device AI horsepower to professional workstations, pairing second-generation RDNA 4 AI accelerators with a massive 32 GB of graphics memory and PCIe Gen 5 throughput to speed local inference, model finetuning, and complex creative\nworkloads.\nWith up to 4X higher AI-accelerator throughput than the previous generation and full ROCm support on Linux (Windows support coming soon), the R9700 delivers high-performance AI with the control and data privacy of on-prem deployment.Engineered for scalability, Radeon AI PRO R9700 excels in multi-GPU configurations—expanding memory and compute capacity for large-language model development, real-time rendering, and parallel simulations.\nAI PC\nAMD executive Jack Huynh was joined by S.Y. Hsu, Co-CEO of ASUS, for the on-stage announcement of the new ASUS Expert P series lineup of PCs powered by up to AMD Ryzen AI PRO 300 Series processors.\nWith industry leading 50 NPU TOPS and AMD PRO Technologies, the new PCs are engineered to deliver lightning-fast AI compute and seamless productivity for working professionals. Designed to support the next generation of Microsoft Copilot+ experiences, Ryzen AI 300 Series processors deliver a leading peak of 50+ NPU TOPS of AI performance.\nCommercial systems powered by Ryzen AI 300 Series processors offer enterprises notebooks with the compute power required to support the shift to an AI-enabled workforce.ASUS notebooks powered by Ryzen AI PRO Series processors also come equipped with AMD PRO Technologies, offering enterprises and SMBs with built-in security and manageability features, as well as long-term platform stability.\nGB Daily\nStay in the know! Get the latest news in your inbox daily\nSubscribe\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nYour daily dose of gaming insights\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-05-22T04:36:45.576042",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-22T04:37:58.428443",
    "audio_file": "eb3a91ddb2687cc1d6185d1ce361cb9f.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/backend/data/audio/eb3a91ddb2687cc1d6185d1ce361cb9f.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-22T04:38:14.277953"
  }
]