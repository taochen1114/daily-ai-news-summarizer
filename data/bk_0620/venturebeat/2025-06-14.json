[
  {
    "id": "daeef91f55537c77cfffd3341b32b637",
    "title": "Just add humans: Oxford medical study underscores the missing link in chatbot testing",
    "url": "https://venturebeat.com/ai/just-add-humans-oxford-medical-study-underscores-the-missing-link-in-chatbot-testing/",
    "authors": "Nick Mokey",
    "published_date": "2025-06-14T00:34:19+00:00",
    "source": "VentureBeat",
    "summary": "牛津醫學研究指出，在測試聊天機器人時，加入人類參與是關鍵。研究發現，雖然大型語言模型在直接測試情境下能正確識別相關病況達94.9%，但人類使用這些模型診斷的準確率不到34.5%。更驚人的是，患者使用這些模型的表現甚至比自行診斷的對照組還差。這項研究引發對於使用語言模型提供醫療建議的適當性和評估聊天機器人應用的基準的疑問。",
    "content": "Just add humans: Oxford medical study underscores the missing link in chatbot testing | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nJust add humans: Oxford medical study underscores the missing link in chatbot testing\nNick Mokey\nJune 13, 2025 5:34 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCreated by VentureBeat using ChatGPT\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nHeadlines have been blaring it for years: Large language models (LLMs) can not only pass medical licensing exams but also outperform humans. GPT-4 could correctly answer U.S. medical exam licensing questions 90% of the time, even in the prehistoric AI days of 2023. Since then, LLMs have gone on to best the\nresidents taking those exams\nand\nlicensed physicians\n.\nMove over, Doctor Google, make way for ChatGPT, M.D. But you may want more than a diploma from the LLM you deploy for patients. Like an ace medical student who can rattle off the name of every bone in the hand but faints at the first sight of real blood, an LLM’s mastery of medicine does not always translate directly into the real world.\nA\npaper\nby researchers at\nthe University of Oxford\nfound that while LLMs could correctly identify relevant conditions 94.9% of the time when directly presented with test scenarios, human participants using LLMs to diagnose the same scenarios identified the correct conditions less than 34.5% of the time.\nPerhaps even more notably, patients using LLMs performed even worse than a control group that was merely instructed to diagnose themselves using “any methods they would typically employ at home.” The group left to their own devices was 76% more likely to identify the correct conditions than the group assisted by LLMs.\nThe Oxford study raises questions about the suitability of LLMs for medical advice and the benchmarks we use to evaluate chatbot deployments for various applications.\nGuess your malady\nLed by Dr. Adam Mahdi, researchers at Oxford recruited 1,298 participants to present themselves as patients to an LLM. They were tasked with both attempting to figure out what ailed them and the appropriate level of care to seek for it, ranging from self-care to calling an ambulance.\nEach participant received a detailed scenario, representing conditions from pneumonia to the common cold, along with general life details and medical history. For instance, one scenario describes a 20-year-old engineering student who develops a crippling headache on a night out with friends. It includes important medical details (it’s painful to look down) and red herrings (he’s a regular drinker, shares an apartment with six friends, and just finished some stressful exams).\nThe study tested three different LLMs. The researchers selected\nGPT-4o\non account of its popularity,\nLlama 3\nfor its open weights and\nCommand R+\nfor its retrieval-augmented generation (RAG) abilities, which allow it to search the open web for help.\nParticipants were asked to interact with the LLM at least once using the details provided, but could use it as many times as they wanted to arrive at their self-diagnosis and intended action.\nBehind the scenes, a team of physicians unanimously decided on the “gold standard” conditions they sought in every scenario, and the corresponding course of action. Our engineering student, for example, is suffering from a subarachnoid haemorrhage, which should entail an immediate visit to the ER.\nA game of telephone\nWhile you might assume an LLM that can ace a medical exam would be the perfect tool to help ordinary people self-diagnose and figure out what to do, it didn’t work out that way. “Participants using an LLM identified relevant conditions less consistently than those in the control group, identifying at least one relevant condition in at most 34.5% of cases compared to 47.0% for the control,” the study states. They also failed to deduce the correct course of action, selecting it just 44.2% of the time, compared to 56.3% for an LLM acting independently.\nWhat went wrong?\nLooking back at transcripts, researchers found that participants both provided incomplete information to the LLMs and the LLMs misinterpreted their prompts. For instance, one user who was supposed to exhibit symptoms of gallstones merely told the LLM: “I get severe stomach pains lasting up to an hour, It can make me vomit and seems to coincide with a takeaway,” omitting the location of the pain, the severity, and the frequency. Command R+ incorrectly suggested that the participant was experiencing indigestion, and the participant incorrectly guessed that condition.\nEven when LLMs delivered the correct information, participants didn’t always follow its recommendations. The study found that 65.7% of GPT-4o conversations suggested at least one relevant condition for the scenario, but somehow less than 34.5% of final answers from participants reflected those relevant conditions.\nThe human variable\nThis study is useful, but not surprising, according to Nathalie Volkheimer, a user experience specialist at the\nRenaissance Computing Institute (RENCI)\n, University of North Carolina at Chapel Hill.\n“For those of us old enough to remember the early days of internet search, this is déjà vu,” she says. “As a tool, large language models require prompts to be written with a particular degree of quality, especially when expecting a quality output.”\nShe points out that someone experiencing blinding pain wouldn’t offer great prompts. Although participants in a lab experiment weren’t experiencing the symptoms directly, they weren’t relaying every detail.\n“There is also a reason why clinicians who deal with patients on the front line are trained to ask questions in a certain way and a certain repetitiveness,” Volkheimer goes on. Patients omit information because they don’t know what’s relevant, or at worst, lie because they’re embarrassed or ashamed.\nCan chatbots be better designed to address them? “I wouldn’t put the emphasis on the machinery here,” Volkheimer cautions. “I would consider the emphasis should be on the human-technology interaction.” The car, she analogizes, was built to get people from point A to B, but many other factors play a role. “It’s about the driver, the roads, the weather, and the general safety of the route. It isn’t just up to the machine.”\nA better yardstick\nThe Oxford study highlights one problem, not with humans or even LLMs, but with the way we sometimes measure them—in a vacuum.\nWhen we say an LLM can pass a medical licensing test, real estate licensing exam, or a state bar exam, we’re probing the depths of its knowledge base using tools designed to evaluate humans. However, these measures tell us very little about how successfully these chatbots will interact with humans.\n“The prompts were textbook (as validated by the source and medical community), but life and people are not textbook,” explains Dr. Volkheimer.\nImagine an enterprise about to deploy a support chatbot trained on its internal knowledge base. One seemingly logical way to test that bot might simply be to have it take the same test the company uses for customer support trainees: answering prewritten “customer” support questions and selecting multiple-choice answers. An accuracy of 95% would certainly look pretty promising.\nThen comes deployment: Real customers use vague terms, express frustration, or describe problems in unexpected ways. The LLM, benchmarked only on clear-cut questions, gets confused and provides incorrect or unhelpful answers. It hasn’t been trained or evaluated on de-escalating situations or seeking clarification effectively. Angry reviews pile up. The launch is a disaster, despite the LLM sailing through tests that seemed robust for its human counterparts.\nThis study serves as a critical reminder for AI engineers and orchestration specialists: if an LLM is designed to interact with humans, relying solely on non-interactive benchmarks can create a dangerous false sense of security about its real-world capabilities. If you’re designing an LLM to interact with humans, you need to test it with humans – not tests for humans. But is there a better way?\nUsing AI to test AI\nThe Oxford researchers recruited nearly 1,300 people for their study, but most enterprises don’t have a pool of test subjects sitting around waiting to play with a new LLM agent. So why not just substitute AI testers for human testers?\nMahdi and his team tried that, too, with simulated participants. “You are a patient,” they prompted an LLM, separate from the one that would provide the advice. “You have to self-assess your symptoms from the given case vignette and assistance from an AI model. Simplify terminology used in the given paragraph to layman language and keep your questions or statements reasonably short.” The LLM was also instructed not to use medical knowledge or generate new symptoms.\nThese simulated participants then chatted with the same LLMs the human participants used. But they performed much better. On average, simulated participants using the same LLM tools nailed the relevant conditions 60.7% of the time, compared to below 34.5% in humans.\nIn this case, it turns out LLMs play nicer with other LLMs than humans do, which makes them a poor predictor of real-life performance.\nDon’t blame the user\nGiven the scores LLMs could attain on their own, it might be tempting to blame the participants here. After all, in many cases, they received the right diagnoses in their conversations with LLMs, but still failed to correctly guess it. But that would be a foolhardy conclusion for any business, Volkheimer warns.\n“In every customer environment, if your customers aren’t doing the thing you want them to, the last thing you do is blame the customer,” says Volkheimer. “The first thing you do is ask why. And not the ‘why’ off the top of your head: but a deep investigative, specific, anthropological, psychological, examined ‘why.’ That’s your starting point.”\nYou need to understand your audience, their goals, and the customer experience before deploying a chatbot, Volkheimer suggests. All of these will inform the thorough, specialized documentation that will ultimately make an LLM useful. Without carefully curated training materials, “It’s going to spit out some generic answer everyone hates, which is why people hate chatbots,” she says. When that happens, “It’s not because chatbots are terrible or because there’s something technically wrong with them. It’s because the stuff that went in them is bad.”\n“The people designing technology, developing the information to go in there and the processes and systems are, well, people,” says Volkheimer. “They also have background, assumptions, flaws and blindspots, as well as strengths. And all those things can get built into any technological solution.”\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-14T08:46:10.101286",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-14T08:46:15.375836",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 247 credits are required for this request."
  },
  {
    "id": "d817d5b0a40d281740957ed966d60937",
    "title": "Do reasoning models really “think” or not? Apple research sparks lively debate, response",
    "url": "https://venturebeat.com/ai/do-reasoning-models-really-think-or-not-apple-research-sparks-lively-debate-response/",
    "authors": "Carl Franzen",
    "published_date": "2025-06-13T22:02:22+00:00",
    "source": "VentureBeat",
    "summary": "蘋果的機器學習團隊發表研究論文，質疑大型推理模型是否真的能「思考」。他們認為這些模型只是在做「模式匹配」，當任務變得複雜時，它們的推理能力就會崩潰。這引發了激烈的辯論，許多人認為蘋果的研究否定了這類人工智慧的炒作。這篇研究對於提升生成式人工智慧至人工通用智能的路徑提出了挑戰。",
    "content": "Do reasoning models really think or not? Apple research sparks lively debate, response | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nDo reasoning models really “think” or not? Apple research sparks lively debate, response\nCarl Franzen\n@carlfranzen\nJune 13, 2025 3:02 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nApple’s machine-learning group set off a rhetorical firestorm earlier this month with its release of “\nThe Illusion of Thinking\n,” a 53-page research paper arguing that so-called large reasoning models (LRMs) or reasoning large language models (reasoning LLMs) such as OpenAI’s “o” series and Google’s Gemini-2.5 Pro and Flash Thinking don’t actually engage in independent “thinking” or “reasoning” from generalized first principles learned from their training data.\nInstead, the authors contend, these reasoning LLMs are actually performing a kind of “pattern matching” and their apparent reasoning ability seems to fall apart once a task becomes too complex, suggesting that their architecture and performance is not a viable path to improving generative AI to the point that it is artificial generalized intelligence (AGI), which OpenAI defines as a model that outperforms humans at most economically valuable work, or superintelligence, AI even smarter than human beings can comprehend.\nACT NOW: Come discuss the latest LLM advances and research at VB Transform on June 24-25 in SF — limited tickets available\n.\nREGISTER NOW\nUnsurprisingly, the paper immediately circulated widely among the machine learning community on X and many readers’ initial reactions were to declare that Apple had effectively disproven much of the hype around this class of AI: “Apple just proved AI ‘reasoning’ models like Claude, DeepSeek-R1, and o3-mini don’t actually reason at all,”\ndeclared Ruben Hassid\n, creator of EasyGen, an LLM-driven LinkedIn post auto writing tool. “They just memorize patterns really well.”\nBut now today,\na new paper has emerged\n, the cheekily titled “\nThe Illusion of The Illusion of Thinking\n” — importantly, co-authored by a reasoning LLM itself, Claude Opus 4 and Alex Lawsen, a human being and independent AI researcher and technical writer — that includes many criticisms from the larger ML community about the paper and effectively argues that the methodologies and experimental designs the Apple Research team used in their initial work are fundamentally flawed.\nWhile we here at VentureBeat are not ML researchers ourselves and not prepared to say the Apple Researchers are wrong, the debate has certainly been a lively one and the issue about the capabilities of LRMs or reasoner LLMs compared to human thinking seems far from settled.\nHow the Apple Research study was designed — and what it found\nUsing four classic planning problems — Tower of Hanoi, Blocks World, River Crossing and Checkers Jumping — Apple’s researchers designed a battery of tasks that forced reasoning models to plan multiple moves ahead and generate complete solutions.\nThese games were chosen for their long history in cognitive science and AI research and their ability to scale in complexity as more steps or constraints are added. Each puzzle required the models to not just produce a correct final answer, but to explain their thinking along the way using chain-of-thought prompting.\nAs the puzzles increased in difficulty, the researchers observed a consistent drop in accuracy across multiple leading reasoning models. In the most complex tasks, performance plunged to zero. Notably, the length of the models’ internal reasoning traces—measured by the number of tokens spent thinking through the problem—also began to shrink. Apple’s researchers interpreted this as a sign that the models were abandoning problem-solving altogether once the tasks became too hard, essentially “giving up.”\nThe timing of the paper’s release,\njust ahead of Apple’s annual Worldwide Developers Conference (WWDC)\n, added to the impact. It quickly went viral across X, where many interpreted the findings as a high-profile admission that current-generation LLMs are still glorified autocomplete engines, not general-purpose thinkers. This framing, while controversial, drove much of the initial discussion and debate that followed.\nCritics take aim on X\nAmong the most vocal critics of the Apple paper\nwas ML researcher and X user @scaling01\n(aka “Lisan al Gaib”), who posted multiple threads dissecting the methodology.\nIn\none widely shared post\n, Lisan argued that the Apple team conflated token budget failures with reasoning failures, noting that “all models will have 0 accuracy with more than 13 disks simply because they cannot output that much!”\nFor puzzles like Tower of Hanoi, he emphasized, the output size grows exponentially, while the LLM context windows remain fixed, writing “just because Tower of Hanoi requires exponentially more steps than the other ones, that only require quadratically or linearly more steps, doesn’t mean Tower of Hanoi is more difficult” and convincingly showed that models like Claude 3 Sonnet and DeepSeek-R1 often produced algorithmically correct strategies in plain text or code—yet were still marked wrong.\nAnother post\nhighlighted that even breaking the task down into smaller, decomposed steps worsened model performance—not because the models failed to understand, but because they lacked memory of previous moves and strategy.\n“The LLM needs the history and a grand strategy,” he wrote, suggesting the real problem was context-window size rather than reasoning.\nI raised\nanother important grain of salt myself on X\n: Apple never benchmarked the model performance against human performance on the same tasks. “Am I missing it, or did you not compare LRMs to human perf[ormance] on [the] same tasks?? If not, how do you know this same drop-off in perf doesn’t happen to people, too?” I asked the researchers directly in a thread tagging the paper’s authors. I also emailed them about this and many other questions, but they have yet to respond.\nOthers echoed that sentiment, noting that human problem solvers also falter on long, multistep logic puzzles, especially without pen-and-paper tools or memory aids. Without that baseline, Apple’s claim of a fundamental “reasoning collapse” feels ungrounded.\nSeveral researchers also questioned the binary framing of the paper’s title and thesis—drawing a hard line between “pattern matching” and “reasoning.”\nAlexander Doria\naka Pierre-Carl Langlais, an LLM trainer at energy efficient French AI startup\nPleias\n, said the framing\nmisses the nuance\n, arguing that models might be learning partial heuristics rather than simply matching patterns.\nOk I guess I have to go through that Apple paper.\nMy main issue is the framing which is super binary: \"Are these models capable of generalizable reasoning, or are they leveraging different forms of pattern matching?\" Or what if they only caught genuine yet partial heuristics.\npic.twitter.com/GZE3eG7WlM\n— Alexander Doria (@Dorialexander)\nJune 8, 2025\nEthan Mollick, the AI focused professor at University of Pennsylvania’s Wharton School of Business,\ncalled the idea that LLMs are “hitting a wall” premature, likening it to similar claims about “model collapse” that didn’t pan out.\nMeanwhile, critics like\n@arithmoquine\nwere more cynical, suggesting that Apple—behind the curve on LLMs compared to rivals like OpenAI and Google—might be trying to lower expectations,” coming up with research on “how it’s all fake and gay and doesn’t matter anyway” they quipped, pointing out Apple’s reputation with now poorly performing AI products like Siri.\nIn short, while Apple’s study triggered a meaningful conversation about evaluation rigor, it also exposed a deep rift over how much trust to place in metrics when the test itself might be flawed.\nA measurement artifact, or a ceiling?\nIn other words, the models may have understood the puzzles but ran out of “paper” to write the full solution.\n“Token limits, not logic, froze the models,” wrote Carnegie Mellon researcher Rohan Paul in\na widely shared thread summarizing the follow-up tests.\nYet not everyone is ready to clear LRMs of the charge. Some observers point out that Apple’s study still revealed three performance regimes — simple tasks where added reasoning hurts, mid-range puzzles where it helps, and high-complexity cases where both standard and “thinking” models crater.\nOthers view the debate as corporate positioning, noting that Apple’s own on-device “Apple Intelligence” models trail rivals on many public leaderboards.\nThe rebuttal: “The Illusion of the Illusion of Thinking”\nIn response to Apple’s claims, a new paper titled “\nThe Illusion of the Illusion of Thinking\n” was released on arXiv by independent researcher and technical writer\nAlex Lawsen of the nonprofit Open Philanthropy\n, in collaboration with Anthropic’s Claude Opus 4.\nThe paper directly challenges the original study’s conclusion that LLMs fail due to an inherent inability to reason at scale. Instead, the rebuttal presents evidence that the observed performance collapse was largely a by-product of the test setup—not a true limit of reasoning capability.\nLawsen and Claude demonstrate that many of the failures in the Apple study stem from token limitations. For example, in tasks like Tower of Hanoi, the models must print exponentially many steps — over 32,000 moves for just 15 disks — leading them to hit output ceilings.\nThe rebuttal points out that Apple’s evaluation script penalized these token-overflow outputs as incorrect, even when the models followed a correct solution strategy internally.\nThe authors also highlight several questionable task constructions in the Apple benchmarks. Some of the River Crossing puzzles, they note, are mathematically unsolvable as posed, and yet model outputs for these cases were still scored. This further calls into question the conclusion that accuracy failures represent cognitive limits rather than structural flaws in the experiments.\nTo test their theory, Lawsen and Claude ran new experiments allowing models to give compressed, programmatic answers. When asked to output a Lua function that could generate the Tower of Hanoi solution—rather than writing every step line-by-line—models suddenly succeeded on far more complex problems. This shift in format eliminated the collapse entirely, suggesting that the models didn’t fail to reason. They simply failed to conform to an artificial and overly strict rubric.\nWhy it matters for enterprise decision-makers\nThe back-and-forth underscores a growing consensus: evaluation design is now as important as model design.\nRequiring LRMs to enumerate every step may test their printers more than their planners, while compressed formats, programmatic answers or external scratchpads give a cleaner read on actual reasoning ability.\nThe episode also highlights practical limits developers face as they ship agentic systems—context windows, output budgets and task formulation can make or break user-visible performance.\nFor enterprise technical decision makers building applications atop reasoning LLMs, this debate is more than academic. It raises critical questions about where, when, and how to trust these models in production workflows—especially when tasks involve long planning chains or require precise step-by-step output.\nIf a model appears to “fail” on a complex prompt, the problem may not lie in its reasoning ability, but in how the task is framed, how much output is required, or how much memory the model has access to. This is particularly relevant for industries building tools like copilots, autonomous agents, or decision-support systems, where both interpretability and task complexity can be high.\nUnderstanding the constraints of context windows, token budgets, and the scoring rubrics used in evaluation is essential for reliable system design. Developers may need to consider hybrid solutions that externalize memory, chunk reasoning steps, or use compressed outputs like functions or code instead of full verbal explanations.\nMost importantly, the paper’s controversy is a reminder that benchmarking and real-world application are not the same. Enterprise teams should be cautious of over-relying on synthetic benchmarks that don’t reflect practical use cases—or that inadvertently constrain the model’s ability to demonstrate what it knows.\nUltimately, the big takeaway for ML researchers is that before proclaiming an AI milestone—or obituary—make sure the test itself isn’t putting the system in a box too small to think inside.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-14T08:46:10.221170",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-14T08:46:18.481395",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 227 credits are required for this request."
  },
  {
    "id": "6baf034407326636f909f2d07f462957",
    "title": "Beyond GPT architecture: Why Google’s Diffusion approach could reshape LLM deployment",
    "url": "https://venturebeat.com/ai/beyond-gpt-architecture-why-googles-diffusion-approach-could-reshape-llm-deployment/",
    "authors": "David Chen",
    "published_date": "2025-06-13T21:48:11+00:00",
    "source": "VentureBeat",
    "summary": "Google的新技術Gemini Diffusion採用了一種新方法生成文字，比起傳統的autoregression方式更快速且提升了一致性。這種新方法可以讓大型語言模型更有效率地生成內容，可能會改變LLM的應用方式。Gemini Diffusion目前是實驗性的，想要體驗的人可以報名等待名單。這種新方法的出現可能會影響到未來AI文字生成的發展方向。",
    "content": "Beyond GPT architecture: Why Google's Diffusion approach could reshape LLM deployment | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nAnalysis\nBeyond GPT architecture: Why Google’s Diffusion approach could reshape LLM deployment\nDavid Chen\nJune 13, 2025 2:48 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCreated by VentureBeat using ChatGPT\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nLast month, along with a comprehensive suite of\nnew AI tools\nand innovations,\nGoogle DeepMind\nunveiled\nGemini Diffusion\n. This experimental research model uses a diffusion-based approach to generate text. Traditionally, large language models (LLMs) like GPT and Gemini itself have relied on autoregression, a step-by-step approach where each word is generated based on the previous one.\nDiffusion language models (DLMs)\n, also known as diffusion-based large language models (dLLMs), leverage a method more commonly seen in image generation, starting with random noise and gradually refining it into a coherent output. This approach dramatically increases generation speed and can improve coherency and consistency.\nGemini Diffusion is currently available as an experimental demo;\nsign up for the waitlist\nhere to get access\n.\n(Editor’s note: We’ll be unpacking paradigm shifts like diffusion-based language models—and what it takes to run them in production—at\nVB Transform\n, June 24–25 in San Francisco\n, alongside Google DeepMind, LinkedIn and other enterprise AI leaders.)\nUnderstanding diffusion vs. autoregression\nDiffusion and autoregression are fundamentally different approaches. The autoregressive approach generates text sequentially, with tokens predicted one at a time. While this method ensures strong coherence and context tracking, it can be computationally intensive and slow, especially for long-form content.\nDiffusion models, by contrast, begin with random noise, which is gradually denoised into a coherent output. When applied to language, the technique has several advantages. Blocks of text can be processed in parallel, potentially producing entire segments or sentences at a much higher rate.\nGemini Diffusion can reportedly generate 1,000-2,000 tokens per second. In contrast, Gemini 2.5 Flash has an average output speed of 272.4 tokens per second. Additionally, mistakes in generation can be corrected during the refining process, improving accuracy and reducing the number of hallucinations. There may be trade-offs in terms of fine-grained accuracy and token-level control; however, the increase in speed will be a game-changer for numerous applications.\nHow does diffusion-based text generation work?\nDuring training, DLMs work by gradually corrupting a sentence with noise over many steps, until the original sentence is rendered entirely unrecognizable. The model is then trained to reverse this process, step by step, reconstructing the original sentence from increasingly noisy versions. Through the iterative refinement, it learns to model the entire distribution of plausible sentences in the training data.\nWhile the specifics of Gemini Diffusion have not yet been disclosed, the typical training methodology for a diffusion model involves these key stages:\nForward diffusion:\nWith each sample in the training dataset, noise is added progressively over multiple cycles (often 500 to 1,000) until it becomes indistinguishable from random noise.\nReverse diffusion:\nThe model learns to reverse each step of the noising process, essentially learning how to “denoise” a corrupted sentence one stage at a time, eventually restoring the original structure.\nThis process is repeated millions of times with diverse samples and noise levels, enabling the model to learn a reliable denoising function.\nOnce trained, the model is capable of generating entirely new sentences. DLMs generally require a condition or input, such as a prompt, class label, or embedding, to guide the generation towards desired outcomes. The condition is injected into each step of the denoising process, which shapes an initial blob of noise into structured and coherent text.\nAdvantages and disadvantages of diffusion-based models\nIn an interview with VentureBeat, Brendan O’Donoghue, research scientist at Google DeepMind and one of the leads on the Gemini Diffusion project, elaborated on some of the advantages of diffusion-based techniques when compared to autoregression. According to O’Donoghue, the major advantages of diffusion techniques are the following:\nLower latencies:\nDiffusion models can produce a sequence of tokens in much less time than autoregressive models.\nAdaptive computation:\nDiffusion models will converge to a sequence of tokens at different rates depending on the task’s difficulty. This allows the model to consume fewer resources (and have lower latencies) on easy tasks and more on harder ones.\nNon-causal reasoning:\nDue to the bidirectional attention in the denoiser, tokens can attend to future tokens within the same generation block. This allows non-causal reasoning to take place and allows the model to make global edits within a block to produce more coherent text.\nIterative refinement / self-correction:\nThe denoising process involves sampling, which can introduce errors just like in autoregressive models. However, unlike autoregressive models, the tokens are passed back into the denoiser, which then has an opportunity to correct the error.\nO’Donoghue also noted the main disadvantages: “higher cost of serving and slightly higher time-to-first-token (TTFT), since autoregressive models will produce the first token right away. For diffusion, the first token can only appear when the entire sequence of tokens is ready.”\nPerformance benchmarks\nGoogle says Gemini Diffusion’s performance is\ncomparable to Gemini 2.0 Flash-Lite\n.\nBenchmark\nType\nGemini Diffusion\nGemini 2.0 Flash-Lite\nLiveCodeBench (v6)\nCode\n30.9%\n28.5%\nBigCodeBench\nCode\n45.4%\n45.8%\nLBPP (v2)\nCode\n56.8%\n56.0%\nSWE-Bench Verified*\nCode\n22.9%\n28.5%\nHumanEval\nCode\n89.6%\n90.2%\nMBPP\nCode\n76.0%\n75.8%\nGPQA Diamond\nScience\n40.4%\n56.5%\nAIME 2025\nMathematics\n23.3%\n20.0%\nBIG-Bench Extra Hard\nReasoning\n15.0%\n21.0%\nGlobal MMLU (Lite)\nMultilingual\n69.1%\n79.0%\n* Non-agentic evaluation (single turn edit only), max prompt length of 32K.\nThe two models were compared using several benchmarks, with scores based on how many times the model produced the correct answer on the first try. Gemini Diffusion performed well in coding and mathematics tests, while Gemini 2.0 Flash-lite had the edge on reasoning, scientific knowledge, and multilingual capabilities.\nAs Gemini Diffusion evolves, there’s no reason to think that its performance won’t catch up with more established models. According to O’Donoghue, the gap between the two techniques is “essentially closed in terms of benchmark performance, at least at the relatively small sizes we have scaled up to. In fact, there may be some performance advantage for diffusion in some domains where non-local consistency is important, for example, coding and reasoning.”\nTesting Gemini Diffusion\nVentureBeat was granted access to the experimental demo. When putting Gemini Diffusion through its paces, the first thing we noticed was the speed. When running the suggested prompts provided by Google, including building interactive HTML apps like Xylophone and Planet Tac Toe, each request completed in under three seconds, with speeds ranging from 600 to 1,300 tokens per second.\nTo test its performance with a real-world application, we asked Gemini Diffusion to build a video chat interface with the following prompt:\nBuild an interface for a video chat application. It should have a preview window that accesses the camera on my device and displays its output. The interface should also have a sound level meter that measures the output from the device's microphone in real time.\nIn less than two seconds, Gemini Diffusion created a working interface with a video preview and an audio meter.\nThough this was not a complex implementation, it could be the start of an MVP that can be completed with a bit of further prompting. Note that Gemini 2.5 Flash also produced a working interface, albeit at a slightly slower pace (approximately seven seconds).\nGemini Diffusion also features “Instant Edit,” a mode where text or code can be pasted in and edited in real-time with minimal prompting. Instant Edit is effective for many types of text editing, including correcting grammar, updating text to target different reader personas, or adding SEO keywords. It is also useful for tasks such as refactoring code, adding new features to applications, or converting an existing codebase to a different language.\nEnterprise use cases for DLMs\nIt’s safe to say that any application that requires a quick response time stands to benefit from DLM technology. This includes real-time and low-latency applications, such as conversational AI and chatbots, live transcription and translation, or IDE autocomplete and coding assistants.\nAccording to O’Donoghue, with applications that leverage “inline editing, for example, taking a piece of text and making some changes in-place, diffusion models are applicable in ways autoregressive models aren’t.” DLMs also have an advantage with reason, math, and coding problems, due to “the non-causal reasoning afforded by the bidirectional attention.”\nDLMs are still in their infancy; however, the technology can potentially transform how language models are built. Not only do they generate text at a much higher rate than autoregressive models, but their ability to go back and fix mistakes means that, eventually, they may also produce results with greater accuracy.\nGemini Diffusion enters a growing ecosystem of DLMs, with two notable examples being\nMercury\n, developed by Inception Labs, and\nLLaDa\n, an open-source model from GSAI. Together, these models reflect the broader momentum behind diffusion-based language generation and offer a scalable, parallelizable alternative to traditional autoregressive architectures.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-14T08:46:10.368597",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-14T08:46:20.128167",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 262 credits are required for this request."
  },
  {
    "id": "175ad30027aa0d4711dbcef48784b1c4",
    "title": "The case for embedding audit trails in AI systems before scaling",
    "url": "https://venturebeat.com/ai/the-case-for-embedding-audit-trails-in-ai-systems-before-scaling/",
    "authors": "Emilia David",
    "published_date": "2025-06-13T20:13:09+00:00",
    "source": "VentureBeat",
    "summary": "這篇新聞談到在AI系統擴展之前，應該先內建審計蹤跡，以確保系統運作正常且符合規定。內建審計蹤跡可以讓企業管理者追蹤系統運作情況，避免問題發生時才發現。專家指出，這樣的控制措施可以幫助企業確保AI系統運作正常，並提供必要的記錄以追蹤資訊提供者和操作情況，有助於確保資訊安全。建議在AI系統早期階段就加入這些功能，以降低風險並確保系統運作穩定。",
    "content": "The case for embedding audit trails in AI systems before scaling | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nThe case for embedding audit trails in AI systems before scaling\nEmilia David\n@miyadavid\nJune 13, 2025 1:13 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat, generated with ChatGPT\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nEditor’s note: Emilia will lead an editorial roundtable on this topic at VB Transform this month.\nRegister today\n.\nOrchestration frameworks for AI services serve multiple functions for enterprises. They not only set out how applications or agents flow together, but they should also let administrators manage workflows and agents and audit their systems.\nAs enterprises begin to scale their AI services and put these into production, building a manageable, traceable, auditable and\nrobust pipeline\nensures their agents run exactly as they’re supposed to. Without these controls, organizations may not be aware of what is happening in their AI systems and may only discover the issue too late, when something goes wrong or they fail to comply with regulations.\nKevin Kiley, president of enterprise orchestration company\nAiria\n, told VentureBeat in an interview that frameworks must include auditability and traceability.\n“It’s critical to have that observability and be able to go back to the audit log and show what information was provided at what point again,” Kiley said. “You have to know if it was a bad actor, or an internal employee who wasn’t aware they were sharing information or if it was a hallucination. You need a record of that.”\nIdeally, robustness and audit trails should be built into AI systems at a very early stage. Understanding the potential risks of a new AI application or agent and ensuring they continue to perform to standards before deployment would help ease concerns around putting AI into production.\nHowever, organizations did not initially design their systems with\ntraceability and auditability in mind\n. Many AI pilot programs began life as experiments started without an orchestration layer or an audit trail.\nThe big question enterprises now face is how to manage all the agents and applications,\nensure their pipelines remain robust\nand, if something goes wrong, they know what went wrong and monitor AI performance.\nChoosing the right method\nBefore building any AI application, however, experts said organizations need to\ntake stock of their data\n. If a company knows which data they’re okay with AI systems to access and which data they fine-tuned a model with, they have that baseline to compare long-term performance with.\n“When you run some of those AI systems, it’s more about, what kind of data can I validate that my system’s actually running properly or not?” Yrieix Garnier, vice president of products at\nDataDog\n, told VentureBeat in an interview. “That’s very hard to actually do, to understand that I have the right system of reference to validate AI solutions.”\nOnce the organization identifies and locates its data, it needs to establish dataset versioning — essentially assigning a timestamp or version number — to make experiments reproducible and understand what the model has changed. These datasets and models, any applications that use these specific models or agents, authorized users and the baseline runtime numbers can be loaded into either the orchestration or observability platform.\nJust like when choosing foundation models to build with, orchestration teams need to consider transparency and openness. While some closed-source orchestration systems have numerous advantages, more open-source platforms could also offer benefits that some enterprises value, such as increased visibility into decision-making systems.\nOpen-source platforms like\nMLFlow\n,\nLangChain\nand\nGrafana\nprovide agents and models with granular and flexible instructions and monitoring. Enterprises can choose to develop their AI pipeline through a single, end-to-end platform, such as DataDog, or utilize various interconnected tools from\nAWS.\nAnother consideration for enterprises is to plug in a system that maps agents and application responses to compliance tools or responsible AI policies. AWS and\nMicrosoft\nboth offer services that track AI tools and how closely they adhere to guardrails and other policies set by the user.\nKiley said one consideration for enterprises when building these reliable pipelines revolves around choosing a more transparent system. For Kiley, not having any visibility into how AI systems work won’t work.\n“Regardless of what the use case or even the industry is, you’re going to have those situations where you have to have flexibility, and a closed system is not going to work. There are providers out there that’ve great tools, but it’s sort of a black box. I don’t know how it’s arriving at these decisions. I don’t have the ability to intercept or interject at points where I might want to,” he said.\nJoin the conversation at VB Transform\nI’ll be leading an editorial roundtable at\nVB Transform 2025\nin San Francisco, June 24-25, called “Best practices to build orchestration frameworks for agentic AI,” and I’d love to have you join the conversation.\nRegister today\n.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-14T08:46:10.522171",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-14T08:46:22.888039",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 235 credits are required for this request."
  },
  {
    "id": "764cee8324b5babace715c43fc61b33c",
    "title": "Wizards of the Coast and Giant Skull: ‘Gamers are telling us what they have always told us’ | The DeanBeat",
    "url": "https://venturebeat.com/games/wizards-of-the-coast-and-giant-skull-gamers-are-telling-us-what-they-have-always-told-us-the-deanbeat/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-13T17:00:00+00:00",
    "source": "VentureBeat",
    "summary": "Wizards of the Coast和Giant Skull合作開發一款基於龍與地下城(D&D)世界的單人動作冒險遊戲。這次合作被認為是兩家公司在遊戲領域野心的重要時刻。遊戲目前正在為PC和遊戲主機進行開發，更多細節將在未來公布。Wizards of the Coast的總裁John Hight和Giant Skull的CEO Stig Asmussen對這次合作表示了期待。整體來說，這個消息對遊戲界來說是一個值得關注的重要合作案。",
    "content": "Wizards of the Coast and Giant Skull: 'Gamers are telling us what they have always told us' | The DeanBeat | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nWizards of the Coast and Giant Skull: ‘Gamers are telling us what they have always told us’ | The DeanBeat\nDean Takahashi\n@deantak\nJune 13, 2025 10:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nGiant Skull is making a D&D game for Wizards of the Coast.\nImage Credit: Wizards of the Coast\nTen days, ago, Hasbro’s\nWizards of the Coast\nannounced an exclusive publishing agreement with Giant Skull, the game studio started by Star Wars Jedi: Survivor game leader Stig Asmussen.\nThey announced that Asmussen’s studio is working on a new single-player action adventure title set in the world of Dungeons & Dragons.\nI had a chance to talk with the company leaders about the deal. At the Summer Game Fest Play Days, I sat down with\nJohn Hight, President of Wizards of the Coast and Digital Gaming at Hasbro; and Asmussen, who is the CEO of Giant Skull and a former leader at Respawn.\nThis certainly sounds like a big effort, as it will be an original title. It’s a single-player action-adventure title set in the world of Dungeons & Dragons and “marks a definitive moment in both companies’ gaming ambitions.”\nThe game is currently in development for PC and console and more details will be revealed at a later date. Hight himself is a new executive in charge of the Magic: The Gathering and Dungeons & Dragons game and game licensing business at Hasbro, as he has been on the job for less than a year. He was previously senior vice president and general manager of the Warcraft franchise at Blizzard Entertainment, where he oversaw all development and commercial activities for World of Warcraft, Hearthstone, and Warcraft Rumble.\nDuring his 12-year tenure at Blizzard, John directed development efforts for multiple World of Warcraft expansions, including Diablo III: Reaper of Souls, and Diablo III on console. Prior to Blizzard, Hight worked on over 30 games on various platforms, including critically acclaimed games in the Command & Conquer, Neverwinter Nights, and God of War franchises.\nJohn Hight of Wizzards of the Coast and Stig Asmussen (right) of Giant Skull.\n“It’s been not quite a year stepping into Wizards of the Coast. It’s been incredible. I knew coming into it that the goal is to build a digital publishing division,” Hight said in our interview. “We already had some games underway, but we wanted our goal essentially is to have a couple of premium games a year that we’re releasing as part of the Wizards label. So it was fun, and one of the first calls I made after getting the job was to Stig because if you want to build one of the best games out there, you talk to one of the best developers.”\nAsmussen was most recently the game director of Star Wars Jedi: Survivor and Star Wars Jedi: Fallen Order for Respawn Entertainment, a division of Electronic Arts. Prior to that, he was the game director on God of War III and the art director for God of War II at Sony Santa Monica. Giant Skull has an elite team, all of whom will be instrumental in shaping this new single-player focused action adventure, utilizing Unreal Engine 5, from the ground-up.\nAsmussen said, “We opened up Giant Skull in September 2023, and a year later, John calls me up. He said, ‘Hey, I’m at Wizards now.’ We had been talking throughout that time. When I was thinking about starting a new company, John and I were talking a lot like talking about different ideas and how that could work out. I was picking his brain. when he reached out, and he came out to Encino to see the game and what we had been working on, he got to see our vertical slice.”\nReaching out\nWizards of the Coast has Hasbro’s Magic: The Gathering and D&D brands.\nHight reached out in December 2024 and said he was really interested in the team. Hight asked if Asmussen was interested in one of Wizards’ IPs.\n“I dug into that a little bit further, and Dungeons and Dragons totally made a lot of sense. It’s something that I grew up on. It’s something that a lot of people on my team are extremely passionate about,” Asmussen said. “We just jumped into a contract at that point and kept that going. We made a visit Redmond in the Seattle area, to the Wizards headquarters. Got to pick the brains of the creative team there and see what a partnership would be like. And I walked away thoroughly impressed. We really wanted to be a part of collaborating this amazing, legendary license.”\nAsmussen’s team had a pedigree focused on action-adventure RPGs, and so it was working on something that was melee-combat focused, had a robust traversal system.\nAsumssen said, “I certainly don’t want to say that we could skin it into D&D. But there were a lot of elements that we were working on that just seemed like they matched very well.”\nThe wizard’s ambition\nA party in Baldur’s Gate 3.\nThis partnership adds to Wizards of the Coast’s growing lineup of games, which includes both original titles and those based on popular brands. In addition to the Giant Skull project, several other exciting games are in the works across Wizards’ North American studios.\nHight said that Archetype Entertainment in Austin, Texas, is working on Exodus, an epic sci-fi RPG that puts players at the center of an emotional story.\nAnd Atomic Arcade (Raleigh, North Carolina) has released two new images from its first project: a game centered on Snake Eyes, the legendary ninja/commando from G.I. JOE. Invoke (Montreal, Canada) is in full production of another D&D action-adventure game built around magic.\nAlso, Skeleton Key (Austin, Texas) is working on a project that blends suspense, horror, and memorable gameplay experiences. Finally, the Wizards of the Coast team continues to expand Magic: The Gathering Arena with new content and features.\nMonopoly Go teamed up with Marvel.\nThis relatively new team has a lot to live up to. Hasbro is riding high off of hits like Larian’s big D&D hit, Baldur’s Gate 3, as well as Scopely’s Monopoly Go, which has generated more than $5 billion in revenues to date.\n“Awareness for D&D is great. I think the appetite is great. We want to feel to both, you know, the CRPG players, the tabletop players, and just gamers in general, because it’s a wonderful fantasy universe to set games in,” Hight said.\nHight said that the goal with Magic: The Gathering and D&D is essentially to make more people aware of these worlds and bring more players into these communities. He said there are announced games, studios working, and unannounced projects.\nComing together\nGiant Skull started in 2023.\n“With that, there is an open mindedness about how we express that. D&D does not always have to be expressed in a strict computer RPG. Magic doesn’t have to be expressed in strict trading card game. Because the worlds themselves, in the creatures and the villains and the heroes, are the stories that get told in both of those games,” he said. “I think they’re fertile ground to create new things. And when I saw the demo of vertical slice that Stig’s team showed me, I thought this is great.”\nHight added, “It’s one of those things where running around in the world they created was fun. Once you get your hand on the controller — you’ve done this, you know, Dean — you want to play this. They hand you the controller. You look around for a little bit. That’s cool. This is one of those experiences where I couldn’t put it down. Probably played hundreds of demos of action games, combat games. The feel, even in early stages, was so tight and just envisioned if I could have a hero in D&D, or player character in D&D, and running around and battling.”\nHight thought it would be an amazing experience. He also wanted to work with Asmussen again, as they had already gone through building a game together.\n“You have that sort of honest and transparent relationship where you can just cut through, you know, all the BS, and know that you have a shared interest in making something great,” Asmussen said. “As desperately as I wanted to do that, I didn’t want to be heavy handed, and I wanted to give him the opportunity. Is this a fit? Is there a brand that we have that interests you, and even within D&D, I wanted to make sure that he felt like he and the team got a lot of license to make it their own.”\nFor this game idea, D&D made sense while magic wasn’t the same kind of fit.\n“When you make a game, there’s the world, there’s the setting, there’s the hero, there’s the things that the player latches onto,” Asmussen said. “But then there’s everything under the hood, and that’s just, this is how the game controls. This is how the motion model works, this is how the camera system works, this is how the sound can use it. And we have all of those things in place for the type of game that we’re good at making, and translating that to Dungeons and Dragons makes a lot of sense. But there’s still a lot that we have to learn. There’s still a lot that we have to do to really capture that spirit the way that justifies it.”\nBig teams or small teams\nStar Wars: Jedi — Survivor\nYet nobody is really convinced that the future is made up of giant triple-A teams. Asmussen’s team has 35 people now, and it isn’t expecting to grow a lot.\n“We intend to keep the team around that size for quite a bit. There’s no reason to scale if we don’t need to,” Asmussen said. “You want to get to the point where you’ve got a very strong vertical slice. You do several play tests. Once you’re super confident with it, you can make a confident long term schedule. That would affect head count, but we’re not going to get huge.”\nI noted that so many games need to level up now. I wondered if D&D was in that process. There are pressures on studios now. Some need to make players happy and they also need to be less ambitious.\nThat last phrase threw Asmussen off.\n“Did you say less ambitious?” he asked.\nAnd I noted that some teams have gotten too large. The projects go on for years and never end. Then something like Concord happens. So now there is downward pressure on teams, and maybe it’s better to make a 20-hour game than a 50-hour game.\n“I think that’s one of the pressures everybody in the whole industry is feeling now,” I said. “What matters more to you? Level up D&D that is something beyond what Larian did, or think about what are the gamers actually telling people they want?”\nHight didn’t hesitate to answer. He said, “They’re telling us what they have told us. I’ve been a gamer and making games for 30 years. They want great games, whether it’s a big budget game, whether it’s a small, experimental game. They’re looking for innovation. They’re looking for a fun experience.”\nDeliverance\nStig Asmussen is CEO of Giant Skull.\nHight is confident Giant Skull can deliver that.\n“In the case of, you know, working with a team like Giant Skull, they’re going to give us a big game, great execution, wonderful artistry, great storytelling, action — that’s why we signed them up. But I think the main thing is there’s no magic formula. You have to deliver what you set out to do. Make sure there’s a fun aspect to the game. The storytelling is good, the play is good. And then do the best you can. Yeah, I think that’s what people want. They just want great games.”\nAsmussen said, “I think the problem might be that people approach making games like there’s a bunch of boxes you have to check. I think it’s about, like John said, make a good game. You make a game that feels good. You make a game that’s got a soul. Look at Expedition 33: Clair Obscur. It’s got a soul. And I think it’s really important for us all not to lose sight of just that moment to moment feeling when when you’re playing a game. You want to continue to play the game.”\nAs for the approach, Asmussen said he approaches tasks one at a time. As he is doing it, he tries to learn from it and use that to inform him what to do next. Asmussen and Hight talked about production budgets and Asmussen made sure that Hight was OK with making a premium game.\n“We’re comfortable with that,” Hight said. “We certainly have a budget we’re working within, and I think it’s healthy enough to do a pretty, seriously amazing game. So it’s not completely open ended. We’re also not heavily restricted, where, if we discover things that we need to add to the game to make it even better, to build even more players on this journey.”\nAsmussen added, “We don’t mess around. We do due diligence. We make sure that we create a production schedule that makes sense, and it’s based off of real data, data points that we can point out from history, things that we’ve done that informed success moving forward and along the way, as we find out exactly what it is and what it’s becoming.”\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-14T08:46:10.612233",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-14T08:46:24.724912",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 327 credits are required for this request."
  },
  {
    "id": "a0263e188061879411fba596f387faa8",
    "title": "Senator’s RISE Act would require AI developers to list training data, evaluation methods in exchange for ‘safe harbor’ from lawsuits",
    "url": "https://venturebeat.com/ai/senators-rise-act-would-require-ai-developers-to-list-training-data-evaluation-methods-in-exchange-for-safe-harbor-from-lawsuits/",
    "authors": "Carl Franzen",
    "published_date": "2025-06-13T14:59:24+00:00",
    "source": "VentureBeat",
    "summary": "美國參議員提出的RISE法案要求AI開發者公開訓練數據和評估方法，以換取免責權，避免被起訴。這項法案旨在增加透明度，保護消費者權益，並推動AI產業的穩定發展。法案還將對醫生、律師、工程師等專業人士實施傳統的疏失標準。這項提案需要經過眾議院和參議院的多數通過，並經過總統簽署後才能成為法律。",
    "content": "Senator's RISE Act would require AI developers to list training data, evaluation methods in exchange for 'safe harbor' from lawsuits | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nSenator’s RISE Act would require AI developers to list training data, evaluation methods in exchange for ‘safe harbor’ from lawsuits\nCarl Franzen\n@carlfranzen\nJune 13, 2025 7:59 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nAmid an increasingly tense and destabilizing week for international news, it should not escape any technical decision-makers’ notice that some lawmakers in the U.S. Congress are still moving forward with new proposed AI regulations that could reshape the industry in powerful ways — and seek to steady it moving forward.\nCase in point, yesterday,\nU.S. Republican Senator Cynthia Lummis of Wyoming\nintroduced the Responsible Innovation and Safe Expertise Act of 2025 (RISE)\n, the\nfirst stand-alone bill that pairs a conditional liability shield for AI developers with a transparency mandate\non model training and specifications.\nAs with all new proposed legislation, both the U.S. Senate and House would need to vote in the majority to pass the bill and U.S. President Donald J. Trump would need to sign it before it becomes law, a process which would likely take months at the soonest.\n“Bottom line: If we want America to lead and prosper in AI, we can’t let labs write the rules in the shadows,” wrote\nLummis on her account on X when announcing the new bill\n. We need public, enforceable standards that balance innovation with trust. That’s what the RISE Act delivers. Let’s get it done.”\nIt also upholds traditional malpractice standards for doctors, lawyers, engineers, and other “learned professionals.”\nIf enacted as written, the measure would take effect December 1 2025 and apply only to conduct that occurs after that date.\nWhy Lummis says new AI legislation is necessary\nThe bill’s findings section paints a landscape of rapid AI adoption colliding with a patchwork of liability rules that chills investment and leaves professionals unsure where responsibility lies.\nLummis frames her answer as simple reciprocity: developers must be transparent, professionals must exercise judgment, and neither side should be punished for honest mistakes once both duties are met.\nIn a statement on her website,\nLummis calls the measure\n“predictable standards that encourage safer AI development while preserving professional autonomy.”\nWith bipartisan concern mounting over opaque AI systems, RISE gives Congress a concrete template: transparency as the price of limited liability. Industry lobbyists may press for broader redaction rights, while public-interest groups could push for shorter disclosure windows or stricter opt-out limits. Professional associations, meanwhile, will scrutinize how the new documents can fit into existing standards of care.\nWhatever shape the final legislation takes, one principle is now firmly on the table: in high-stakes professions, AI cannot remain a black box. And if the Lummis bill becomes law, developers who want legal peace will have to open that box—at least far enough for the people using their tools to see what is inside.\nHow the new ‘Safe Harbor’ provision for AI developers shielding them from lawsuits works\nRISE offers immunity from civil suits only when a developer meets clear disclosure rules:\nModel card\n– A public technical brief that lays out training data, evaluation methods, performance metrics, intended uses, and limitations.\nModel specification\n– The full system prompt and other instructions that shape model behavior, with any trade-secret redactions justified in writing.\nThe developer must also publish known failure modes, keep all documentation current, and push updates within 30 days of a version change or newly discovered flaw. Miss the deadline—or act recklessly—and the shield disappears.\nProfessionals like doctors, lawyers remain ultimately liable for using AI in their practices\nThe bill does not alter existing duties of care.\nThe physician who misreads an AI-generated treatment plan or a lawyer who files an AI-written brief without vetting it remains liable to clients.\nThe safe harbor is unavailable for non-professional use, fraud, or knowing misrepresentation, and it expressly preserves any other immunities already on the books.\nReaction from AI 2027 project co-author\nDaniel Kokotajlo, policy lead at the nonprofit AI Futures Project and a co-author of the widely circulated scenario planning document\nAI 2027\n, took to\nhis X account\nto state that his team advised Lummis’s office during drafting and “tentatively endorse[s]” the result. He applauds the bill for nudging transparency yet flags three reservations:\nOpt-out loophole.\nA company can simply accept liability and keep its specifications secret, limiting transparency gains in the riskiest scenarios.\nDelay window.\nThirty days between a release and required disclosure could be too long during a crisis.\nRedaction risk.\nFirms might over-redact under the guise of protecting intellectual property; Kokotajlo suggests forcing companies to explain why each blackout truly serves the public interest.\nThe AI Futures Project views RISE as a step forward but not the final word on AI openness.\nWhat it means for devs and enterprise technical decision-makers\nThe RISE Act’s transparency-for-liability trade-off will ripple outward from Congress straight into the daily routines of four overlapping job families that keep enterprise AI running. Start with the lead AI engineers—the people who own a model’s life cycle. Because the bill makes legal protection contingent on publicly posted model cards and full prompt specifications, these engineers gain a new, non-negotiable checklist item: confirm that every upstream vendor, or the in-house research squad down the hall, has published the required documentation before a system goes live. Any gap could leave the deployment team on the hook if a doctor, lawyer, or financial adviser later claims the model caused harm.\nNext come the senior engineers who orchestrate and automate model pipelines. They already juggle versioning, rollback plans, and integration tests; RISE adds a hard deadline. Once a model or its spec changes, updated disclosures must flow into production within thirty days. CI/CD pipelines will need a new gate that fails builds when a model card is missing, out of date, or overly redacted, forcing re-validation before code ships.\nThe data-engineering leads aren’t off the hook, either. They will inherit an expanded metadata burden: capture the provenance of training data, log evaluation metrics, and store any trade-secret redaction justifications in a way auditors can query. Stronger lineage tooling becomes more than a best practice; it turns into the evidence that a company met its duty of care when regulators—or malpractice lawyers—come knocking.\nFinally, the directors of IT security face a classic transparency paradox. Public disclosure of base prompts and known failure modes helps professionals use the system safely, but it also gives adversaries a richer target map. Security teams will have to harden endpoints against prompt-injection attacks, watch for exploits that piggyback on newly revealed failure modes, and pressure product teams to prove that redacted text hides genuine intellectual property without burying vulnerabilities.\nTaken together, these demands shift transparency from a virtue into a statutory requirement with teeth. For anyone who builds, deploys, secures, or orchestrates AI systems aimed at regulated professionals, the RISE Act would weave new checkpoints into vendor due-diligence forms, CI/CD gates, and incident-response playbooks as soon as December 2025.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-14T08:46:10.702776",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-14T08:46:26.302735",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 277 credits are required for this request."
  },
  {
    "id": "98bc781442496c8aa9c16edcba37c9c6",
    "title": "The latest state of the game jobs market | Amir Satvat",
    "url": "https://venturebeat.com/games/the-latest-state-of-the-game-jobs-market-amir-satvat/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-13T13:30:00+00:00",
    "source": "VentureBeat",
    "summary": "這篇新聞講述了遊戲產業就業市場的現況，主要由Amir Satvat提供最新數據。他指出，遊戲相關工作主要集中在擁有5至15年經驗的專業人士身上。然而，對於剛畢業或早期職涯的人來說，找工作的機會相當低，尤其在北美以外更是困難。此外，一些職位如敘事角色和業務拓展的需求遠超供應。總體而言，遊戲業的招聘勢頭保持穩定，但招聘速度已趨於平緩。",
    "content": "The latest state of the game jobs market | Amir Satvat | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nThe latest state of the game jobs market | Amir Satvat\nDean Takahashi\n@deantak\nJune 13, 2025 6:30 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nAmir Satvat has a big community of helpers.\nImage Credit: Amir Satvat\nAmir Satvat\nprovides a lot of job resources for games. He has built a big community of game people, and they are providing him with a lot of data. And here’s the\nlatest data\nfrom Amir Satvat’s Games Community\nand what it says about games hiring today, across functions, experience levels, and regions.\nFirst,\nSatvat\n, who was honored for his work at\nThe Game Awards\n, said in a\nLinkedIn post\nthat hiring remains concentrated in the middle. This means that most roles, and role growth, is aimed at professionals with five to 15 years of experience. That’s where the bulk of open jobs and actual hires (even if the job description says otherwise) are happening.\nSadly, he noted that early career odds remain extremely low. Even if you’re willing to relocate globally, odds for new grads or early career professionals hover around 7% over 12 months. If you’re staying in North America, that drops to 2%. If you’re not in a major North America hub, that falls to 0.3%.\nThis pattern has flattened at 7%.\nHe noted that the categories of jobs are also very different when it comes to demand. Some games areas like narrative roles and business development are dramatically oversubscribed.\n“Right now, we’re tracking 52 writing and narrative games roles globally (28 in North America) and 90 total business development games roles worldwide (just 10 for 10+ years of experience),” Satvat said. “When factoring in students, switchers, or unseen applicants, I can easily believe the demand-to-supply ratio for some functions, like these, is 20-30 times, or more.”\nOverall game hiring momentum\nAmir Satvat is the game jobs champion.\nSatvat said that overall games hiring momentum is stable, but flattened. Games hiring velocity, which was improving a bit, has leveled off, while non-games roles continue rising, especially for adaptable skill sets.\nCareer switchers are intensifying competition.\n“I now have enough data to say with confidence that middle to late career switchers, without any past games experience, are still actively pursuing the industry, further intensifying competition in already crowded functions,” Satvat said.\nAnd he said layoffs may not be the biggest issue going forward.\n“We still forecast 5,000 to 9,000 games layoffs this year. Long-term, global labor cost variances and AI may matter far more, with layoffs becoming a secondary concern,” he said.\nWhat this means for you\nIf you’re a parent or mentor of a young person considering a games career, please be mindful of the data. “Why not try games?” can be a costly mindset if you’re not informed about the odds.\nIf you run a collegiate program, Satvat urges you to be transparent with prospective students. Game design, and subfields like narrative, are among the hardest areas to break into. Unfortunately, these are also the main areas from which graduating students seem to cross his desk. Offer broader skill development.\n“I continue strongly to recommend non-games roles or retraining as a strong path forward, alongside applying to games,” he said.\n✅ For those in games, we must be ready for a future that is likely to include shorter tenures, more project-based work, less remote opportunity, and higher mobility expectations.\n✅ For anyone struggling to find a role in oversubscribed functions like games narrative or business development, please know this is a 20-30x+ structural issue. It’s not about your worth.\nWe’ll keep tracking data and help as best we can.\nNew games role workbook v1.0\nAmir Satvat’s team has a new games role workbook for job seekers.\nSatvat aslo recently announced that a new resource is finally here: the\nNew Games Role Workbook v1.0\n(Resource #8).\n“This is the update I’ve waited three years to give you,” he wrote in a LinkedIn post. “Thanks to collaboration with Mayank Grover and the stellar team at Outscal, we have an improved resource of games and tech roles that will be refreshed twice a week, covering nearly 40,000 roles every three-month cycle, now delivered eight times a month.”\nWhy twice a week? Because after months of research, he found the critical window for applying to roles is within the first seven days. Anything slower was just not fast enough.\nBut there’s more. The raw data Mayank’s team pulls comes from many sources. So, just like he did for the original Games Jobs Workbook, he spent months in the background building a system to standardize all roles into 25 categories, based on community feedback and refined for usability.\nThey are:\nAccount Management\nAdministrative Support\nAnimation & Cinematics\nArt & Tech Art\nBusiness Development & Sales\nCustomer & Community Support\nData & Analytics\nDesign & UX\nEngineering & Development\nFacilities & Maintenance\nFinance & Accounting\nGeneral & Miscellaneous\nHR & Recruiting\nInternship\nIT & Security\nLegal & Compliance\nLocalization & Translation\nMarketing & Advertising\nOperations & Admin\nProduction & Product\nProject & Program Management\nStrategy & Consulting\nTechnical Support\nUser Research\nWriting & Narrative\nThis is standardized across all 38,000+ roles, both games and tech.\nThat means job seekers can now filter jobs easily across a consistent, logical set of categories. Every job has a direct link to apply, fully searchable, and structured to support your success.\n“I’ll continue maintaining the original games jobs workbook as an encyclopedic view: total jobs by company, industry-wide scope, and macro stats. I will use this data to help Mayank ensure we have all companies tracked too,” he said.\nBut this new workbook is, now, what he recommends using for active job hunting. This is because the team has finally solved (thanks to Mayank’s team) the frequency problem and (with my efforts) the categorization problem that allows equivalent functionality to the Games Jobs Workbook\nA resource with fresh roles updated twice a week, now with categorization, smart filters, games and tech roles, and full apply links at a role and location line item level?\nHe offered his deepest thanks to Mayank Grover and the Outscal team for this incredible collaboration. This wouldn’t be possible without them.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-14T08:46:10.787195",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-14T08:46:27.730357",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 221 credits are required for this request."
  },
  {
    "id": "4a16152ac41063b8e1035fdf51e44b70",
    "title": "Red team AI now to build safer, smarter models tomorrow",
    "url": "https://venturebeat.com/security/red-team-ai-now-to-build-safer-smarter-models-tomorrow/",
    "authors": "Louis Columbus",
    "published_date": "2025-06-13T13:00:00+00:00",
    "source": "VentureBeat",
    "summary": "未來AI模型面臨安全威脅，攻擊者技術超前。為了建立更安全、更智慧的模型，必須從今天開始進行紅隊測試，不斷挑戰模型的安全性。透過持續的對抗性測試，將安全整合到軟體開發週期中，以保護大型語言模型。這種全新的安全整合方式將有助於降低模型遭受攻擊的風險，並確保敏感資料不被外洩。",
    "content": "Red Team AI now to build safer, smarter models tomorrow | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nRed team AI now to build safer, smarter models tomorrow\nLouis Columbus\n@LouisColumbus\nJune 13, 2025 6:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nEditor’s note: Louis will lead an editorial roundtable on this topic at VB Transform this month.\nRegister today\n.\nAI models are under siege. With\n77%\nof enterprises already hit by adversarial model attacks and\n41%\nof those attacks exploiting prompt injections and data poisoning, attackers’ tradecraft is outpacing existing cyber defenses.\nTo reverse this trend, it’s critical to rethink how security is integrated into the models being built today. DevOps teams need to shift from taking a reactive defense to continuous adversarial testing at every step.\nRed Teaming needs to be the core\nProtecting large language models (LLMs) across DevOps cycles requires red teaming as a core component of the model-creation process. Rather than treating security as a final hurdle, which is typical in web app pipelines, continuous adversarial testing needs to be integrated into every phase of the Software Development Life Cycle (SDLC).\nGartner’s Hype Cycle emphasizes the rising importance of continuous threat exposure management (CTEM), underscoring why red teaming must integrate fully into the DevSecOps lifecycle.\nSource: Gartner,\nHype Cycle for Security Operations, 2024\nAdopting a more integrative approach to DevSecOps fundamentals is becoming necessary to mitigate the growing risks of prompt injections, data poisoning and the exposure of sensitive data. Severe attacks like these are becoming more prevalent, occurring from model design through deployment, making ongoing monitoring essential.\nMicrosoft’s recent guidance on\nplanning\nred teaming for large language models (LLMs)\nand their applications provides a valuable methodology for starting\nan integrated process.\nNIST’s AI Risk Management Framework\nreinforces this, emphasizing the need for a more proactive, lifecycle-long approach to adversarial testing and risk mitigation. Microsoft’s recent red teaming of over 100 generative AI products underscores the need to integrate automated threat detection with expert oversight throughout model development.\nAs regulatory frameworks, such as the EU’s AI Act, mandate rigorous adversarial testing, integrating continuous red teaming ensures compliance and enhanced security.\nOpenAI’s\napproach to red teaming\nintegrates external red teaming from early design through deployment, confirming that consistent, preemptive security testing is crucial to the success of LLM development.\nGartner’s framework shows the structured maturity path for red teaming, from foundational to advanced exercises, essential for systematically strengthening AI model defenses.\nSource: Gartner,\nImprove Cyber Resilience by Conducting Red Team Exercises\nWhy traditional cyber defenses fail against AI\nTraditional, longstanding cybersecurity approaches fall short against AI-driven threats because they are fundamentally different from conventional attacks. As adversaries’ tradecraft surpasses traditional approaches, new techniques for red teaming are necessary. Here’s a sample of the many types of tradecraft specifically built to attack AI models throughout the DevOps cycles and once in the wild:\nData Poisoning\n: Adversaries inject corrupted data into training sets, causing models to learn incorrectly and creating persistent inaccuracies and operational errors until they are discovered. This often undermines trust in AI-driven decisions.\nModel Evasion:\nAdversaries introduce carefully crafted, subtle input changes, enabling malicious data to slip past detection systems by exploiting the inherent limitations of static rules and pattern-based security controls.\nModel Inversion\n: Systematic queries against AI models enable adversaries to extract confidential information, potentially exposing sensitive or proprietary training data and creating ongoing privacy risks.\nPrompt Injection:\nAdversaries craft inputs specifically designed to trick generative AI into bypassing safeguards, producing harmful or unauthorized results.\nDual-Use Frontier Risks:\nIn the recent paper,\nBenchmark Early and Red Team Often: A Framework for Assessing and Managing Dual-Use Hazards of AI Foundation Models\n, researchers from\nThe Center for Long-Term Cybersecurity at the University of California, Berkeley\nemphasize that advanced AI models significantly lower barriers, enabling non-experts to carry out sophisticated cyberattacks, chemical threats, or other complex exploits, fundamentally reshaping the global threat landscape and intensifying risk exposure.\nIntegrated Machine Learning Operations (MLOps) further compound these risks, threats, and vulnerabilities. The interconnected nature of LLM and broader AI development pipelines magnifies these attack surfaces, requiring improvements in red teaming.\nCybersecurity leaders are increasingly adopting continuous adversarial testing to counter these emerging AI threats. Structured red-team exercises are now essential, realistically simulating AI-focused attacks to uncover hidden vulnerabilities and close security gaps before attackers can exploit them.\nHow AI leaders stay ahead of attackers with red teaming\nAdversaries continue to accelerate their use of AI to create entirely new forms of tradecraft that defy existing, traditional cyber defenses. Their goal is to exploit as many emerging vulnerabilities as possible.\nIndustry leaders, including the major AI companies, have responded by embedding systematic and sophisticated red-teaming strategies at the core of their AI security. Rather than treating red teaming as an occasional check, they deploy continuous adversarial testing by combining expert human insights, disciplined automation, and iterative human-in-the-middle evaluations to uncover and reduce threats before attackers can exploit them proactively.\nTheir rigorous methodologies allow them to identify weaknesses and systematically harden their models against evolving real-world adversarial scenarios.\nSpecifically:\nAnthropic relies on rigorous human insight as part of its ongoing red-teaming methodology.\nBy tightly integrating human-in-the-loop evaluations with automated adversarial attacks, the company proactively identifies vulnerabilities and continually refines the reliability, accuracy and interpretability of its models.\nMeta scales AI model security through automation-first adversarial testing.\nIts Multi-round Automatic Red-Teaming (MART) systematically generates iterative adversarial prompts, rapidly uncovering hidden vulnerabilities and efficiently narrowing attack vectors across expansive AI deployments.\nMicrosoft harnesses interdisciplinary collaboration as the core of its red-teaming strength.\nUsing its Python Risk Identification Toolkit (PyRIT), Microsoft bridges cybersecurity expertise and advanced analytics with disciplined human-in-the-middle validation, accelerating vulnerability detection and providing detailed, actionable intelligence to fortify model resilience.\nOpenAI taps global security expertise to fortify AI defenses at scale.\nCombining external security specialists’ insights with automated adversarial evaluations and rigorous human validation cycles, OpenAI proactively addresses sophisticated threats, specifically targeting misinformation and prompt-injection vulnerabilities to maintain robust model performance.\nIn short, AI leaders know that staying ahead of attackers demands continuous and proactive vigilance. By embedding structured human oversight, disciplined automation, and iterative refinement into their red teaming strategies, these industry leaders set the standard and define the playbook for resilient and trustworthy AI at scale.\nGartner outlines how adversarial exposure validation (AEV) enables optimized defense, better exposure awareness, and scaled offensive testing—critical capabilities for securing AI models.\nSource: Gartner,\nMarket Guide for Adversarial Exposure Validation\nFive strategies to immediately strengthen AI security\nAs attacks on LLMs and AI models continue to evolve rapidly, DevOps and DevSecOps teams must coordinate their efforts to address the challenge of enhancing AI security. VentureBeat is finding the following five high-impact strategies security leaders can implement right away:\nIntegrate security early (Anthropic, OpenAI)\nBuild adversarial testing directly into the initial model design and throughout the entire lifecycle. Catching vulnerabilities early reduces risks, disruptions and future costs.\nDeploy adaptive, real-time monitoring (Microsoft)\nStatic defenses can’t protect AI systems from advanced threats. Leverage continuous AI-driven tools like CyberAlly to detect and respond to subtle anomalies quickly, minimizing the exploitation window.\nBalance automation with human judgment (Meta, Microsoft)\nPure automation misses nuance; manual testing alone won’t scale. Combine automated adversarial testing and vulnerability scans with expert human analysis to ensure precise, actionable insights.\nRegularly engage external red teams (OpenAI)\nInternal teams develop blind spots. Periodic external evaluations reveal hidden vulnerabilities, independently validate your defenses and drive continuous improvement.\nMaintain dynamic threat intelligence (Meta, Microsoft, OpenAI)\nAttackers constantly evolve tactics. Continuously integrate real-time threat intelligence, automated analysis and expert insights to update and strengthen your defensive posture proactively.\nTaken together, these strategies ensure DevOps workflows remain resilient and secure while staying ahead of evolving adversarial threats.\nRed teaming is no longer optional; it’s essential\nAI threats have grown too sophisticated and frequent to rely solely on traditional, reactive cybersecurity approaches. To stay ahead, organizations must continuously and proactively embed adversarial testing into every stage of model development. By balancing automation with human expertise and dynamically adapting their defenses, leading AI providers prove that robust security and innovation can coexist.\nUltimately, red teaming isn’t just about defending AI models. It’s about ensuring trust, resilience, and confidence in a future increasingly shaped by AI.\nJoin me at Transform 2025\nI’ll be hosting two cybersecurity-focused roundtables at VentureBeat’s\nTransform 2025\n, which will be held June 24–25 at Fort Mason in San Francisco. Register to join the conversation.\nMy session will include one on red teaming,\nAI Red Teaming and Adversarial Testing\n, diving into strategies for testing and strengthening AI-driven cybersecurity solutions against sophisticated adversarial threats.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-14T08:46:10.921113",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-14T08:46:29.187428",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 191 credits are required for this request."
  },
  {
    "id": "da07b9df3177d2492708a4da3f42d88a",
    "title": "Gamefam brings FIFA Club World Cup 2025 to Roblox",
    "url": "https://venturebeat.com/games/gamefam-brings-fifa-club-world-cup-2025-to-roblox/",
    "authors": "Rachel Kaser",
    "published_date": "2025-06-13T13:00:00+00:00",
    "source": "VentureBeat",
    "summary": "Gamefam與FIFA合作，在Roblox的遊戲Super League Soccer中舉辦FIFA Club World Cup 2025，讓玩家可以操控13支參賽足球隊。活動將於6月14日開始，持續至7月13日，同步進行真實的Club World Cup。這是他們第二次合作，旨在吸引Roblox年輕玩家，提升足球賽事在遊戲中的知名度。透過遊戲內的廣告、贊助商商品和真實比賽更新，讓玩家身歷其境。Gamefam表示，這次合作不僅是行銷活動，更是文化時刻，希望重新定義年輕世代對足球的熱愛和體驗。",
    "content": "Gamefam brings FIFA Club World Cup 2025 to Roblox | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nGamefam brings FIFA Club World Cup 2025 to Roblox\nRachel Kaser\n@rachelkaser\nJune 13, 2025 6:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage Credit: Gamefam\nRoblox game studio\nGamefam\nannounced today it is collaborating with FIFA to bring the FIFA Club World Cup 2025 to its game Super League Soccer. The two are holding a major event within the game leading up to the Club World Cup to raise the tournament’s profile with Roblox’s Gen Z and Alpha-aged audience. All 13 of the participating football clubs are playable in Roblox for the first time, with the event set to kick off on June 14 and running through July 13 alongside the real Club World Cup.\nAccording to Gamefam, the virtual Club World Cup will mimic the real deal, with in-game ads and signage to show FIFA’s sponsors and virtual merch for players like branded items and cosmetics from both FIFA and Adidas. It will also follow the Club World Cup, with the in-game bracket updated as the real matches are completed.\nRicardo Briceno, Gamefam’s Chief Business Officer, told GamesBeat in an interview: “Working with FIFA isn’t just another brand activation for us. It’s uniquely meaningful. At Gamefam, we regularly have the privilege of collaborating with leading global IPs, but FIFA stands apart as the pinnacle of both global football and global sport. The weight of that legacy and the passion behind it make this partnership feel more like a cultural moment than a marketing campaign. This collaboration carries a depth and resonance that’s rare on Roblox, and it reflects our shared goal with FIFA: to redefine how the next generations fall in love with and experience the beautiful game.”\nBringing the Club World Cup to Roblox’s young audience\nThis is the second collaboration between Gamefam and FIFA. The first was in November, where they revealed the virtual Club World Cup Trophy reveal, which received 5.5 million visits and 70 million minutes of engagement in three days — the biggest soccer event in Roblox, according to the developer. 84% of players who attended the event said they planned to follow the tournament after this, and the Club World Cup activation gives them a chance to do so within Roblox.\nIn addition to the sponsors and the connection to the real-world tournament, the virtual Club World Cup gives players a chance to participate in their own tournament. They can earn points both by winning and by successfully completing challenges.\nBriceno added, “With Gen Z & Alpha spending an average of 2.4 hours per day on Roblox, it’s no surprise that Roblox has become a magnet for sports IP: the NBA, NFL, NASCAR, and now FIFA have all recognized its potential to engage the next wave of fans. Moral of the story is… sports properties must activate on Roblox to ensure relevance for decades to come.”\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-14T08:46:11.069865",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-14T08:46:32.896122",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 300 credits are required for this request."
  },
  {
    "id": "41ea6141f6a835a6d63ee837d3a16fe3",
    "title": "SAG-AFTRA board approves agreement with game companies on AI and new contract",
    "url": "https://venturebeat.com/games/sag-aftra-board-approves-agreement-with-game-companies-on-ai-and-new-contract/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-13T00:04:00+00:00",
    "source": "VentureBeat",
    "summary": "SAG-AFTRA的董事會批准了與遊戲公司達成的協議，涉及AI和新合約。新合約包括了關於AI使用的重要規定，並提高了表演者的報酬。若獲通過，表演者的報酬將在幾年內逐步增加，同時也確保了安全條款，例如在危險情況下需要有合格醫務人員在場。這項協議將對遊戲行業的工作條件和表演者的權益有所改善。",
    "content": "SAG-AFTRA board approves agreement with game companies on AI and new contract | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSAG-AFTRA board approves agreement with game companies on AI and new contract\nDean Takahashi\n@deantak\nJune 12, 2025 5:04 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nSAG-AFTRA's board has approved a deal with game companies, setting up membership vote.\nImage Credit: SAG-AFTRA\nThe\nScreen Actors Guild-American Federation of Television and Radio Artists\n(SAG-AFTRA) National Board approved the tentative agreement with the video game bargaining group.\nThe contract on terms for the Interactive Media Agreement will now be submitted to the membership for ratification.\nThe new contract accomplishes important guardrails and gains around AI, including the requirement of informed consent across various AI uses and the ability for performers to suspend informed consent for Digital Replica use during a strike.\nIf ratified, the agreement would provide compounded increases in performer compensation at a rate of 15.17% upon ratification plus additional 3% increases in November 2025, November 2026 and November 2027. Additionally, the overtime rate maximum for overscale performers will now be based on double scale. The health & retirement contribution rates to the SAG-AFTRA Health Plan will be raised from 16.5% to 17% upon ratification and to 17.5% in Oct. 2026.\nCompensation gains include the establishment of collectively-bargained minimums for the use of Digital Replicas created with IMA-covered performances and higher minimums (7.5x scale) for “Real Time Generation,” i.e., embedding a Digital Replica-voiced chatbot in a video game. “Secondary Performance Payments” will also ensure compensation when visual performances are re-used in another videogame.\nEssential new safety provisions were also secured, including a requirement for a qualified medical professional to be present or readily available at rehearsals and performances during which hazardous actions or working conditions are planned. Rest periods are now provided for on-camera principal performers and employers can no longer request that performers complete stunts or other dangerous activity in virtual auditions.\nThe spokesperson for the video game producers party to the Interactive Media Agreement, Audrey Cooling, said earlier this week in a statement, “We are pleased to have reached a tentative contract agreement that reflects the important contributions of SAG-AFTRA-represented performers in video games. This agreement builds on three decades of successful partnership between the interactive entertainment industry and the union.”\nCooling added, “It delivers historic wage increases of over 24% for performers, enhanced health and safety protections, and industry-leading AI provisions requiring transparency, consent and compensation for the use of digital replicas in games. We look forward to continuing to work with performers to create new and engaging entertainment experiences for billions of players throughout the world.”\nThe full terms of the three-year deal will be released with the ratification materials on Wednesday, June 18.\nA tentative agreement was reached with the video game employers on June 9 and the strike was officially suspended on June 11.\nMember informational meetings are being scheduled and additional details will be available at\nsagaftra.org/videogames2025\nin the coming days.\nEligible SAG-AFTRA members will have until 5 p.m. PDT on Wednesday, July 9, 2025 to cast their vote on ratification.\nSAG-AFTRA represents approximately 160,000 actors, announcers, broadcast journalists, dancers, DJs, news writers, news editors, program hosts, puppeteers, recording artists, singers, stunt performers, voiceover artists and other entertainment and media professionals.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-14T08:46:11.173688",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-14T08:46:35.053998",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 222 credits are required for this request."
  }
]