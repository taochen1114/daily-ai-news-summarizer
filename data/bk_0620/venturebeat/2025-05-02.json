[
  {
    "id": "29dbdc643db9e0e93ab47b7b8d12b372",
    "title": "Hidden costs in AI deployment: Why Claude models may be 20-30% more expensive than GPT in enterprise settings",
    "url": "https://venturebeat.com/ai/hidden-costs-in-ai-deployment-why-claude-models-may-be-20-30-more-expensive-than-gpt-in-enterprise-settings/",
    "authors": "Lavanya Gupta",
    "published_date": "Thu, 01 May 2025 20:14:04 +0000",
    "source": "VentureBeat AI",
    "summary": "這篇新聞談到在企業AI應用中，Anthropic的Claude模型可能比OpenAI的GPT模型貴20-30%，原因在於Claude模型的tokenizer效率較低，導致同樣的輸入會被分成更多的token，增加成本。儘管Claude的輸入token價格較低，但總體運行成本仍較高。這提醒企業在部署AI模型時要留意隱藏的成本，選擇適合且經濟有效的模型。",
    "content": "Hidden costs in AI deployment: Why Claude models may be 20-30% more expensive than GPT in enterprise settings | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nHidden costs in AI deployment: Why Claude models may be 20-30% more expensive than GPT in enterprise settings\nLavanya Gupta\nMay 1, 2025 1:14 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat using DALL-E 3\nIt is a well-known fact that different model families can use different tokenizers. However, there has been limited analysis on how the process of\n“\ntokenization\n”\nitself varies across these tokenizers. Do all tokenizers result in the same number of tokens for a given input text? If not, how different are the generated tokens? How significant are the differences?\nIn this article, we explore these questions and examine the practical implications of tokenization variability. We present a comparative story of two frontier model families:\nOpenAI\n’s ChatGPT vs\nAnthropic\n’s Claude. Although their advertised “cost-per-token” figures are highly competitive, experiments reveal that Anthropic models can be 20–30% more expensive than GPT models.\nAPI Pricing — Claude 3.5 Sonnet vs GPT-4o\nAs of June 2024, the pricing structure for these two advanced frontier models is highly competitive. Both Anthropic’s Claude 3.5 Sonnet and OpenAI’s GPT-4o have identical costs for output tokens, while Claude 3.5 Sonnet offers a 40% lower cost for input tokens.\nSource:\nVantage\nThe hidden “tokenizer inefficiency”\nDespite lower input token rates of the Anthropic model, we observed that the total costs of running experiments (on a given set of fixed prompts) with GPT-4o is much cheaper when compared to Claude Sonnet-3.5.\nWhy?\nThe Anthropic tokenizer tends to break down the same input into more tokens compared to OpenAI’s tokenizer.\nThis means that, for identical prompts, Anthropic models produce considerably more tokens than their OpenAI counterparts. As a result, while the per-token cost for Claude 3.5 Sonnet’s input may be lower, the increased tokenization can offset these savings, leading to higher overall costs in practical use cases.\nThis hidden cost stems from the way Anthropic’s tokenizer encodes information, often using more tokens to represent the same content. The token count inflation has a significant impact on costs and context window utilization.\nDomain-dependent tokenization inefficiency\nDifferent types of domain content are tokenized differently by Anthropic’s tokenizer, leading to varying levels of increased token counts compared to OpenAI’s models. The AI research community has noted similar tokenization differences\nhere\n. We tested our findings on three popular domains, namely: English articles, code (Python) and math.\nDomain\nModel Input\nGPT Tokens\nClaude Tokens\n% Token Overhead\nEnglish articles\n77\n89\n~16%\nCode (Python)\n60\n78\n~30%\nMath\n114\n138\n~21%\n% Token Overhead of Claude 3.5 Sonnet Tokenizer (relative to GPT-4o) Source: Lavanya Gupta\nWhen comparing Claude 3.5 Sonnet to GPT-4o, the degree of tokenizer inefficiency varies significantly across content domains. For English articles, Claude’s tokenizer produces approximately 16% more tokens than GPT-4o for the same input text. This overhead increases sharply with more structured or technical content: for mathematical equations, the overhead stands at 21%, and for Python code, Claude generates 30% more tokens.\nThis variation arises because some content types, such as technical documents and code, often contain patterns and symbols that Anthropic’s tokenizer fragments into smaller pieces, leading to a higher token count. In contrast, more natural language content tends to exhibit a lower token overhead.\nOther practical implications of tokenizer inefficiency\nBeyond the direct implication on costs, there is also an indirect impact on the context window utilization.  While Anthropic models claim a larger context window of 200K tokens, as opposed to OpenAI’s 128K tokens, due to verbosity, the effective usable token space may be smaller for Anthropic models. Hence, there could potentially be a small or large difference in the “advertised” context window sizes vs the “effective” context window sizes.\nImplementation of tokenizers\nGPT models use\nByte Pair Encoding (BPE\n)\n, which merges frequently co-occurring character pairs to form tokens. Specifically, the latest GPT models use the open-source o200k_base tokenizer. The actual tokens used by GPT-4o (in the tiktoken tokenizer) can be viewed\nhere\n.\nJSON\n{\n#reasoning\n\"o1-xxx\": \"o200k_base\",\n\"o3-xxx\": \"o200k_base\",\n# chat\n\"chatgpt-4o-\": \"o200k_base\",\n\"gpt-4o-xxx\": \"o200k_base\",\n# e.g., gpt-4o-2024-05-13\n\"gpt-4-xxx\": \"cl100k_base\",\n# e.g., gpt-4-0314, etc., plus gpt-4-32k\n\"gpt-3.5-turbo-xxx\": \"cl100k_base\",\n# e.g, gpt-3.5-turbo-0301, -0401, etc.\n}\nUnfortunately, not much can be said about Anthropic tokenizers as their tokenizer is not as directly and easily available as GPT. Anthropic\nreleased their Token Counting API in Dec 2024\n. However, it was soon demised in later 2025 versions.\nLatenode\nreports that “Anthropic uses a unique tokenizer with only 65,000 token variations, compared to OpenAI’s 100,261 token variations for GPT-4.” This\nColab notebook\ncontains Python code to analyze the tokenization differences between GPT and Claude models. Another\ntool\nthat enables interfacing with some common, publicly available tokenizers validates our findings.\nThe ability to proactively estimate token counts (without invoking the actual model API) and budget costs is crucial for AI enterprises.\nKey Takeaways\nAnthropic’s competitive pricing comes with hidden costs:\nWhile Anthropic’s Claude 3.5 Sonnet offers 40% lower input token costs compared to OpenAI’s GPT-4o, this apparent cost advantage can be misleading due to differences in how input text is tokenized.\nHidden “tokenizer inefficiency”:\nAnthropic models are inherently more\nverbose\n. For businesses that process large volumes of text, understanding this discrepancy is crucial when evaluating the true cost of deploying models.\nDomain-dependent tokenizer inefficiency:\nWhen choosing between OpenAI and Anthropic models,\nevaluate the nature of your input text\n. For natural language tasks, the cost difference may be minimal, but technical or structured domains may lead to significantly higher costs with Anthropic models.\nEffective context window:\nDue to the verbosity of Anthropic’s tokenizer, its larger advertised 200K context window may offer less effective usable space than OpenAI’s 128K, leading to a\npotential\ngap between advertised and actual context window\n.\nAnthropic did not respond to VentureBeat’s requests for comment by press time. We’ll update the story if they respond.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-05-02T13:45:23.677592",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-02T13:45:35.485927",
    "audio_file": "29dbdc643db9e0e93ab47b7b8d12b372.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/29dbdc643db9e0e93ab47b7b8d12b372.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-02T13:45:46.773133"
  },
  {
    "id": "0be900b8a90caabcc322a5791fc3ac59",
    "title": "Astronomer’s $93M raise underscores a new reality: Orchestration is king in AI infrastructure",
    "url": "https://venturebeat.com/ai/astronomer-93m-raise-underscores-a-new-reality-orchestration-is-king-in-ai-infrastructure/",
    "authors": "Michael Nuñez",
    "published_date": "Thu, 01 May 2025 13:00:00 +0000",
    "source": "VentureBeat AI",
    "summary": "最新消息指出，Astronomer公司籌得9300萬美元資金，強調在AI基礎設施中，管控是王道。他們的資金將用於研發和擴展業務，特別是在歐洲、澳洲和紐西蘭地區。企業正積極尋求通過更好地管理數據管道來實現AI計劃。數據管控被視為成功部署AI的關鍵，幫助企業克服技術和組織障礙。",
    "content": "Astronomer’s $93M raise underscores a new reality: Orchestration is king in AI infrastructure | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nAstronomer’s $93M raise underscores a new reality: Orchestration is king in AI infrastructure\nMichael Nuñez\n@MichaelFNunez\nMay 1, 2025 6:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nAstronomer\n, the company behind the Apache Airflow-powered data orchestration platform Astro, has secured $93 million in Series D funding as enterprises increasingly seek to operationalize AI initiatives through better management of their data pipelines.\nThe funding round was led by\nBain Capital Ventures\n, with participation from\nSalesforce Ventures\nand existing investors including\nInsight\n,\nMeritech\n, and\nVenrock\n.\nBosch Ventures\nis also seeking to participate in the round, reflecting industrial interest in the technology.\nIn an exclusive interview with VentureBeat, Astronomer CEO Andy Byron explained that the company will use the funding to expedite research and development efforts and expand its global footprint, particularly in Europe, Australia, and New Zealand.\n“For us, this is just a step along the way,” Byron said. “We want to build something awesome here. I couldn’t be more excited about our venture partners, our customers, our product vision, which I think is super strong in going after collapsing the data ops market.”\nHow data orchestration became the hidden key to enterprise AI success\nThe funding targets what industry analysts have identified as the “AI implementation gap” — the significant technical and organizational hurdles that prevent companies from deploying AI at scale. Data orchestration, the process of automating and coordinating complex data workflows across disparate systems, has become an essential component of successful AI deployments.\nEnrique Salem\n, Partner at Bain Capital Ventures, explained the critical challenges facing enterprises today: “Every company operates a sprawling, fragmented data ecosystem—using a patchworks of tools, teams, and workflows that struggle to deliver reliable insights, creating operational bottlenecks and limiting agility. At the heart of this complexity is orchestration—the layer that coordinates all these moving pieces.”\nSalem noted that despite its importance, “today’s orchestration landscape is where cloud infrastructure was 15 years ago: mission critical, yet fragmented, brittle and often built in-house with limited scalability. Data engineers spend more time maintaining pipelines than driving innovation. Without robust orchestration, data is unreliable, agility is lost, and businesses fall behind.”\nThe company’s platform,\nAstro\n, is built on Apache Airflow, an open-source framework that has seen explosive growth. According to the company’s recently released\nState of Airflow 2025 report\n, which surveyed over 5,000 data practitioners, Airflow was downloaded more than 324 million times in 2024 alone — more than all previous years combined.\n“Airflow has established itself as the proven de facto standard for data pipeline orchestration,” Astronomer SVP of Marketing Mark Wheeler explained. “When we look at the competitive landscape in the orchestration layer, Airflow has clearly emerged as the standard solution for moving modern data efficiently from source to destination.”\nFrom invisible plumbing to enterprise AI backbone: The evolution of data infrastructure\nAstronomer’s growth reflects a transformative shift in how enterprises view data orchestration — from hidden backend infrastructure to mission-critical technology that enables AI initiatives and drives business value.\n“BCV’s belief in Astronomer goes way back. We invested in the company’s seed round in 2019 and have supported the company over the years, now culminating in leading their Series D,” Salem said. “Beyond the impressive growth, Astronomer’s data orchestration has become even more important in the age of AI, which requires scalable orchestration and model deployment automation amidst a ballooning sea of data tools that don’t talk to each other.”\nAccording to the company’s internal data, 69% of customers who have used its platform for two or more years are using Airflow for AI and machine learning applications. This adoption rate is significantly higher than the broader Airflow community, suggesting that Astronomer’s managed service accelerates enterprise AI deployments.\nThe company has seen 150% year-over-year growth in Astro (managed SaaS platform) annual recurring revenue and boasts a 130% net revenue retention rate, indicating strong customer expansion.\n“While market analysts may be looking for a clear winner in the cloud data platforms battle, enterprises have clearly chosen a multi-solution strategy—just like they earlier determined that multi-cloud would far outpace standardization on any single cloud provider,” Wheeler explained. “Leading enterprises refuse to lock into a single vendor, opting for multi-cloud and diverse data platform approaches to stay agile and take advantage of the latest innovations.”\nInside Ford’s massive AI operation: How petabytes of weekly data power next-generation vehicles\nMajor enterprises are already leveraging Astronomer’s platform for sophisticated AI use cases that would be challenging to implement without robust orchestration.\nAt\nFord Motor Company\n, Astronomer’s platform powers the company’s Advanced Driver Assistance Systems (ADAS) and its multi-million dollar “\nMach1ML\n” machine learning operations platform.\nThe automotive giant processes more than one petabyte of data weekly and runs over 300 parallel workflows, balancing CPU- and GPU-intensive tasks for AI model development across a hybrid public/private cloud platform. These workflows power everything from autonomous driving systems to Ford’s specialized FordLLM platform for large language models.\nFord initially built its MLOps platform using\nKubeflow\nfor orchestration but encountered significant challenges, including a steep learning curve and tight integration with\nGoogle Cloud\n, which limited flexibility. After transitioning to Airflow for Mach1ML 2.0, Ford reports dramatically streamlined workflows and seamless integration across on-premises, cloud, and hybrid environments.\nFrom AI experiments to production: How orchestration bridges the implementation divide\nA common challenge for enterprises is moving AI from proof-of-concept to production. According to Astronomer’s research, organizations that establish strong data orchestration foundations are more successful at operationalizing AI.\n“As more enterprises are running ML workflows and real-time AI pipelines, they require scalable orchestration and model deployment automation,” Salem explained. “Astronomer delivers on this today, and as the orchestrator, is the one system that sees everything happening across the stack — when data moves, when transformations run, when models are trained.”\nOver 85% of Airflow users surveyed expect an increase in external-facing or revenue-generating solutions built on Airflow in the next year, highlighting how data orchestration is increasingly powering customer-facing applications rather than just internal analytics.\nThis trend is evident across industries, from automotive to legal technology companies that are building specialized AI models to automate professional workflows. These organizations are turning to Astronomer to handle the complex orchestration challenges that arise when scaling AI systems from prototypes to production environments serving thousands of users.\nStrategic technology expansion: Airflow 3.0 and cloud partnerships position Astronomer for market leadership\nThe company recently announced the general availability of\nAirflow 3.0\n, which it describes as “the most significant release in Airflow’s history.” The update introduces several transformative capabilities designed specifically for AI workloads, including the ability to run tasks “anywhere, any time, in any language.”\n“Airflow 3.0 lays the foundation for executing tasks on any machine, on-prem or in the cloud, triggered by events across the data ecosystem,” Byron explained. “It also introduces a proof of concept for defining tasks in languages beyond Python, greatly improving data team agility and facilitating migration from legacy systems to Airflow.”\nAstronomer has also expanded its industry partnerships, recently achieving the Google Cloud Ready – BigQuery Designation, making its platform available for purchase directly from the\nGoogle Cloud Marketplace\n. This allows existing Google Cloud customers to expedite their purchase of Astro and use their existing Google Cloud commit credits.\n“We’ve just signed an awesome partnership with IBM,” Byron told VentureBeat. “They’re putting us into their broader data portfolio of products. And we think there’s an awesome opportunity for us, not only in North America, but internationally, to get a lot of momentum with IBM as well.”\nUnified DataOps: The next evolution in enterprise data management\nSalem believes Astronomer is positioned to redefine enterprise data operations, moving beyond orchestration to what the company calls “unified DataOps” — a comprehensive approach integrating observability, quality management, and governance into a single platform.\n“We invested in Astronomer in 2019 with a simple bet: Airflow would become the standard for data orchestration,” Salem said. “Today, it runs at over 80,000 companies and drives 30 million downloads a month. We backed Astronomer because they’re not only riding that wave; they’re building the enterprise control plane on top of it.”\nFor enterprises struggling to realize value from their AI investments, Astronomer’s growth signals a crucial shift in how data infrastructure is built and managed — one where orchestration serves as the foundation for the entire data stack.\n“As AI raises the stakes for reliable, scalable data infrastructure, we’re doubling down on our investment,” Salem concluded. “Orchestration is just the start. The team at Astronomer are poised to unify the entire DataOps stack.”\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Tour Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Tour. See if we're coming to your area!\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-05-02T13:45:24.041712",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-02T13:45:37.944038",
    "audio_file": "0be900b8a90caabcc322a5791fc3ac59.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/0be900b8a90caabcc322a5791fc3ac59.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-02T13:45:47.992100"
  },
  {
    "id": "f20d263c2063d01fbea44ba6515ad649",
    "title": "Microsoft launches Phi-4-Reasoning-Plus, a small, powerful, open weights reasoning model!",
    "url": "https://venturebeat.com/ai/microsoft-launches-phi-4-reasoning-plus-a-small-powerful-open-weights-reasoning-model/",
    "authors": "Carl Franzen",
    "published_date": "Thu, 01 May 2025 12:41:29 +0000",
    "source": "VentureBeat AI",
    "summary": "微軟推出了Phi-4-Reasoning-Plus，這是一個小巧、強大、開放權重的推理模型。這個模型在數學、科學、編碼和邏輯任務上表現出色，並且比大型模型表現更優秀。Phi-4-Reasoning-Plus採用了監督微調和強化學習，強調品質而非規模，並且可以廣泛應用於商業和企業應用中。這個模型的推出展示了微軟對訓練小型模型的重視，這些小型模型在性能上可以與大型系統匹敵。",
    "content": "Microsoft launches Phi-4-Reasoning-Plus, a small, powerful, open weights reasoning model! | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nMicrosoft launches Phi-4-Reasoning-Plus, a small, powerful, open weights reasoning model!\nCarl Franzen\n@carlfranzen\nMay 1, 2025 5:41 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nVentureBeat made with Midjourney\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nMicrosoft Research has\nannounced the release of Phi-4-reasoning-plus\n, an open-weight language model built for tasks requiring deep, structured reasoning.\nBuilding on the architecture of the\npreviously released\nPhi-4\n, the new model integrates supervised fine-tuning and reinforcement learning to deliver improved performance on benchmarks in mathematics, science, coding, and logic-based tasks.\nPhi-4-reasoning-plus is a 14-billion parameter dense decoder-only Transformer model that emphasizes quality over scale. Its training process involved 16 billion tokens—about 8.3 billion of them unique—drawn from synthetic and curated web-based datasets.\nA reinforcement learning (RL) phase, using only about 6,400 math-focused problems, further refined the model’s reasoning capabilities.\nThe model has been released under a\npermissive MIT license\n— enabling its use for broad commercial and enterprise applications, and fine-tuning or distillation, without restriction — and is compatible with widely used inference frameworks including Hugging Face Transformers, vLLM, llama.cpp, and Ollama.\nMicrosoft provides detailed recommendations on inference parameters and system prompt formatting to help developers get the most from the model.\nOutperforms larger models\nThe model’s development reflects Microsoft’s growing emphasis on training smaller models capable of rivaling much larger systems in performance.\nDespite its relatively modest size, Phi-4-reasoning-plus outperforms larger open-weight models such as DeepSeek-R1-Distill-70B on a number of demanding benchmarks.\nOn the AIME 2025 math exam, for instance, it delivers a higher average accuracy at passing all 30 questions on the first try (a feat known as “pass@1”) than the 70B parameter distillation model, and approaches the performance of DeepSeek-R1 itself, which is far larger at 671B parameters.\nStructured thinking via fine-tuning\nTo achieve this, Microsoft employed a data-centric training strategy.\nDuring the supervised fine-tuning stage, the model was trained using a curated blend of synthetic chain-of-thought reasoning traces and filtered high-quality prompts.\nA key innovation in the training approach was the use of structured reasoning outputs marked with special\n<think>\nand\n</think>\ntokens.\nThese guide the model to separate its intermediate reasoning steps from the final answer, promoting both transparency and coherence in long-form problem solving.\nReinforcement learning for accuracy and depth\nFollowing fine-tuning, Microsoft used outcome-based reinforcement learning—specifically, the Group Relative Policy Optimization (GRPO) algorithm—to improve the model’s output accuracy and efficiency.\nThe RL reward function was crafted to balance correctness with conciseness, penalize repetition, and enforce formatting consistency. This led to longer but more thoughtful responses, particularly on questions where the model initially lacked confidence.\nOptimized for research and engineering constraints\nPhi-4-reasoning-plus is intended for use in applications that benefit from high-quality reasoning under memory or latency constraints. It supports a context length of 32,000 tokens by default and has demonstrated stable performance in experiments with inputs up to 64,000 tokens.\nIt is best used in a chat-like setting and performs optimally with a system prompt that explicitly instructs it to reason through problems step-by-step before presenting a solution.\nExtensive safety testing and use guidelines\nMicrosoft positions the model as a research tool and a component for generative AI systems rather than a drop-in solution for all downstream tasks.\nDevelopers are advised to carefully evaluate performance, safety, and fairness before deploying the model in high-stakes or regulated environments.\nPhi-4-reasoning-plus has undergone extensive safety evaluation, including red-teaming by Microsoft’s AI Red Team and benchmarking with tools like Toxigen to assess its responses across sensitive content categories.\nAccording to Microsoft, this release demonstrates that with carefully curated data and training techniques, small models can deliver strong reasoning performance — and democratic, open access to boot.\nHere’s a revised version of the enterprise implications section in a more technical, news-style tone, aligning with a business-technology publication:\nImplications for enterprise technical decision-makers\nThe release of Microsoft’s Phi-4-reasoning-plus may present meaningful opportunities for enterprise technical stakeholders managing AI model development, orchestration, or data infrastructure.\nFor AI engineers and model lifecycle managers, the model’s 14B parameter size coupled with competitive benchmark performance introduces a viable option for high-performance reasoning without the infrastructure demands of significantly larger models. Its compatibility with frameworks such as Hugging Face Transformers, vLLM, llama.cpp, and Ollama provides deployment flexibility across different enterprise stacks, including containerized and serverless environments.\nTeams responsible for deploying and scaling machine learning models may find the model’s support for 32k-token contexts—expandable to 64k in testing—particularly useful in document-heavy use cases such as legal analysis, technical QA, or financial modeling. The built-in structure of separating chain-of-thought reasoning from the final answer could also simplify integration into interfaces where interpretability or auditability is required.\nFor AI orchestration teams, Phi-4-reasoning-plus offers a model architecture that can be more easily slotted into pipelines with resource constraints. This is relevant in scenarios where real-time reasoning must occur under latency or cost limits. Its demonstrated ability to generalize to out-of-domain problems, including NP-hard tasks like 3SAT and TSP, suggests utility in algorithmic planning and decision support use cases beyond those explicitly targeted during training.\nData engineering leads may also consider the model’s reasoning format—designed to reflect intermediate problem-solving steps—as a mechanism for tracking logical consistency across long sequences of structured data. The structured output format could be integrated into validation layers or logging systems to support explainability in data-rich applications.\nFrom a governance and safety standpoint, Phi-4-reasoning-plus incorporates multiple layers of post-training safety alignment and has undergone adversarial testing by Microsoft’s internal AI Red Team. For organizations subject to compliance or audit requirements, this may reduce the overhead of developing custom alignment workflows from scratch.\nOverall, Phi-4-reasoning-plus shows how the reasoning craze kicked off by the likes of\nOpenAI’s “o” series of models\nand\nDeepSeek R1\nis continuing to accelerate and move downstream to smaller, more accessible, affordable, and customizable models.\nFor technical decision-makers tasked with managing performance, scalability, cost, and risk, it offers a modular, interpretable alternative that can be evaluated and integrated on a flexible basis—whether in isolated inference endpoints, embedded tooling, or full-stack generative AI systems.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Tour Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Tour. See if we're coming to your area!\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-05-02T13:45:24.378854",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-02T13:45:40.706036",
    "audio_file": "f20d263c2063d01fbea44ba6515ad649.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/f20d263c2063d01fbea44ba6515ad649.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-02T13:45:49.251732"
  }
]