[
  {
    "id": "155874e03a2ba386852ad9dab40cac60",
    "title": "Is your AI product actually working? How to develop the right metric system",
    "url": "https://venturebeat.com/ai/is-your-ai-product-actually-working-how-to-develop-the-right-metric-system/",
    "authors": "Sharanya Rao, Intuit",
    "published_date": "Sun, 27 Apr 2025 19:15:00 +0000",
    "source": "VentureBeat AI",
    "summary": "這篇新聞談到如何確保你的AI產品真的運作正常，重點在於發展正確的衡量系統。如果不追蹤產品表現，就像飛機降落時沒有空中交通管制指示一樣危險。確保團隊共同明確衡量指標，避免各自搞自己的版本，才能確保大家朝著同一目標前進。要確保AI產品運作正常，首先要明確想要了解產品的哪些方面。",
    "content": "Is your AI product actually working? How to develop the right metric system | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nGuest\nIs your AI product actually working? How to develop the right metric system\nSharanya Rao, Intuit\nApril 27, 2025 12:15 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nVentureBeat/Midjourney\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nIn my first stint as a machine learning (ML) product manager, a simple question inspired passionate debates across functions and leaders: How do we know if this product is actually working? The product in question that I managed catered to both internal and external customers. The model enabled internal teams to identify the top issues faced by our customers so that they could prioritize the right set of experiences to fix customer issues. With such a complex web of interdependencies among internal and external customers, choosing the\nright metrics\nto capture the impact of the product was critical to steer it towards success.\nNot tracking whether your product is working well is like landing a plane without any instructions from air traffic control. There is absolutely no way that you can make informed decisions for your customer without knowing what is going right or wrong. Additionally, if you do not actively define the metrics, your team will identify their own back-up metrics. The risk of having multiple flavors of an ‘accuracy’ or ‘quality’ metric is that everyone will develop their own version, leading to a scenario where you might not all be working toward the same outcome.\nFor example, when I reviewed my annual goal and the underlying metric with our engineering team, the immediate feedback was: “But this is a business metric, we already track precision and recall.”\nFirst, identify what you want to know about your AI product\nOnce you do get down to the task of defining the metrics for your product — where to begin? In my experience, the complexity of operating an\nML product\nwith multiple customers translates to defining metrics for the model, too. What do I use to measure whether a model is working well? Measuring the outcome of internal teams to prioritize launches based on our models would not be quick enough; measuring whether the customer adopted solutions recommended by our model could risk us drawing conclusions from a very broad adoption metric (what if the customer didn’t adopt the solution because they just wanted to reach a support agent?).\nFast-forward to the era of\nlarge language models\n(LLMs) — where we don’t just have a single output from an ML model, we have text answers, images and music as outputs, too. The dimensions of the product that require metrics now rapidly increases — formats, customers, type … the list goes on.\nAcross all my products, when I try to come up with metrics, my first step is to distill what I want to know about its impact on customers into a few key questions. Identifying the right set of questions makes it easier to identify the right set of metrics. Here are a few examples:\nDid the customer get an output? → metric for coverage\nHow long did it take for the product to provide an output? → metric for latency\nDid the user like the output? → metrics for customer feedback, customer adoption and retention\nOnce you identify your key questions, the next step is to identify a set of sub-questions for ‘input’ and ‘output’ signals. Output metrics are lagging indicators where you can measure an event that has already happened. Input metrics and leading indicators can be used to identify trends or predict outcomes. See below for ways to add the right sub-questions for lagging and leading indicators to the questions above. Not all questions need to have leading/lagging indicators.\nDid the customer get an output? → coverage\nHow long did it take for the product to provide an output? → latency\nDid the user like the output? → customer feedback, customer adoption and retention\nDid the user indicate that the output is right/wrong? (output)\nWas the output good/fair? (input)\nThe third and final step is to identify the method to gather metrics. Most metrics are gathered at-scale by new instrumentation via data engineering. However, in some instances (like question 3 above) especially for ML based products, you have the option of manual or automated evaluations that assess the model outputs. While it’s always best to develop automated evaluations, starting with manual evaluations for “was the output good/fair” and creating a rubric for the definitions of good, fair and not good will help you lay the groundwork for a rigorous and tested automated evaluation process, too.\nExample use cases: AI search, listing descriptions\nThe above framework can be applied to any\nML-based product\nto identify the list of primary metrics for your product. Let’s take search as an example.\nQuestion\nMetrics\nNature of Metric\nDid the customer get an output? → Coverage\n% search sessions with search results shown to customer\nOutput\nHow long did it take for the product to provide an output? → Latency\nTime taken to display search results for the user\nOutput\nDid the user like the output? → Customer feedback, customer adoption and retention\nDid the user indicate that the output is right/wrong? (Output) Was the output good/fair? (Input)\n% of search sessions with ‘thumbs up’ feedback on search results from the customer or % of search sessions with clicks from the customer\n% of search results marked as ‘good/fair’ for each search term, per quality rubric\nOutput\nInput\nHow about a product to generate descriptions for a listing (whether it’s a menu item in Doordash or a product listing on Amazon)?\nQuestion\nMetrics\nNature of Metric\nDid the customer get an output? → Coverage\n% listings with generated description\nOutput\nHow long did it take for the product to provide an output? → Latency\nTime taken to generate descriptions to the user\nOutput\nDid the user like the output? → Customer feedback, customer adoption and retention\nDid the user indicate that the output is right/wrong? (Output) Was the output good/fair? (Input)\n% of listings with generated descriptions that required edits from the technical content team/seller/customer\n% of listing descriptions marked as ‘good/fair’, per quality rubric\nOutput\nInput\nThe approach outlined above is extensible to multiple ML-based products. I hope this framework helps you define the right set of metrics for your ML model.\nSharanya Rao is a group product manager at\nIntuit\n.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Tour Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Tour. See if we're coming to your area!\nLearn More\nDataDecisionMakers\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:11.245205",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:49.995039",
    "audio_file": "155874e03a2ba386852ad9dab40cac60.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/155874e03a2ba386852ad9dab40cac60.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:46:15.050399"
  },
  {
    "id": "f5c385a16003fad5b066d90a7dafecd7",
    "title": "DeepSeek’s success shows why motivation is key to AI innovation",
    "url": "https://venturebeat.com/ai/deepseeks-success-shows-why-motivation-is-key-to-ai-innovation/",
    "authors": "Debasish Ray Chawdhuri, Talentica Software",
    "published_date": "Sat, 26 Apr 2025 19:55:00 +0000",
    "source": "VentureBeat AI",
    "summary": "一家名為DeepSeek的中國公司在AI領域取得成功，挑戰了OpenAI和美國科技巨頭。他們雖然沒有在技術上完全超越對手，但卻在節省成本和能源使用效率上有所突破。DeepSeek的成功關鍵在於他們的動力和創新精神，這點讓他們在硬體效率上有所優勢。這也提醒了大公司，要在AI創新上不只看技術，更要看動力。",
    "content": "DeepSeek's success shows why motivation is key to AI innovation | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nGuest\nDeepSeek’s success shows why motivation is key to AI innovation\nDebasish Ray Chawdhuri, Talentica Software\nApril 26, 2025 12:55 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nVentureBeat/Midjourney\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nJanuary 2025\nshook the AI landscape\n. The seemingly unstoppable OpenAI and the powerful American tech giants were shocked by what we can certainly call an underdog in the area of large language models (LLMs). DeepSeek, a Chinese firm not on anyone’s radar, suddenly challenged OpenAI. It is not that DeepSeek-R1 was better than the top models from American giants; it was slightly behind in terms of the benchmarks, but it suddenly made everyone think about the efficiency in terms of hardware and energy usage.\nGiven the unavailability of the best high-end hardware, it seems that DeepSeek was motivated to innovate in the area of efficiency, which was a lesser concern for larger players. OpenAI has claimed they have evidence suggesting\nDeepSeek\nmay have used their model for training, but we have no concrete proof to support this. So, whether it is true or it’s OpenAI simply trying to appease their investors is a topic of debate. However, DeepSeek has published their work, and people have verified that the results are reproducible at least on a much smaller scale.\nBut how could\nDeepSeek\nattain such cost-savings while American companies could not? The short answer is simple: They had more motivation. The long answer requires a little bit more of a technical explanation.\nDeepSeek used KV-cache optimization\nOne important cost-saving for GPU memory was optimization of the Key-Value cache used in every attention layer in an LLM.\nLLMs are made up of transformer blocks, each of which comprises an attention layer followed by a regular vanilla feed-forward network. The feed-forward network conceptually models arbitrary relationships, but in practice, it is difficult for it to always determine patterns in the data. The attention layer solves this problem for language modeling.\nThe model processes texts using tokens, but for simplicity, we will refer to them as words. In an LLM, each word gets assigned a vector in a high dimension (say, a thousand dimensions). Conceptually, each dimension represents a concept, like being hot or cold, being green, being soft, being a noun. A word’s vector representation is its meaning and values according to each dimension.\nHowever, our language allows other words to modify the meaning of each word. For example, an apple has a meaning. But we can have a green apple as a modified version. A more extreme example of modification would be that an apple in an iPhone context differs from an apple in a meadow context. How do we let our system modify the vector meaning of a word based on another word? This is where attention comes in.\nThe attention model assigns two other vectors to each word: a key and a query. The query represents the qualities of a word’s meaning that can be modified, and the key represents the type of modifications it can provide to other words. For example, the word ‘green’ can provide information about color and green-ness. So, the key of the word ‘green’ will have a high value on the ‘green-ness’ dimension. On the other hand, the word ‘apple’ can be green or not, so the query vector of ‘apple’ would also have a high value for the green-ness dimension. If we take the dot product of the key of ‘green’ with the query of ‘apple,’ the product should be relatively large compared to the product of the key of ‘table’ and the query of ‘apple.’ The attention layer then adds a small fraction of the value of the word ‘green’ to the value of the word ‘apple’. This way, the value of the word ‘apple’ is modified to be a little greener.\nWhen the LLM generates text, it does so one word after another. When it generates a word, all the previously generated words become part of its context. However, the keys and values of those words are already computed. When another word is added to the context, its value needs to be updated based on its query and the keys and values of all the previous words. That’s why all those values are stored in the GPU memory. This is the KV cache.\nDeepSeek determined that the key and the value of a word are related. So, the meaning of the word green and its ability to affect greenness are obviously very closely related. So, it is possible to compress both as a single (and maybe smaller) vector and decompress while processing very easily. DeepSeek has found that it does affect their\nperformance on benchmarks\n, but it saves a lot of GPU memory.\nDeepSeek applied MoE\nThe nature of a neural network is that the entire network needs to be evaluated (or computed) for every query. However, not all of this is useful computation. Knowledge of the world sits in the weights or parameters of a network. Knowledge about the Eiffel Tower is not used to answer questions about the history of South American tribes. Knowing that an apple is a fruit is not useful while answering questions about the general theory of relativity. However, when the network is computed, all parts of the network are processed regardless. This incurs huge computation costs during text generation that should ideally be avoided. This is where the idea of the mixture-of-experts (MoE) comes in.\nIn an MoE model, the neural network is divided into multiple smaller networks called experts. Note that the ‘expert’ in the subject matter is not explicitly defined; the network figures it out during training. However, the networks assign some relevance score to each query and only activate the parts with higher matching scores. This provides huge cost savings in computation. Note that some questions need expertise in multiple areas to be answered properly, and the performance of such queries will be degraded. However, because the areas are figured out from the data, the number of such questions is minimised.\nThe importance of reinforcement learning\nAn LLM is taught to think through a chain-of-thought model, with the model fine-tuned to imitate thinking before delivering the answer. The model is asked to verbalize its thought (generate the thought before generating the answer). The model is then evaluated both on the thought and the answer, and trained with reinforcement learning (rewarded for a correct match and penalized for an incorrect match with the training data).\nThis requires expensive training data with the thought token. DeepSeek only asked the system to generate the thoughts between the tags <think> and </think> and to generate the answers between the tags <answer> and </answer>. The model is rewarded or penalized purely based on the form (the use of the tags) and the match of the answers. This required much less expensive training data. During the early phase of RL, the model tried generated very little thought, which resulted in incorrect answers. Eventually, the model learned to generate both long and coherent thoughts, which is what DeepSeek calls the ‘a-ha’ moment. After this point, the quality of the answers improved quite a lot.\nDeepSeek employs several additional optimization tricks. However, they are highly technical, so I will not delve into them here.\nFinal thoughts about DeepSeek and the larger market\nIn any technology research, we first need to see what is possible before improving efficiency. This is a natural progression. DeepSeek’s contribution to the LLM landscape is phenomenal. The academic contribution cannot be ignored, whether or not they are trained using OpenAI output. It can also transform the way startups operate. But there is no reason for OpenAI or the other American giants to despair. This is how\nresearch works\n— one group benefits from the research of the other groups. DeepSeek certainly benefited from the earlier research performed\nby Google, OpenAI and numerous other researchers.\nHowever, the idea that OpenAI will dominate the LLM world indefinitely is now very unlikely. No amount of regulatory lobbying or finger-pointing will preserve their monopoly. The technology is already in the hands of many and out in the open, making its progress unstoppable. Although this may be a little bit of a headache for the investors of OpenAI, it’s ultimately a win for the rest of us. While the future belongs to many, we will always be thankful to early contributors like Google and OpenAI.\nDebasish Ray Chawdhuri is senior principal engineer at\nTalentica Software\n.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Tour Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Tour. See if we're coming to your area!\nLearn More\nDataDecisionMakers\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:12.150172",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:52.697208",
    "audio_file": "f5c385a16003fad5b066d90a7dafecd7.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/f5c385a16003fad5b066d90a7dafecd7.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:46:24.877315"
  },
  {
    "id": "bffba51be3693064b13daa81cc2e1923",
    "title": "Liquid AI is revolutionizing LLMs to work on edge devices like smartphones with new ‘Hyena Edge’ model",
    "url": "https://venturebeat.com/ai/liquid-ai-is-revolutionizing-llms-to-work-on-edge-devices-like-smartphones-with-new-hyena-edge-model/",
    "authors": "Carl Franzen",
    "published_date": "Fri, 25 Apr 2025 22:02:47 +0000",
    "source": "VentureBeat AI",
    "summary": "Liquid AI推出新的「Hyena Edge」模型，革新了大型語言模型在智慧裝置上運作的方式，尤其針對智慧手機等邊緣裝置。這個模型在效率和品質上表現優異，能在智慧手機上提供更快速、更節省記憶體的人工智慧應用。這個新架構不同於傳統的設計，採用了新的卷積技術，讓AI在邊緣裝置上更有效率。Liquid AI致力於推動科技產業超越目前主流的Transformer架構，開創新的AI應用領域。",
    "content": "Liquid AI is revolutionizing LLMs to work on edge devices like smartphones with new 'Hyena Edge' model | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nLiquid AI is revolutionizing LLMs to work on edge devices like smartphones with new ‘Hyena Edge’ model\nCarl Franzen\n@carlfranzen\nApril 25, 2025 3:02 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nLiquid AI, the Boston-based foundation model startup spun out of the Massachusetts Institute of Technology (MIT), is seeking to move the tech industry beyond its reliance on the Transformer architecture underpinning most popular large language models (LLMs) such as\nOpenAI’s GPT\nseries and\nGoogle’s Gemini\nfamily.\nYesterday, the company announced “\nHyena Edge\n,” a new convolution-based, multi-hybrid model designed for smartphones and other edge devices in advance of the\nInternational Conference on Learning Representations (ICLR) 2025.\nThe conference, one of the premier events for machine learning research, is taking place this year in Singapore.\nNew convolution-based model promises faster, more memory-efficient AI at the edge\nHyena Edge is engineered to outperform strong Transformer baselines on both computational efficiency and language model quality.\nIn real-world tests on a Samsung Galaxy S24 Ultra smartphone, the model delivered lower latency, smaller memory footprint, and better benchmark results compared to a parameter-matched Transformer++ model.\nA new architecture for a new era of edge AI\nUnlike most small models designed for mobile deployment — including SmolLM2, the Phi models, and Llama 3.2 1B — Hyena Edge steps away from traditional attention-heavy designs. Instead, it strategically replaces two-thirds of grouped-query attention (GQA) operators with gated convolutions from the Hyena-Y family.\nThe new architecture is the result of Liquid AI’s Synthesis of Tailored Architectures (STAR) framework, which uses evolutionary algorithms to automatically design model backbones and\nwas announced back in December 2024.\nSTAR explores a wide range of operator compositions, rooted in the mathematical theory of linear input-varying systems, to optimize for multiple hardware-specific objectives like latency, memory usage, and quality.\nBenchmarked directly on consumer hardware\nTo validate Hyena Edge’s real-world readiness, Liquid AI ran tests directly on the Samsung Galaxy S24 Ultra smartphone.\nResults show that Hyena Edge achieved up to 30% faster prefill and decode latencies compared to its Transformer++ counterpart, with speed advantages increasing at longer sequence lengths.\nPrefill latencies at short sequence lengths also outpaced the Transformer baseline — a critical performance metric for responsive on-device applications.\nIn terms of memory, Hyena Edge consistently used less RAM during inference across all tested sequence lengths, positioning it as a strong candidate for environments with tight resource constraints.\nOutperforming Transformers on language benchmarks\nHyena Edge was trained on 100 billion tokens and evaluated across standard benchmarks for small language models, including Wikitext, Lambada, PiQA, HellaSwag, Winogrande, ARC-easy, and ARC-challenge.\nOn every benchmark, Hyena Edge either matched or exceeded the performance of the GQA-Transformer++ model, with noticeable improvements in perplexity scores on Wikitext and Lambada, and higher accuracy rates on PiQA, HellaSwag, and Winogrande.\nThese results suggest that the model’s efficiency gains do not come at the cost of predictive quality — a common tradeoff for many edge-optimized architectures.\nHyena Edge Evolution: A look at performance and operator trends\nFor those seeking a deeper dive into Hyena Edge’s development process, a recent\nvideo walkthrough\nprovides a compelling visual summary of the model’s evolution.\nThe video highlights how key performance metrics — including prefill latency, decode latency, and memory consumption — improved over successive generations of architecture refinement.\nIt also offers a rare behind-the-scenes look at how the internal composition of Hyena Edge shifted during development. Viewers can see dynamic changes in the distribution of operator types, such as Self-Attention (SA) mechanisms, various Hyena variants, and SwiGLU layers.\nThese shifts offer insight into the architectural design principles that helped the model reach its current level of efficiency and accuracy.\nBy visualizing the trade-offs and operator dynamics over time, the video provides valuable context for understanding the architectural breakthroughs underlying Hyena Edge’s performance.\nOpen-source plans and a broader vision\nLiquid AI said it plans to open-source a series of Liquid foundation models, including Hyena Edge, over the coming months. The company’s goal is to build capable and efficient general-purpose AI systems that can scale from cloud datacenters down to personal edge devices.\nThe debut of Hyena Edge also highlights the growing potential for alternative architectures to challenge Transformers in practical settings. With mobile devices increasingly expected to run sophisticated AI workloads natively, models like Hyena Edge could set a new baseline for what edge-optimized AI can achieve.\nHyena Edge’s success — both in raw performance metrics and in showcasing automated architecture design — positions Liquid AI as one of the emerging players to watch in the evolving AI model landscape.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Tour Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Tour. See if we're coming to your area!\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:12.710863",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:55.596869",
    "audio_file": "bffba51be3693064b13daa81cc2e1923.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/bffba51be3693064b13daa81cc2e1923.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:46:35.997791"
  },
  {
    "id": "19be4957aff9619bf956c91a25672c92",
    "title": "The new AI calculus: Google’s 80% cost edge vs. OpenAI’s ecosystem",
    "url": "https://venturebeat.com/ai/the-new-ai-calculus-googles-80-cost-edge-vs-openais-ecosystem/",
    "authors": "Matt Marshall",
    "published_date": "Fri, 25 Apr 2025 20:26:46 +0000",
    "source": "VentureBeat AI",
    "summary": "Google在AI領域擁有80%的成本優勢，而OpenAI則依賴其生態系統。企業在選擇AI平台時，不僅要看模型表現，還要考慮生態系統對成本、開發策略、模型可靠性等的影響。Google利用自家的矽片技術，讓AI運算成本大幅降低，而OpenAI則使用Nvidia的GPU成本較高。企業需在計算成本、AI代理建構策略、模型可靠性等方面仔細衡量，才能選擇適合的AI平台。",
    "content": "The new AI calculus: Google’s 80% cost edge vs. OpenAI’s ecosystem | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nAnalysis\nThe new AI calculus: Google’s 80% cost edge vs. OpenAI’s ecosystem\nMatt Marshall\n@mmarshall\nApril 25, 2025 1:26 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage Credit: VentureBeat via ChatGPT\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nThe relentless pace of generative AI innovation shows no signs of slowing. In just the past couple of weeks, OpenAI dropped its\npowerful o3 and o4-mini reasoning models\nalongside the\nGPT-4.1 series\n, while Google countered with Gemini 2.5 Flash,\nrapidly iterating on its flagship Gemini 2.5 Pro\nreleased shortly before. For enterprise technical leaders navigating this dizzying landscape, choosing the right AI platform requires looking far beyond rapidly shifting model benchmarks\nWhile model-versus-model benchmarks grab headlines, the decision for technical leaders goes far deeper. Choosing an AI platform is a commitment to an ecosystem, impacting everything from core compute costs and agent development strategy to model reliability and enterprise integration.\nBut perhaps the most stark differentiator, bubbling beneath the surface but with profound long-term implications, lies in the economics of the hardware powering these AI giants. Google wields a massive cost advantage thanks to its custom silicon, potentially running its AI workloads at a fraction of the cost OpenAI incurs relying on Nvidia’s market-dominant (and high-margin) GPUs.\nThis analysis delves beyond the benchmarks to compare the Google and OpenAI/Microsoft AI ecosystems across the critical factors enterprises must consider today: the significant disparity in compute economics, diverging strategies for building AI agents, the crucial trade-offs in model capabilities and reliability and the realities of enterprise fit and distribution. The analysis builds upon\nan in-depth video discussion exploring these systemic shifts\nbetween myself and AI developer Sam Witteveen earlier this week.\n1. Compute economics: Google’s TPU “secret weapon” vs. OpenAI’s Nvidia tax\nThe most significant, yet often under-discussed, advantage Google holds is its “secret weapon:” its decade-long investment in custom Tensor Processing Units (TPUs). OpenAI and the broader market rely heavily on Nvidia’s powerful but expensive GPUs (like the H100 and A100). Google, on the other hand, designs and deploys its own TPUs, like the\nrecently unveiled Ironwood generation\n, for its core AI workloads. This includes training and serving Gemini models.\nWhy does this matter? It makes a huge cost difference.\nNvidia GPUs command staggering gross margins, estimated by analysts to\nbe in the 80% range\nfor\ndata center chips like the H100\nand upcoming B100 GPUs. This means OpenAI (via Microsoft Azure) pays a hefty premium — the “Nvidia tax” — for its compute power. Google, by manufacturing TPUs in-house, effectively bypasses this markup.\nWhile manufacturing GPUs might cost Nvidia $3,000-$5,000, hyperscalers like Microsoft (supplying OpenAI) pay $20,000-$35,000+ per unit in volume,\naccording\nto\nreports\n. Industry conversations and analysis suggest that Google may be obtaining its AI compute power at roughly 20% of the cost incurred by those purchasing high-end Nvidia GPUs. While the exact numbers are internal, the implication is a\n4x-6x cost efficiency\nadvantage per unit of compute for Google at the hardware level.\nThis structural advantage is reflected in API pricing. Comparing the flagship models, OpenAI’s o3 is\nroughly 8 times more expensive\nfor input tokens and 4 times more expensive for output tokens\nthan Google’s Gemini 2.5 Pro\n(for standard context lengths).\nThis cost differential isn’t academic; it has profound strategic implications. Google can likely sustain lower prices and offer better “intelligence per dollar,” giving enterprises more predictable long-term Total Cost of Ownership (TCO) – and that’s exactly\nwhat it is doing right now in practice\n.\nOpenAI’s costs, meanwhile, are intrinsically tied to Nvidia’s pricing power and the terms of its Azure deal. Indeed, compute costs represent an estimated\n55-60% of OpenAI’s total $9B operating expenses\nin 2024, according to some reports, and are\nprojected\nto\nexceed 80% in 2025 as th\ney scale\n. While OpenAI’s projected revenue growth is astronomical – potentially hitting $125 billion by 2029\naccording to reported internal forecasts\n– managing this compute spend remains a critical challenge,\ndriving their pursuit of custom silicon\n.\n2. Agent frameworks: Google’s open ecosystem approach vs. OpenAI’s integrated one\nBeyond hardware, the two giants are pursuing divergent strategies for building and deploying the AI agents poised to automate enterprise workflows.\nGoogle is making a clear push for interoperability and a more open ecosystem.\nAt Cloud Next two weeks ago,\nit unveiled\nthe Agent-to-Agent (A2A) protocol, designed to allow agents built on different platforms to communicate, alongside its Agent Development Kit (ADK) and the Agentspace hub for discovering and managing agents.\nWhile A2A adoption faces hurdles — key players like Anthropic haven’t signed on (VentureBeat reached out to Anthropic about this, but Anthropic declined to comment) — and some developers debate its necessity alongside Anthropic’s existing Model Context Protocol (MCP). Google’s intent is clear: to foster a multi-vendor agent marketplace, potentially hosted within its Agent Garden or via a rumored Agent App Store.\nOpenAI, conversely, appears focused on creating powerful, tool-using agents tightly integrated within its own stack. The new o3 model exemplifies this, capable of making hundreds of tool calls within a single reasoning chain. Developers leverage the Responses API and Agents SDK, along with tools like the new Codex CLI, to build sophisticated agents that operate within the OpenAI/Azure trust boundary. While frameworks like Microsoft’s Autogen offer some flexibility, OpenAI’s core strategy seems less about cross-platform communication and more about maximizing agent capabilities vertically within its controlled environment.\nThe enterprise takeaway:\nCompanies prioritizing flexibility and the ability to mix-and-match agents from various vendors (e.g., plugging a Salesforce agent into Vertex AI) may find Google’s open approach appealing. Those deeply invested in the Azure/Microsoft ecosystem or preferring a more vertically managed, high-performance agent stack might lean towards OpenAI.\n3. Model capabilities: parity, performance, and pain points\nThe relentless release cycle means model leadership is fleeting. While OpenAI’s o3 currently edges out Gemini 2.5 Pro on some coding benchmarks like SWE-Bench Verified and Aider, Gemini 2.5 Pro matches or leads on others like GPQA and AIME. Gemini 2.5 Pro is also the overall leader on the large language model (LLM) Arena Leaderboard. For many enterprise use cases, however, the models have reached rough parity in core capabilities.\nThe\nreal\ndifference lies in their distinct trade-offs:\nContext vs. Reasoning Depth:\nGemini 2.5 Pro boasts a massive 1-million-token context window (with 2M planned), ideal for processing large codebases or document sets. OpenAI’s o3 offers a 200k window but emphasizes deep, tool-assisted reasoning within a single turn, enabled by its reinforcement learning approach.\nReliability vs. Risk:\nThis is emerging as a critical differentiator. While o3 showcases impressive reasoning, OpenAI’s own model card for 03\nrevealed it hallucinates significantly more (2x the rate of o1 on PersonQA)\n. Some analyses suggest this might stem from its\ncomplex reasoning and tool-use mechanisms\n. Gemini 2.5 Pro, while perhaps sometimes perceived as less innovative in its output structure, is often described by users as more reliable and predictable for enterprise tasks. Enterprises must weigh o3’s cutting-edge capabilities against this documented increase in hallucination risk.\nThe enterprise takeaway:\nThe “best” model depends on the task. For analyzing vast amounts of context or prioritizing predictable outputs, Gemini 2.5 Pro holds an edge. For tasks demanding the deepest multi-tool reasoning, where hallucination risk can be carefully managed, o3 is a powerful contender. As Sam Witteveen noted in our\nin-depth podcast about this\n, rigorous testing within specific enterprise use cases is essential.\n4. Enterprise fit & distribution: integration depth vs. market reach\nUltimately, adoption often hinges on how easily a platform slots into an enterprise’s existing infrastructure and workflows.\nGoogle’s strength lies in deep integration for existing Google Cloud and Workspace customers. Gemini models, Vertex AI, Agentspace and tools like BigQuery are designed to work seamlessly together, offering a unified control plane, data governance, and potentially faster time-to-value for companies\nalready invested in Google’s ecosystem\n. Google is\nactively courting large enterprises\n, showcasing deployments with firms like Wendy’s, Wayfair, and Wells Fargo.\nOpenAI, via Microsoft, boasts unparalleled market reach and accessibility. ChatGPT’s enormous user base (~800M MAU) creates broad familiarity. More importantly, Microsoft is aggressively embedding OpenAI models (including the latest o-series) into its ubiquitous Microsoft 365 Copilot and Azure services, making powerful AI capabilities readily available to potentially hundreds of millions of enterprise users, often within the tools they already use daily. For organizations that are already standardized on Azure and Microsoft 365, adopting OpenAI can be a more natural extension. Furthermore, the extensive use of OpenAI APIs by developers means many enterprise prompts and workflows are already optimized for OpenAI models.\nThe strategic decision:\nThe choice often boils down to existing vendor relationships. Google offers a compelling, integrated story for its current customers. OpenAI, powered by Microsoft’s distribution engine, offers broad accessibility and potentially easier adoption for the vast number of Microsoft-centric enterprises.\nGoogle vs OpenAI/Microsoft has tradeoffs for enterprises\nThe generative AI platform war between Google and OpenAI/Microsoft has moved far beyond simple model comparisons. While both offer state-of-the-art capabilities, they represent different strategic bets and present distinct advantages and trade-offs for the enterprise.\nEnterprises must weigh differing approaches to agent frameworks, the nuanced trade-offs between model capabilities like context length versus cutting-edge reasoning and the practicalities of enterprise integration and distribution reach.\nHowever, looming over all these factors is the stark reality of compute cost, which emerges as perhaps the most critical and defining long-term differentiator, especially if OpenAI doesn’t manage to address it quickly. Google’s vertically integrated TPU strategy, allowing it to potentially bypass the ~80% “Nvidia Tax” embedded in GPU pricing that burdens OpenAI, represents a fundamental economic advantage, potentially a game-changing one.\nThis is more than a minor price difference; it impacts everything from API affordability and long-term TCO predictability to the sheer scalability of AI deployments. As AI workloads grow exponentially, the platform with the more sustainable economic engine — fueled by hardware cost efficiency — holds a powerful strategic edge. Google is leveraging this advantage while also pushing an open vision for agent interoperability.\nOpenAI, backed by Microsoft’s scale, counters with deeply integrated tool-using models and an unparalleled market reach, although questions remain about its cost structure and model reliability.\nTo make the right choice, enterprise technical leaders must look past the benchmarks and evaluate these ecosystems based on their long-term TCO implications, their preferred approach to agent strategy and openness, their tolerance for model reliability risks versus raw reasoning power, their existing technology stack and their specific application needs.\nWatch the video where Sam Witteveen and I break things down:\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Tour Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Tour. See if we're coming to your area!\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:13.413322",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:58.984567",
    "audio_file": "19be4957aff9619bf956c91a25672c92.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/19be4957aff9619bf956c91a25672c92.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:46:46.092669"
  },
  {
    "id": "fa22b54c2d648a7f8da92c4d6422ab1e",
    "title": "Is that really your boss calling? Jericho Security raises $15M to stop deepfake fraud that’s cost businesses $200M in 2025 alone",
    "url": "https://venturebeat.com/ai/is-that-really-your-boss-calling-jericho-security-raises-15m-to-stop-deepfake-fraud-thats-cost-businesses-200m-in-2025-alone/",
    "authors": "Michael Nuñez",
    "published_date": "Thu, 24 Apr 2025 22:18:45 +0000",
    "source": "VentureBeat AI",
    "summary": "Jericho Security成功籌得1500萬美元資金，用於開發AI防護平台，阻止深偽造詐騙。這種詐騙在2025年已讓企業損失2億美元。他們的技術可以防止駭客製作聲音複製品，假冒高層要求匯款。這筆投資來自多家投資者，包括Era Fund、Lux Capital等。Jericho Security在軍方合約中獲得1.8百萬美元，成功提升了在競爭激烈市場中的聲譽。",
    "content": "Is that really your boss calling? Jericho Security raises $15M to stop deepfake fraud that's cost businesses $200M in 2025 alone | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nIs that really your boss calling? Jericho Security raises $15M to stop deepfake fraud that’s cost businesses $200M in 2025 alone\nMichael Nuñez\n@MichaelFNunez\nApril 24, 2025 3:18 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nNew York-based\nJericho Security\nhas secured $15 million in Series A funding to scale its AI-powered cybersecurity training platform. The investment, announced today, follows the company’s successful five-month execution of a\n$1.8 million Department of Defense contract\nthat put the two-year-old startup on the cybersecurity map.\n“Within minutes, a sophisticated attacker can now create a voice clone that sounds exactly like your CFO requesting an urgent wire transfer,” said Sage Wohns, co-founder and Chief Executive Officer of Jericho Security, in an exclusive interview with VentureBeat. “Traditional cybersecurity training simply hasn’t kept pace with these threats.”\nThe funding round was led by Jasper Lau at\nEra Fund\n, who previously backed the company’s $3 million seed round in August 2023. Additional investors include\nLux Capital\n,\nDash Fund\n,\nGaingels Enterprise Fund\nand\nGaingels AI Fund\n,\nDistique Ventures\n,\nPlug & Play Ventures\n, and several specialized venture firms.\nMilitary cybersecurity contract established credibility in competitive market\nJericho’s profile rose significantly last November when the Pentagon selected the company for its first generative AI defense contract.\nThe $1.8 million award through AFWERX\n, the innovation arm of the Air Force, charged Jericho with protecting military personnel from increasingly sophisticated phishing attacks.\n“There was a highly publicized spear-phishing attack targeting Air Force drone pilots using fake user manuals,” Wohns noted in an earlier interview. The incident underscored how even highly trained personnel can fall victim to carefully crafted deception.\nThis federal contract helped Jericho stand out in a crowded cybersecurity market where established players like\nKnowBe4\n,\nProofpoint\n, and\nCofense\ndominate. Industry analysts value the security awareness training sector at $5 billion annually, with\nprojected growth to $10 billion by 2027\nas organizations increasingly recognize human vulnerability as their primary security weakness.\nHow AI fights AI: Automated adversaries that learn employee weaknesses\nUnlike conventional security training that relies on static templates and predictable scenarios, Jericho’s platform employs what Wohns calls “agentic AI” — autonomous systems that behave like actual attackers.\n“If an employee ignores a suspicious email, our system might follow up with a text message that appears to come from their manager,” Wohns explained. “Just like real attackers, our AI adapts to behavior, learning which approaches work best against specific individuals.”\nThis multi-channel approach addresses a fundamental limitation of traditional security training: most programs prepare employees for yesterday’s attacks, not tomorrow’s. Jericho’s simulations can span email, voice, text messaging, and even video calls, creating personalized attack scenarios based on an employee’s role, behavior patterns, and previous responses.\nThe company’s client dashboard shows which employees fall for which types of attacks, allowing organizations to deliver targeted remediation. Early data suggests that employees trained with adaptive, AI-driven simulations are 64% less likely to fall for actual phishing attempts than those who receive traditional security awareness training.\nSingapore CFO loses $500,000 to deepfake executive impersonation\nThe financial stakes of these new threats became clear in a case Wohns highlighted involving a finance executive deceived by artificially generated versions of company leadership.\n“A CFO in Singapore was deceived into transferring nearly $500,000 during a video call that appeared to include the company’s CEO and other executives,” Wohns recounted. “Unbeknownst to the CFO, these participants were AI-generated deepfakes, crafted using publicly available videos and recordings.”\nThe attack began with a seemingly innocent WhatsApp message requesting an urgent Zoom meeting. During the call, the deepfake avatars persuaded the CFO to authorize the transfer. Only when the attackers attempted to extract more funds did suspicions arise, eventually involving authorities who recovered the initial transfer.\nSuch incidents are becoming alarmingly common. According to\nResemble AI’s Q1 2025 Deepfake Incident Report\n, financial losses from\ndeepfake-enabled fraud exceeded $200 million globally\nduring just the first quarter of 2025. The report found that North America experienced the highest number of incidents (38%), followed by Asia (27%) and Europe (21%).\nIndustry reports have documented staggering growth rates in recent years, with some studies showing deepfake fraud attempts increasing by\nmore than 1,700% in North America\nand exceeding 2,000% in certain European financial sectors.\nNew threat horizon: When AI systems attack other AI systems\nWohns identified an even more concerning emerging threat that few security teams are prepared for: “AI agents phishing AI agents.”\n“As AI tools proliferate inside companies from customer support chatbots to internal automations, attackers are beginning to target and exploit these agents directly,” he explained. “It’s no longer just humans being deceived. AI systems are now both the targets and the unwitting accomplices of compromise.”\nThis represents a fundamental shift in the cybersecurity landscape. When organizations deploy AI assistants that can access internal systems, approve requests, or provide information, they create new attack surfaces that traditional security approaches don’t address.\nSelf-service platform opens access to smaller businesses as attack targets broaden\nWhile major enterprises have long been primary targets for sophisticated attacks, smaller organizations are increasingly finding themselves in cybercriminals’ crosshairs. Recognizing this trend, Jericho has launched a self-service platform that allows companies to deploy AI-powered security training without the enterprise sales cycle.\n“The self-service registration is in addition to our enterprise sales approach,” Wohns said. “Self-Service is designed to provide no-touch/low-touch for Small to Medium Businesses.”\nUsers can sign up for a seven-day free trial and explore the product without sales meetings. This approach stands in contrast to industry norms, where cybersecurity solutions typically involve lengthy procurement processes and high-touch sales approaches.\nFuture-proofing security as AI capabilities accelerate\nThe $15 million investment will primarily fund three initiatives: expanding research and development, scaling go-to-market strategies through partnerships, and growing Jericho’s team with a focus on AI and cybersecurity talent.\n“One of our biggest technical challenges has been keeping pace with the rapid evolution of AI itself,” said Wohns. “The tools, models, and techniques are improving at an extraordinary rate, which means our architecture needs to be flexible enough to adapt quickly.”\nEarly customers have responded enthusiastically to Jericho’s approach. “Customers have been exceedingly frustrated at the lack of innovation with incumbent solutions and the subsequent decline in efficacy,” Wohns noted. “Within 30 days, customers identify vulnerabilities across multiple channels and build highly personalized and dynamic remediation programs based on contemporary threats and techniques.”\nAs the boundaries between human and machine communications blur, the very nature of trust in digital environments is being redefined. The executive on a video call, the urgent email from IT support, or the customer service chatbot might not be what they appear. In this new reality, Jericho Security is betting that the best defense isn’t just teaching employees to be suspicious — it’s showing them exactly how they’ll be deceived before the real attackers get the chance\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Tour Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Tour. See if we're coming to your area!\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:13.863928",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:44:01.323913",
    "audio_file": "fa22b54c2d648a7f8da92c4d6422ab1e.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/fa22b54c2d648a7f8da92c4d6422ab1e.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:46:57.139727"
  },
  {
    "id": "3517836409e904079b6da053f1e82462",
    "title": "Intel’s new CEO signals streamlining efforts but does not spell out exact layoff numbers",
    "url": "https://venturebeat.com/games/intels-new-ceo-signals-streamlining-efforts-but-does-not-spell-out-exact-layoff-numbers/",
    "authors": "Dean Takahashi",
    "published_date": "Thu, 24 Apr 2025 21:15:00 +0000",
    "source": "VentureBeat AI",
    "summary": "Intel的新CEO譚立步向員工發出直接訊息，表示公司必須重新組織以提高效率，暗示將進行精簡化努力，但並未明確說明裁員數字。這代表Intel可能會有一波組織調整，以期提升競爭力。",
    "content": "Lip-Bu Tan, the new CEO of Intel, sent out a blunt message to employees saying the company has to reorganize to be more efficient.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:14.353010",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:44:02.849019",
    "audio_file": "3517836409e904079b6da053f1e82462.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/3517836409e904079b6da053f1e82462.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:47:03.501231"
  },
  {
    "id": "981196fc54734376655d1866441ead4e",
    "title": "Zencoder buys Machinet to challenge GitHub Copilot as AI coding assistant consolidation accelerates",
    "url": "https://venturebeat.com/ai/zencoder-buys-machinet-to-challenge-github-copilot-as-ai-coding-assistant-consolidation-accelerates/",
    "authors": "Michael Nuñez",
    "published_date": "Thu, 24 Apr 2025 20:36:27 +0000",
    "source": "VentureBeat AI",
    "summary": "Zencoder收購Machinet，挑戰GitHub Copilot在AI程式碼助手領域的地位，加速整合。這次收購強化了Zencoder在競爭激烈的AI程式碼助手市場中的地位，擴大了對Java開發人員和其他JetBrains開發環境使用者的影響力。這對Zencoder來說是戰略擴張，讓他們成為GitHub Copilot等AI程式碼工具的嚴肅競爭對手之一。",
    "content": "Zencoder buys Machinet to challenge GitHub Copilot as AI coding assistant consolidation accelerates | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nZencoder buys Machinet to challenge GitHub Copilot as AI coding assistant consolidation accelerates\nMichael Nuñez\n@MichaelFNunez\nApril 24, 2025 1:36 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nZencoder\nannounced today the acquisition of\nMachinet\n, a developer of context-aware AI coding assistants with more than 100,000 downloads in the\nJetBrains\necosystem. The acquisition strengthens Zencoder’s position in the competitive AI coding assistant landscape and expands its reach among Java developers and other users of JetBrains’ popular development environments.\nThe deal represents a strategic expansion for\nZencoder\n, which emerged from stealth mode just six months ago but has quickly established itself as a serious competitor to\nGitHub Copilot\nand other AI coding tools.\n“At this point, there are three strong coordination products in the market that are production grade: it’s us, Cursor, and Windsurf. For smaller companies, it’s becoming harder and harder to compete,” said Andrew Filev, CEO and founder of Zencoder, in an exclusive interview with VentureBeat about the acquisition. “Our technical staff includes more than 50 engineers. For some startups, it’s very hard to keep that pace.”\nThe great AI coding assistant shakeout: Why small players can’t compete\nThis acquisition comes at a pivotal moment in the AI coding assistant market. Just last week, reports emerged that\nOpenAI is in discussions to acquire Windsurf\n, another AI coding assistant, for approximately $3 billion. While Filev maintains the timing is coincidental, he acknowledges that it reflects broader market dynamics.\n“I think there’s going to be more to it, and I’m looking forward to it,” Filev said. “It’s a huge product surface. You have to support multiple IDEs, you have to integrate with multiple DevOps tools, you have to support different parts of software life cycle. There are 70-plus, 100-plus programming languages… There’s so much work there that it’s very, very hard for the smaller companies that only have like sub-10 engineers to compete in the long term.”\nHow Zencoder’s JetBrains strategy outflanks Microsoft-dependent rivals\nOne of the key strategic values of acquiring\nMachinet\nis its strong presence in the\nJetBrains\necosystem, which is particularly popular among Java developers and enterprise backend teams.\n“JetBrains audiences are millions of engineers. They’re one of the leading providers for certain programming languages and technologies. They’re particularly well known in the Java world, which is a big chunk of enterprise backend,” Filev explained.\nThis gives Zencoder an advantage over competitors like\nCursor\nand\nWindsurf\n, which are built as forks of\nVisual Studio Code\nand may face increasing constraints due to Microsoft’s tightening of licensing restrictions.\n“Both Cursor and Windsurf are what’s called forks of Visual Studio, and Microsoft recently started tightening their licensing restrictions,” Filev noted. “The support that VS Code has for certain languages is better than the support that Cursor and Windsurf can offer, specifically for C Sharp, C++.”\nBy contrast, Zencoder works with Microsoft’s native platforms on VS Code and also integrates directly with JetBrains IDEs, giving it more flexibility across development environments.\nBeyond hype: How Zencoder’s benchmark victories translate to real developer value\nZencoder differentiates itself from competitors through what it calls “\nRepo Grokking\n” technology, which analyzes entire code repositories to provide AI models with better context, and an error-corrected inference pipeline that aims to reduce code errors.\nThe company claims impressive performance on industry benchmarks, with Filev highlighting results from March that showed Zencoder outperforming competitors:\n“On\nSWE-Bench Multimodal\n, the best result was around 13%, and we have been able to easily do 27% which we submitted, so we doubled the next best result. We later resubmitted even higher results of 31%,” Filev said.\nHe also noted performance on OpenAI’s benchmark: “On the\nSWE-Lancer\n‘diamond’ subset, OpenAI’s best result that they published was in the high 20s. Our result was in the low 30s, so we beat OpenAI on that benchmark by 20%.”\nThese benchmarks matter because they measure an AI’s ability to solve real-world coding problems, not just generate syntactically correct but functionally flawed code.\nMulti-agent architecture: Zencoder’s answer to code quality and security concerns\nA significant concern among developers regarding AI coding tools is whether they produce secure, high-quality code. Zencoder’s approach, according to Filev, is to build on established software engineering best practices rather than reinventing them.\n“I think when we design AI systems, we definitely should borrow from the wisdom of human systems. The software engineering industry was rapidly developing for the last 40 years,” Filev explained. “Sometimes you don’t have to reinvent the wheel. Sometimes the best approach is to take whatever best practices and tools are in the market and leverage them.”\nThis philosophy manifests in Zencoder’s agentic approach, where AI acts as an orchestrator that uses various tools, similar to how human developers use multiple tools in their workflows.\n“We enable AI to use all of those tools,” said Filev. “We’re building a truly multi-agentic platform. In our previous release, we not only shipped coding agents, like some of our competitors, but we also shipped unit testing agents, and you’re going to see more agents from us in that multi-agent interaction platform.”\nCoffee mode and the future: When AI does the work while developers take a break\nOne of Zencoder’s most talked-about features is its recently launched “\nCoffee Mode\n,” which allows developers to set the AI to work on tasks like writing unit tests while they take a break.\n“You can literally hit that button and go grab a coffee, and the agent will do that work by itself,” Filev told VentureBeat in a\nprevious interview\n. “As we like to say in the company, you can watch forever the waterfall, the fire burning, and the agent working in coffee mode.”\nThis approach reflects Zencoder’s vision of AI as a developer’s companion rather than a replacement.\n“We’re not trying to substitute humans,” Filev emphasized. “We’re trying to progressively and rapidly make them 10x more productive. The more powerful the AI technology is, the more powerful is the human that uses it.”\nAs part of the acquisition, Machinet will transfer its domain and marketplace presence to Zencoder. Current Machinet customers will receive guidance on transitioning to Zencoder’s platform, which offers enhanced capabilities through its proprietary Repo Grokking technology and AI agents.\nThe new developer landscape: A rapidly evolving ecosystem\nThe acquisition of\nMachinet\nby\nZencoder\nsignals a turning point in the AI coding assistant market, as larger players absorb innovative smaller companies with specialized expertise. For enterprise decision-makers evaluating AI coding tools, the landscape is shifting from a question of whether to adopt these technologies to which platform will provide the most strategic advantage.\n“Jokingly, I think like half of the Y Combinator batch is AI startups, and it’s just impossible to compete in this space with two engineers at this point,” Filev noted. “You’ve got to have some real resources, technical resources and market resources in order to succeed here.”\nAs industry titans like\nMicrosoft\nand\nOpenAI\ndeepen their investments in this space, companies like\nZencoder\nare carving out distinctive positions based on integration flexibility, benchmark performance, and engineering philosophies that align with enterprise needs.\nFor developers watching this market consolidation unfold, one thing is becoming increasingly clear: the future won’t be about whether AI writes your code, but rather which AI becomes your preferred pair programmer when you return from that coffee break.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Tour Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Tour. See if we're coming to your area!\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:14.752535",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:44:05.128601",
    "audio_file": "981196fc54734376655d1866441ead4e.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/981196fc54734376655d1866441ead4e.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:47:12.820947"
  },
  {
    "id": "a9a29d20f1abf19e44561048f5333859",
    "title": "Ethically trained AI startup Pleias releases new small reasoning models optimized for RAG with built-in citations",
    "url": "https://venturebeat.com/ai/ethically-trained-ai-startup-pleias-releases-new-small-reasoning-models-optimized-for-rag-with-built-in-citations/",
    "authors": "Carl Franzen",
    "published_date": "Thu, 24 Apr 2025 17:03:56 +0000",
    "source": "VentureBeat AI",
    "summary": "一家注重倫理訓練的AI新創公司 Pleias 推出了新的小型推理模型，特別針對具備內建引用功能的 RAG 進行優化。這些模型適合整合到搜尋增強助手、教育工具和使用者支援系統中，有助於提升搜尋效能和使用者體驗。",
    "content": "Pleias emphasizes the models’ suitability for integration into search-augmented assistants, educational tools, and user support systems.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:15.186255",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:44:06.611779",
    "audio_file": "a9a29d20f1abf19e44561048f5333859.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/a9a29d20f1abf19e44561048f5333859.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:47:20.623206"
  },
  {
    "id": "3af562527411e8986b006b9d547d4da5",
    "title": "Evil Geniuses and Theta Labs launch AI chatbot based on esports mascot Meesh",
    "url": "https://venturebeat.com/games/evil-geniuses-and-theta-labs-launch-ai-chatbot-based-on-esports-mascot-meesh/",
    "authors": "Dean Takahashi",
    "published_date": "Thu, 24 Apr 2025 14:00:00 +0000",
    "source": "VentureBeat AI",
    "summary": "Evil Geniuses和Theta Labs合作推出基於電競吉祥物Meesh的AI聊天機器人，讓粉絲可以和Meesh互動，包括嘻哈對話、問答遊戲等。Evil Geniuses一直致力於AI技術，這次他們的Meesh聊天機器人是在Theta Labs的協助下開發的。粉絲可以在Evil Geniuses的Discord伺服器和官方網站上與Meesh互動，提問有關Evil Geniuses的問題。這個AI聊天機器人讓粉絲更貼近他們喜愛的電競團隊。",
    "content": "Evil Geniuses and Theta Labs launch AI chatbot based on esports mascot Meesh | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nEvil Geniuses and Theta Labs launch AI chatbot based on esports mascot Meesh\nDean Takahashi\n@deantak\nApril 24, 2025 7:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nEvil Geniuses and Theta Labs have created an AI avatar chatbot based on Meesh.\nImage Credit: Evil Geniuses\nEvil Geniuses\n, the well-known esports organization and brand, has launched its Meesh AI chatbot in a partnership with\nTheta Labs\n.\nEvil Geniuses, the 2023 Valorant Champions Tour (VCT) world champion, has launched the text-based chatbot that was built by Theta Labs, a provider of decentralized cloud infrastructure for AI, media, and entertainment.\nThe new AI chatbot has been personalized to bring the loveable critter’s personality to life, allowing fans the chance to interact with Meesh in a variety of ways, including meme-worthy trash talk, trivia challenges, personalized interactions, and much more.\n“EG has always been a pioneer in the AI space, starting around 2021 and 2022. We helped bring Moneyball to esports, using data scientists to build AI tools and advanced algorithms to scout talent across the globe and help define winning strategies,” said Chris DeAppolonio CEO at Evil Geniuses, in an interview with GamesBeat. “We won a Sports Business Journal award because of it in 2022 and have continued that work through our competitive teams, especially on Valorant.”\nChris DeAppolonio CEO at Evil Geniuses.\nHe added, “We’re always looking at ways we can utilize technology to empower champions every day, whether that’s our own players, whether that’s our fans, whether that’s the people here at EEG, and for us, you know, we it kind of all came together this this year because we also launched EEG’s first ever critter.”\n(Other people would call this a mascot). Meesh is a minion.\n“And every evil genius needs their own minion,” DeAppolonio said. “After extensive work with focus groups of our fans, we helped Theta create an IP, a critter Meesh, which is short for mischievous.”\nMeesh is fully integrated into Evil Geniuses Discord server and official website, giving fans the chance to ask them questions about all things Evil Geniuses, at all times of the day. Through Theta EdgeCloud, Meesh will use AI to answer community questions that include things like match schedule, team standings, roster information and history, player stats and past highlights, along with providing iconic EG esports moments on demand.\nFans will also be able to participate in exclusive giveaways, community events, and find the latest news about the organization.\n“Our team at Evil Geniuses worked directly with the Theta team to create an AI chatbot that is a true representation of the personality we envisioned for Meesh when we started imagining our digital pet ahead of their launch last year,” said Kayci Evans, head of marketing and global partnerships at Evil Geniuses, in a statement. “Now, with the help of Theta, that personality is on full display and we couldn’t be happier with the results. Our fans can now fully engage with Meesh to learn more about our history, ask what’s coming up for Evil Geniuses or simply chat with Meesh for a personal interaction.”\nKayci Evans, head of marketing and global partnerships at Evil Geniuses.\nEvil Geniuses Theta partnership is the first of more exciting partnerships to come for the organization as it puts an emphasis on finding new and unique ways for fans to become more immersed in Evil Geniuses happenings.\nDeAppolonio said that Meesh gives fans another touchpoint to engage with the brand. The company has embued Meesh with a personality and built a storyline and lore around the creature as someone that fans can interact with.\nWhile Meesh isn’t so cutting edge as to be able to speak with fans, it can take typed questions from the fans and respond to them with text replies. Meesh is also animated in ways that make the character more believable. There are some Easter Eggs in the AI responses for passionate fans. Meesh is also integrated with the Discord community for Evil Geniuses.\n“We want Meesh to come to live for our fans, to really make Meesh feel alive and unique as they are,” DeAppolonio said. “As a character, Meesh is fairly non-verbal. Meesh speaks in sounds and squeaks, but can type. When fans type questions, Meesh can respond with dialogue.”\nMeesh’s personality is strong and replicates what we believe our fans connect with, their hobbies and their interests and all those things. In a demo, Kayci Evans, global head of marketing at Evil Geniuses, took me into the environment of Meesh’s lair, which is very artistic. the demo didn’t work so well at first but Meesh started responding after a bit. Meesh can answer questions like where will Evil Geniuses play next in its worldwide schedule and it can tell inside jokes that fans get.\nEvans said, “We updated and uploaded the profiles of all of our players into the AI to make sure that Meesh really felt like part of the team and could teach fans about what our team does in their free time. You can ask Meesh for advice and it can tell you about games or other lore about the brand.\nMeesh widget\nEvil Geniuses also had to make sure that Meesh wouldn’t write anything inappropriate. That’s not an easy thing to protect against, given that fans will no doubt probe for such vulnerabilities in Meesh. Theta had experience in working with other teams in the NHL to do similar systems that could handle toxicity.\n“It’s something we continue to monitor,” DeAppolonio said.\n“We created Easter Eggs for fans who want to engage in a different way, and lead them down a path of exploration and fun while getting to know the personality of our brand in a way that is really unique to us,” Evans said. “The most fun part has been getting to push our partner, Theta, in more creative directions. We have a great partner in theta who’s willing to explore how to make it more unique and creative.”\nThis is all happening in a context where AI has been polarizing for the esports industry. Some view AI as something that could potentially take away people’s jobs, but the company sees this as empowering people to do their work in a more creative way.\n“AI has been a way to enhance our gameplay, enhance how we compete and scout talent across the globe, specifically because games are based on data, and we can get millions and millions of players and rate them and figure out who could be great, no matter what their background is and where they’re from,” said DeAppolonio. “This AI allows us to connect and engage with fans in a way we couldn’t on an everyday basis.”\nEvil Geniuses is a global esports organization at the intersection of gaming, sports, technology and entertainment. Founded in 1999, Evil Geniuses is the oldest North American esports organization with more than 180+ championships.\nTheta\nLabs is a provider of decentralized cloud infrastructure for AI, media and entertainment powered by a global network of 30,000 distributed edge nodes and a native blockchain. Backed by Samsung, Sony, Bertelsmann Digital Media Investments and CAA, Theta is among the top\nAI tokens on Binance.com\nand top 10 DePIN blockchains by market capitalization on\nCoingecko\n.\nTheta EdgeCloud\nis the first hybrid cloud-edge AI computing platform with over 80 PetaFLOPS of always available distributed GPU compute power.\nEvans said that costs for doing chatbots using AI have gone down significantly, enabling the company to do something with low cloud compute and storage costs. AI is a new frontier, but it’s one that Evil Geniuses believes in and wants to be a pioneer in,DeAppolonio said.\n“We want to make sure that we’re using technology to help improve our business and make informed decisions and provide engaging content for our fans, where we can while still embracing the creativity and opportunity for esports enthusiasts and creators to build on our platforms as well,” he said.\nAs for the state of esports itself, DeAppolonio said, “I’m feeling a lot better about the industry these days. We’ve had continued growth on viewership for Valorant, which is the game we currently play. We won the world championship in 2023. From an EEG perspective, our merchandise sales are up significantly, 250% year over year. Our average video views are up 40% year over year.”\nHe added, “We’re moving upwards. We’re seeing higher engagement rates, having much more conversations with brands now than we’ve had in the past few years. I wouldn’t say it’s pre-pandemic levels when esports was at its highest, but the esports winter feels like it’s starting to end.”\nGB Daily\nStay in the know! Get the latest news in your inbox daily\nSubscribe\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nYour daily dose of gaming insights\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:15.496072",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:44:09.456538",
    "audio_file": "3af562527411e8986b006b9d547d4da5.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/3af562527411e8986b006b9d547d4da5.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:47:31.283011"
  },
  {
    "id": "e8ba003e4515f86eed61927d6443c130",
    "title": "Google adds more AI tools to its Workspace productivity apps",
    "url": "https://venturebeat.com/ai/google-adds-more-ai-tools-to-its-workspace-productivity-apps/",
    "authors": "Emilia David",
    "published_date": "Wed, 23 Apr 2025 22:43:25 +0000",
    "source": "VentureBeat AI",
    "summary": "Google在其Workspace生產力應用程式中新增更多AI工具，包括音頻概述和Canvas功能。音頻概述讓使用者能夠根據上傳的文件和投影片創建音頻文件，並在深度研究報告中生成音頻概述。Canvas功能則讓使用者使用Gemini模型創建草稿並優化文字或程式碼。此外，Google還簡化了用戶將事件和會議添加到日曆的流程。這些更新提供了更多創造和協作的方式，提升了工作效率。",
    "content": "Google adds more AI tools to its Workspace productivity apps | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nGoogle adds more AI tools to its Workspace productivity apps\nEmilia David\n@miyadavid\nApril 23, 2025 3:43 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nGoogle\ncontinues to bring its flagship AI models to its productivity apps, expanding its Gemini features.\nThe company today announced several updates to its Workspace products, including the addition of Audio Overviews and new streamlined methods for tracking meetings.\nAudio Overviews, which was first introduced in\nGoogle’s popular NotebookLM\n, allows people to create podcasts on their chosen research topic.\nNow, through Gemini, users can create audio files based on uploaded documents and slides. They can also generate\naudio overviews within\ndeep re\nsearch reports. These podcast-style audio files are downloadable. Audio Overview generates voices and grounds its discussions solely on the provided documents.\nGoogle\npreviously told VentureBeat\nthat its tests showed some people prefer learning through listening, where information is presented in a conversational format.\nThe company also launched a new feature called Canvas in Gemini, which lets people create drafts and refine text or code using the Gemini model. Google said Canvas helps “generate, optimize and preview code.” Canvas documents can be shared with Google Docs.\nUpdated calendars\nGoogle also streamlined how users can add events and meetings to their calendars. Gemini will detect if an email contains details of events and can prompt people to add it to their calendar. The model will surface emails with potential appointments if the user misses them.\nSome plug-ins for Google, such as Boomerang, offer similar features that display appointments above the subject line. The Gemini-powered calendar feature will open a Gemini chat window alerting the user of the event.\nPointing AI models to surface data or events from emails has become a cornerstone of enterprise AI assistants and agents. Microsoft’s\nnew agents parse through emails\nfor input. Startup\nMartin AI has an AI assistant\nthat manages calendars, emails and to-do lists.\nMelding generative AI with productivity\nGoogle added\nGemini chat to Workspace\nlast year to integrate the standalone chat platform with Gmail, Google Docs and Calendars. This brought Google closer to Microsoft’s Copilot, which added AI models to its productivity platforms, including Outlook.\nEnterprises continue to add AI features to the workplace, and it’s possible that if their employees regularly access things like Gemini on their Gmails and use AI models for research, AI adoption rates may be even higher.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe AI Impact Tour Dates\nJoin leaders in enterprise AI for networking, insights, and engaging conversations at the upcoming stops of our AI Impact Tour. See if we're coming to your area!\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:15.793401",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:44:11.714269",
    "audio_file": "e8ba003e4515f86eed61927d6443c130.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/e8ba003e4515f86eed61927d6443c130.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:47:41.965753"
  }
]