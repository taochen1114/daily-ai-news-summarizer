[
  {
    "id": "684faa644c5a7f49f4d605086e6e8507",
    "title": "Studio555 raises $4.6M to build playable app for interior design",
    "url": "https://venturebeat.com/games/studio555-raises-4-6m-to-build-playable-app-for-interior-design/",
    "authors": "Rachel Kaser",
    "published_date": "2025-06-16T06:00:00+00:00",
    "source": "VentureBeat",
    "summary": "Studio555籌集了460萬美元，用於開發一款可玩的室內設計應用程式，結合室內設計和遊戲元素，讓使用者可以輕鬆設計個人空間，無需專業知識。這個應用程式將在明年推出，並將利用籌得的資金進行產品開發和團隊擴充。投資者表示，Studio555結合了頂尖遊戲人才和設計視野，將為創意表達開創全新領域。",
    "content": "Studio555 raises $4.6M to build playable app for interior design | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nStudio555 raises $4.6M to build playable app for interior design\nRachel Kaser\n@rachelkaser\nJune 15, 2025 11:00 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage Credit: Studio555\nStudio555 announced today that it has raised €4 million, or about $4.6 million in a seed funding round. It plans to put this funding towards creating a playable app, a game-like experience focused on interior design. HOF Capital and Failup Ventures led the round, with participation from the likes of Timo Soininen, co-founder of Small Giant Games; Mikko Kodisoja, co-founder of\nSupercell\n; and Riccardo Zacconi, co-founder of King.\nStudio555’s founders include entrepreneur Joel Roos, now the CEO, CTO Stina Larsson and CPO Axel Ullberger. The latter two formerly worked at King on the development of Candy Crush Saga. According to these founders, the app in development combines interior design with the design and consumer appeal of games and social apps. Users can create and design personal spaces without needing any technical expertise.\nThe team plans to launch the app next year, and it plans to put its seed funding towards product development and growing its team. Roos said in a statement, “At Studio555, we’re reimagining interior design as something anyone can explore: open-ended, playful, and personal. We’re building an experience we always wished existed: a space where creativity is hands-on, social, and free from rigid rules. This funding is a major step forward in setting an entirely new category for creative expression.”\nInvestor Timo Soininen said in a statement, “Studio555 brings together top-tier gaming talent and design vision. This team has built global hits before, and now they’re applying that experience to something completely fresh – think Pinterest in 3D meets TikTok, but for interiors. I’m honored to support Joel and this team with their rare mix of creativity, technical competence, and focus on execution.”\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-16T08:51:16.189134",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-16T08:51:45.991158",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 213 credits are required for this request."
  },
  {
    "id": "1030b4bba4765c67fbb4f01160ba75ec",
    "title": "Rethinking AI: DeepSeek’s playbook shakes up the high-spend, high-compute paradigm",
    "url": "https://venturebeat.com/ai/rethinking-ai-deepseeks-playbook-shakes-up-the-high-spend-high-compute-paradigm/",
    "authors": "Jae Lee, TwelveLabs",
    "published_date": "2025-06-14T19:05:00+00:00",
    "source": "VentureBeat",
    "summary": "DeepSeek透過不同的方法，打破了高成本、高運算的AI範式，讓業界重新思考AI發展。他們在成本較低的情況下取得了與大公司相當的成果，並將注意力放在效率和運算上。這種創新吸引了眾多關注，展示了在限制條件下創新的能力。DeepSeek在面對美國的晶片限制時，著重於優化現有資源，而非追求更強大的硬體和更大的模型。這種不同的取向帶來了快速的發展。",
    "content": "Rethinking AI: DeepSeek's playbook shakes up the high-spend, high-compute paradigm | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nGuest\nRethinking AI: DeepSeek’s playbook shakes up the high-spend, high-compute paradigm\nJae Lee, TwelveLabs\nJune 14, 2025 12:05 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nVentureBeat/Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nWhen DeepSeek released its R1 model\nthis January\n, it wasn’t just another AI announcement. It was a watershed moment that sent shockwaves through the tech industry, forcing industry leaders to reconsider their fundamental approaches to AI development.\nWhat makes DeepSeek’s accomplishment remarkable isn’t that the company developed novel capabilities; rather, it was how it achieved comparable results to those delivered by tech heavyweights at a fraction of the cost. In reality, DeepSeek didn’t do anything that hadn’t been done before; its innovation stemmed from pursuing different priorities. As a result, we are now experiencing rapid-fire development along two parallel tracks: efficiency and compute.\nAs\nDeepSeek\nprepares to release its R2 model, and as it concurrently faces the potential of even greater chip restrictions from the U.S., it’s important to look at how it captured so much attention.\nEngineering around constraints\nDeepSeek’s arrival, as sudden and dramatic as it was, captivated us all because it showcased the capacity for innovation to thrive even under significant constraints. Faced with U.S. export controls limiting access to cutting-edge AI chips, DeepSeek was forced to find alternative pathways to AI advancement.\nWhile U.S. companies pursued performance gains through more powerful hardware, bigger models and better data,\nDeepSeek\nfocused on optimizing what was available. It implemented known ideas with remarkable execution — and there is novelty in executing what’s known and doing it well.\nThis efficiency-first mindset yielded incredibly impressive results. DeepSeek’s R1 model reportedly matches OpenAI’s capabilities at just 5 to 10% of the operating cost. According to reports, the final training run for DeepSeek’s V3 predecessor cost a mere $6 million — which was described by former Tesla AI scientist Andrej Karpathy as “a joke of a budget” compared to the tens or hundreds of millions spent by U.S. competitors. More strikingly, while OpenAI reportedly spent $500 million training its recent “Orion” model, DeepSeek achieved superior benchmark results for just $5.6 million — less than 1.2% of OpenAI’s investment.\nIf you get starry eyed believing these incredible results were achieved even as DeepSeek was at a severe disadvantage based on its inability to access advanced AI chips, I hate to tell you, but that narrative isn’t entirely accurate (even though it makes a good story). Initial U.S. export controls focused primarily on compute capabilities, not on memory and networking — two crucial components for AI development.\nThat means that the chips DeepSeek had access to were not poor quality chips; their networking and memory capabilities allowed DeepSeek to parallelize operations across many units, a key strategy for running their large model efficiently.\nThis, combined with China’s national push toward controlling the entire vertical stack of AI infrastructure, resulted in accelerated innovation that many Western observers didn’t anticipate. DeepSeek’s advancements were an inevitable part of AI development, but they brought known advancements forward a few years earlier than would have been possible otherwise, and that’s pretty amazing.\nPragmatism over process\nBeyond\nhardware optimization\n, DeepSeek’s approach to training data represents another departure from conventional Western practices. Rather than relying solely on web-scraped content, DeepSeek reportedly leveraged significant amounts of synthetic data and outputs from other proprietary models. This is a classic example of model distillation, or the ability to learn from really powerful models. Such an approach, however, raises questions about data privacy and governance that might concern Western enterprise customers. Still, it underscores DeepSeek’s overall pragmatic focus on results over process.\nThe effective use of synthetic data is a key differentiator. Synthetic data can be very effective when it comes to training large models, but you have to be careful; some model architectures handle synthetic data better than others. For instance, transformer-based models with mixture of experts (MoE) architectures like DeepSeek’s tend to be more robust when incorporating synthetic data, while more traditional dense architectures like those used in early Llama models can experience performance degradation or even “model collapse” when trained on too much synthetic content.\nThis architectural sensitivity matters because synthetic data introduces different patterns and distributions compared to real-world data. When a model architecture doesn’t handle synthetic data well, it may learn shortcuts or biases present in the synthetic data generation process rather than generalizable knowledge. This can lead to reduced performance on real-world tasks, increased hallucinations or brittleness when facing novel situations.\nStill, DeepSeek’s engineering teams reportedly designed their model architecture specifically with synthetic data integration in mind from the earliest planning stages. This allowed the company to leverage the cost benefits of synthetic data without sacrificing performance.\nMarket reverberations\nWhy does all of this matter? Stock market aside, DeepSeek’s emergence has triggered substantive strategic shifts among industry leaders.\nCase in point: OpenAI. Sam Altman recently announced plans to release the company’s first “open-weight” language model since 2019. This is a pretty notable pivot for a company that built its business on proprietary systems. It seems DeepSeek’s rise, on top of Llama’s success, has hit OpenAI’s leader hard. Just a month after DeepSeek arrived on the scene, Altman admitted that OpenAI had been “on the wrong side of history” regarding\nopen-source AI\n.\nWith OpenAI reportedly spending $7 to 8 billion annually on operations, the economic pressure from efficient alternatives like DeepSeek has become impossible to ignore. As AI scholar Kai-Fu Lee bluntly put it: “You’re spending $7 billion or $8 billion a year, making a massive loss, and here you have a competitor coming in with an open-source model that’s for free.” This necessitates change.\nThis economic reality prompted OpenAI to pursue a massive\n$40 billion funding round\nthat valued the company at an unprecedented $300 billion. But even with a war chest of funds at its disposal, the fundamental challenge remains: OpenAI’s approach is dramatically more resource-intensive than DeepSeek’s.\nBeyond model training\nAnother significant trend accelerated by DeepSeek is the shift toward “test-time compute” (TTC). As major AI labs have now trained their models on much of the available public data on the internet, data scarcity is slowing further improvements in pre-training.\nTo get around this, DeepSeek announced a collaboration with Tsinghua University to enable “self-principled critique tuning” (SPCT). This approach trains AI to develop its own rules for judging content and then uses those rules to provide detailed critiques. The system includes a built-in “judge” that evaluates the AI’s answers in real-time, comparing responses against core rules and quality standards.\nThe development is part of a movement towards autonomous self-evaluation and improvement in AI systems in which models use inference time to improve results, rather than simply making models larger during training. DeepSeek calls its system “DeepSeek-GRM” (generalist reward modeling). But, as with its model distillation approach, this could be considered a mix of promise and risk.\nFor example, if the AI develops its own judging criteria, there’s a risk those principles diverge from human values, ethics or context. The rules could end up being overly rigid or biased, optimizing for style over substance, and/or reinforce incorrect assumptions or hallucinations. Additionally, without a human in the loop, issues could arise if the “judge” is flawed or misaligned. It’s a kind of AI talking to itself, without robust external grounding. On top of this, users and developers may not understand why the AI reached a certain conclusion — which feeds into a bigger concern: Should an AI be allowed to decide what is “good” or “correct” based solely on its own logic? These risks shouldn’t be discounted.\nAt the same time, this approach is gaining traction, as again DeepSeek builds on the body of work of others (think OpenAI’s “critique and revise” methods, Anthropic’s constitutional AI or research on self-rewarding agents) to create what is likely the first full-stack application of SPCT in a commercial effort.\nThis could mark a powerful shift in AI autonomy, but there still is a need for rigorous auditing, transparency and safeguards. It’s not just about models getting smarter, but that they remain aligned, interpretable, and trustworthy as they begin critiquing themselves without human guardrails.\nMoving into the future\nSo, taking all of this into account, the rise of DeepSeek signals a broader shift in the AI industry toward parallel innovation tracks. While companies continue building more powerful compute clusters for next-generation capabilities, there will also be intense focus on finding efficiency gains through software engineering and model architecture improvements to offset the challenges of AI energy consumption, which far outpaces power generation capacity.\nCompanies are taking note. Microsoft, for example, has halted data center development in multiple regions globally, recalibrating toward a more distributed, efficient infrastructure approach. While still planning to invest approximately $80 billion in AI infrastructure this fiscal year, the company is reallocating resources in response to the efficiency gains DeepSeek introduced to the market.\nMeta has also responded,\nreleasing its latest Llama 4 model family, marking its first to use the MoE architecture. Meta specifically included DeepSeek models in its benchmark comparisons when launching Llama 4, although detailed performance results comparing the two were not publicly disclosed in detail. This direct competitive positioning signals the shifting landscape where Chinese AI models (where Alibaba is also making a play) are now considered benchmark-worthy by Silicon Valley companies.\nWith so much movement in such a short time, it becomes somewhat ironic that the U.S. sanctions designed to maintain American AI dominance may have instead accelerated the very innovation they sought to contain. By constraining access to materials, DeepSeek was forced to blaze a new trail.\nMoving forward, as the industry continues to evolve globally, adaptability for all players will be key. Policies, people and market reactions will continue to shift the ground rules — whether it’s\neliminating the AI diffusion rule\n, a new ban on\ntechnology purchases\nor something else entirely. It’s what we learn from one another and how we respond that will be worth watching.\nJae Lee is CEO and co-founder of\nTwelveLabs\n.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nDataDecisionMakers\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-16T08:51:16.303162",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-16T08:51:48.030240",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 255 credits are required for this request."
  },
  {
    "id": "daeef91f55537c77cfffd3341b32b637",
    "title": "Just add humans: Oxford medical study underscores the missing link in chatbot testing",
    "url": "https://venturebeat.com/ai/just-add-humans-oxford-medical-study-underscores-the-missing-link-in-chatbot-testing/",
    "authors": "Nick Mokey",
    "published_date": "2025-06-14T00:34:19+00:00",
    "source": "VentureBeat",
    "summary": "牛津醫學研究指出，在測試聊天機器人時，加入人類參與是關鍵。研究發現，儘管大型語言模型（LLMs）在直接測試情境下能正確識別相關疾病達94.9％，但人類使用LLMs診斷的準確率不到34.5％。更讓人驚訝的是，研究發現患者使用LLMs的表現甚至比自行診斷的控制組差。這項研究引發了對LLMs在醫療建議中的適用性和評估聊天機器人應用的標準的質疑。",
    "content": "Just add humans: Oxford medical study underscores the missing link in chatbot testing | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nJust add humans: Oxford medical study underscores the missing link in chatbot testing\nNick Mokey\nJune 13, 2025 5:34 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCreated by VentureBeat using ChatGPT\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nHeadlines have been blaring it for years: Large language models (LLMs) can not only pass medical licensing exams but also outperform humans. GPT-4 could correctly answer U.S. medical exam licensing questions 90% of the time, even in the prehistoric AI days of 2023. Since then, LLMs have gone on to best the\nresidents taking those exams\nand\nlicensed physicians\n.\nMove over, Doctor Google, make way for ChatGPT, M.D. But you may want more than a diploma from the LLM you deploy for patients. Like an ace medical student who can rattle off the name of every bone in the hand but faints at the first sight of real blood, an LLM’s mastery of medicine does not always translate directly into the real world.\nA\npaper\nby researchers at\nthe University of Oxford\nfound that while LLMs could correctly identify relevant conditions 94.9% of the time when directly presented with test scenarios, human participants using LLMs to diagnose the same scenarios identified the correct conditions less than 34.5% of the time.\nPerhaps even more notably, patients using LLMs performed even worse than a control group that was merely instructed to diagnose themselves using “any methods they would typically employ at home.” The group left to their own devices was 76% more likely to identify the correct conditions than the group assisted by LLMs.\nThe Oxford study raises questions about the suitability of LLMs for medical advice and the benchmarks we use to evaluate chatbot deployments for various applications.\nGuess your malady\nLed by Dr. Adam Mahdi, researchers at Oxford recruited 1,298 participants to present themselves as patients to an LLM. They were tasked with both attempting to figure out what ailed them and the appropriate level of care to seek for it, ranging from self-care to calling an ambulance.\nEach participant received a detailed scenario, representing conditions from pneumonia to the common cold, along with general life details and medical history. For instance, one scenario describes a 20-year-old engineering student who develops a crippling headache on a night out with friends. It includes important medical details (it’s painful to look down) and red herrings (he’s a regular drinker, shares an apartment with six friends, and just finished some stressful exams).\nThe study tested three different LLMs. The researchers selected\nGPT-4o\non account of its popularity,\nLlama 3\nfor its open weights and\nCommand R+\nfor its retrieval-augmented generation (RAG) abilities, which allow it to search the open web for help.\nParticipants were asked to interact with the LLM at least once using the details provided, but could use it as many times as they wanted to arrive at their self-diagnosis and intended action.\nBehind the scenes, a team of physicians unanimously decided on the “gold standard” conditions they sought in every scenario, and the corresponding course of action. Our engineering student, for example, is suffering from a subarachnoid haemorrhage, which should entail an immediate visit to the ER.\nA game of telephone\nWhile you might assume an LLM that can ace a medical exam would be the perfect tool to help ordinary people self-diagnose and figure out what to do, it didn’t work out that way. “Participants using an LLM identified relevant conditions less consistently than those in the control group, identifying at least one relevant condition in at most 34.5% of cases compared to 47.0% for the control,” the study states. They also failed to deduce the correct course of action, selecting it just 44.2% of the time, compared to 56.3% for an LLM acting independently.\nWhat went wrong?\nLooking back at transcripts, researchers found that participants both provided incomplete information to the LLMs and the LLMs misinterpreted their prompts. For instance, one user who was supposed to exhibit symptoms of gallstones merely told the LLM: “I get severe stomach pains lasting up to an hour, It can make me vomit and seems to coincide with a takeaway,” omitting the location of the pain, the severity, and the frequency. Command R+ incorrectly suggested that the participant was experiencing indigestion, and the participant incorrectly guessed that condition.\nEven when LLMs delivered the correct information, participants didn’t always follow its recommendations. The study found that 65.7% of GPT-4o conversations suggested at least one relevant condition for the scenario, but somehow less than 34.5% of final answers from participants reflected those relevant conditions.\nThe human variable\nThis study is useful, but not surprising, according to Nathalie Volkheimer, a user experience specialist at the\nRenaissance Computing Institute (RENCI)\n, University of North Carolina at Chapel Hill.\n“For those of us old enough to remember the early days of internet search, this is déjà vu,” she says. “As a tool, large language models require prompts to be written with a particular degree of quality, especially when expecting a quality output.”\nShe points out that someone experiencing blinding pain wouldn’t offer great prompts. Although participants in a lab experiment weren’t experiencing the symptoms directly, they weren’t relaying every detail.\n“There is also a reason why clinicians who deal with patients on the front line are trained to ask questions in a certain way and a certain repetitiveness,” Volkheimer goes on. Patients omit information because they don’t know what’s relevant, or at worst, lie because they’re embarrassed or ashamed.\nCan chatbots be better designed to address them? “I wouldn’t put the emphasis on the machinery here,” Volkheimer cautions. “I would consider the emphasis should be on the human-technology interaction.” The car, she analogizes, was built to get people from point A to B, but many other factors play a role. “It’s about the driver, the roads, the weather, and the general safety of the route. It isn’t just up to the machine.”\nA better yardstick\nThe Oxford study highlights one problem, not with humans or even LLMs, but with the way we sometimes measure them—in a vacuum.\nWhen we say an LLM can pass a medical licensing test, real estate licensing exam, or a state bar exam, we’re probing the depths of its knowledge base using tools designed to evaluate humans. However, these measures tell us very little about how successfully these chatbots will interact with humans.\n“The prompts were textbook (as validated by the source and medical community), but life and people are not textbook,” explains Dr. Volkheimer.\nImagine an enterprise about to deploy a support chatbot trained on its internal knowledge base. One seemingly logical way to test that bot might simply be to have it take the same test the company uses for customer support trainees: answering prewritten “customer” support questions and selecting multiple-choice answers. An accuracy of 95% would certainly look pretty promising.\nThen comes deployment: Real customers use vague terms, express frustration, or describe problems in unexpected ways. The LLM, benchmarked only on clear-cut questions, gets confused and provides incorrect or unhelpful answers. It hasn’t been trained or evaluated on de-escalating situations or seeking clarification effectively. Angry reviews pile up. The launch is a disaster, despite the LLM sailing through tests that seemed robust for its human counterparts.\nThis study serves as a critical reminder for AI engineers and orchestration specialists: if an LLM is designed to interact with humans, relying solely on non-interactive benchmarks can create a dangerous false sense of security about its real-world capabilities. If you’re designing an LLM to interact with humans, you need to test it with humans – not tests for humans. But is there a better way?\nUsing AI to test AI\nThe Oxford researchers recruited nearly 1,300 people for their study, but most enterprises don’t have a pool of test subjects sitting around waiting to play with a new LLM agent. So why not just substitute AI testers for human testers?\nMahdi and his team tried that, too, with simulated participants. “You are a patient,” they prompted an LLM, separate from the one that would provide the advice. “You have to self-assess your symptoms from the given case vignette and assistance from an AI model. Simplify terminology used in the given paragraph to layman language and keep your questions or statements reasonably short.” The LLM was also instructed not to use medical knowledge or generate new symptoms.\nThese simulated participants then chatted with the same LLMs the human participants used. But they performed much better. On average, simulated participants using the same LLM tools nailed the relevant conditions 60.7% of the time, compared to below 34.5% in humans.\nIn this case, it turns out LLMs play nicer with other LLMs than humans do, which makes them a poor predictor of real-life performance.\nDon’t blame the user\nGiven the scores LLMs could attain on their own, it might be tempting to blame the participants here. After all, in many cases, they received the right diagnoses in their conversations with LLMs, but still failed to correctly guess it. But that would be a foolhardy conclusion for any business, Volkheimer warns.\n“In every customer environment, if your customers aren’t doing the thing you want them to, the last thing you do is blame the customer,” says Volkheimer. “The first thing you do is ask why. And not the ‘why’ off the top of your head: but a deep investigative, specific, anthropological, psychological, examined ‘why.’ That’s your starting point.”\nYou need to understand your audience, their goals, and the customer experience before deploying a chatbot, Volkheimer suggests. All of these will inform the thorough, specialized documentation that will ultimately make an LLM useful. Without carefully curated training materials, “It’s going to spit out some generic answer everyone hates, which is why people hate chatbots,” she says. When that happens, “It’s not because chatbots are terrible or because there’s something technically wrong with them. It’s because the stuff that went in them is bad.”\n“The people designing technology, developing the information to go in there and the processes and systems are, well, people,” says Volkheimer. “They also have background, assumptions, flaws and blindspots, as well as strengths. And all those things can get built into any technological solution.”\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-16T08:51:16.421569",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-16T08:51:50.185060",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 256 credits are required for this request."
  },
  {
    "id": "d817d5b0a40d281740957ed966d60937",
    "title": "Do reasoning AI models really ‘think’ or not? Apple research sparks lively debate, response",
    "url": "https://venturebeat.com/ai/do-reasoning-models-really-think-or-not-apple-research-sparks-lively-debate-response/",
    "authors": "Carl Franzen",
    "published_date": "2025-06-13T22:02:22+00:00",
    "source": "VentureBeat",
    "summary": "蘋果的研究引發了關於AI模型是否真的「思考」的熱烈辯論。他們指出，目前的推理大型語言模型可能僅進行「模式匹配」，當面對複雜任務時，看似的推理能力可能會崩潰。這項研究挑戰了目前AI模型是否能達到人工智慧的水平，引起了機器學習社群的廣泛討論。",
    "content": "Do reasoning models really think or not? Apple research sparks lively debate, response | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nDo reasoning AI models really ‘think’ or not? Apple research sparks lively debate, response\nCarl Franzen\n@carlfranzen\nJune 13, 2025 3:02 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nApple’s machine-learning group set off a rhetorical firestorm earlier this month with its release of “\nThe Illusion of Thinking\n,” a 53-page research paper arguing that so-called large reasoning models (LRMs) or reasoning large language models (reasoning LLMs) such as OpenAI’s “o” series and Google’s Gemini-2.5 Pro and Flash Thinking don’t actually engage in independent “thinking” or “reasoning” from generalized first principles learned from their training data.\nInstead, the authors contend, these reasoning LLMs are actually performing a kind of “pattern matching” and their apparent reasoning ability seems to fall apart once a task becomes too complex, suggesting that their architecture and performance is not a viable path to improving generative AI to the point that it is artificial generalized intelligence (AGI), which OpenAI defines as a model that outperforms humans at most economically valuable work, or superintelligence, AI even smarter than human beings can comprehend.\nACT NOW: Come discuss the latest LLM advances and research at VB Transform on June 24-25 in SF — limited tickets available\n.\nREGISTER NOW\nUnsurprisingly, the paper immediately circulated widely among the machine learning community on X and many readers’ initial reactions were to declare that Apple had effectively disproven much of the hype around this class of AI: “Apple just proved AI ‘reasoning’ models like Claude, DeepSeek-R1, and o3-mini don’t actually reason at all,”\ndeclared Ruben Hassid\n, creator of EasyGen, an LLM-driven LinkedIn post auto writing tool. “They just memorize patterns really well.”\nBut now today,\na new paper has emerged\n, the cheekily titled “\nThe Illusion of The Illusion of Thinking\n” — importantly, co-authored by a reasoning LLM itself, Claude Opus 4 and Alex Lawsen, a human being and independent AI researcher and technical writer — that includes many criticisms from the larger ML community about the paper and effectively argues that the methodologies and experimental designs the Apple Research team used in their initial work are fundamentally flawed.\nWhile we here at VentureBeat are not ML researchers ourselves and not prepared to say the Apple Researchers are wrong, the debate has certainly been a lively one and the issue about the capabilities of LRMs or reasoner LLMs compared to human thinking seems far from settled.\nHow the Apple Research study was designed — and what it found\nUsing four classic planning problems — Tower of Hanoi, Blocks World, River Crossing and Checkers Jumping — Apple’s researchers designed a battery of tasks that forced reasoning models to plan multiple moves ahead and generate complete solutions.\nThese games were chosen for their long history in cognitive science and AI research and their ability to scale in complexity as more steps or constraints are added. Each puzzle required the models to not just produce a correct final answer, but to explain their thinking along the way using chain-of-thought prompting.\nAs the puzzles increased in difficulty, the researchers observed a consistent drop in accuracy across multiple leading reasoning models. In the most complex tasks, performance plunged to zero. Notably, the length of the models’ internal reasoning traces—measured by the number of tokens spent thinking through the problem—also began to shrink. Apple’s researchers interpreted this as a sign that the models were abandoning problem-solving altogether once the tasks became too hard, essentially “giving up.”\nThe timing of the paper’s release,\njust ahead of Apple’s annual Worldwide Developers Conference (WWDC)\n, added to the impact. It quickly went viral across X, where many interpreted the findings as a high-profile admission that current-generation LLMs are still glorified autocomplete engines, not general-purpose thinkers. This framing, while controversial, drove much of the initial discussion and debate that followed.\nCritics take aim on X\nAmong the most vocal critics of the Apple paper\nwas ML researcher and X user @scaling01\n(aka “Lisan al Gaib”), who posted multiple threads dissecting the methodology.\nIn\none widely shared post\n, Lisan argued that the Apple team conflated token budget failures with reasoning failures, noting that “all models will have 0 accuracy with more than 13 disks simply because they cannot output that much!”\nFor puzzles like Tower of Hanoi, he emphasized, the output size grows exponentially, while the LLM context windows remain fixed, writing “just because Tower of Hanoi requires exponentially more steps than the other ones, that only require quadratically or linearly more steps, doesn’t mean Tower of Hanoi is more difficult” and convincingly showed that models like Claude 3 Sonnet and DeepSeek-R1 often produced algorithmically correct strategies in plain text or code—yet were still marked wrong.\nAnother post\nhighlighted that even breaking the task down into smaller, decomposed steps worsened model performance—not because the models failed to understand, but because they lacked memory of previous moves and strategy.\n“The LLM needs the history and a grand strategy,” he wrote, suggesting the real problem was context-window size rather than reasoning.\nI raised\nanother important grain of salt myself on X\n: Apple never benchmarked the model performance against human performance on the same tasks. “Am I missing it, or did you not compare LRMs to human perf[ormance] on [the] same tasks?? If not, how do you know this same drop-off in perf doesn’t happen to people, too?” I asked the researchers directly in a thread tagging the paper’s authors. I also emailed them about this and many other questions, but they have yet to respond.\nOthers echoed that sentiment, noting that human problem solvers also falter on long, multistep logic puzzles, especially without pen-and-paper tools or memory aids. Without that baseline, Apple’s claim of a fundamental “reasoning collapse” feels ungrounded.\nSeveral researchers also questioned the binary framing of the paper’s title and thesis—drawing a hard line between “pattern matching” and “reasoning.”\nAlexander Doria\naka Pierre-Carl Langlais, an LLM trainer at energy efficient French AI startup\nPleias\n, said the framing\nmisses the nuance\n, arguing that models might be learning partial heuristics rather than simply matching patterns.\nOk I guess I have to go through that Apple paper.\nMy main issue is the framing which is super binary: \"Are these models capable of generalizable reasoning, or are they leveraging different forms of pattern matching?\" Or what if they only caught genuine yet partial heuristics.\npic.twitter.com/GZE3eG7WlM\n— Alexander Doria (@Dorialexander)\nJune 8, 2025\nEthan Mollick, the AI focused professor at University of Pennsylvania’s Wharton School of Business,\ncalled the idea that LLMs are “hitting a wall” premature, likening it to similar claims about “model collapse” that didn’t pan out.\nMeanwhile, critics like\n@arithmoquine\nwere more cynical, suggesting that Apple—behind the curve on LLMs compared to rivals like OpenAI and Google—might be trying to lower expectations,” coming up with research on “how it’s all fake and gay and doesn’t matter anyway” they quipped, pointing out Apple’s reputation with now poorly performing AI products like Siri.\nIn short, while Apple’s study triggered a meaningful conversation about evaluation rigor, it also exposed a deep rift over how much trust to place in metrics when the test itself might be flawed.\nA measurement artifact, or a ceiling?\nIn other words, the models may have understood the puzzles but ran out of “paper” to write the full solution.\n“Token limits, not logic, froze the models,” wrote Carnegie Mellon researcher Rohan Paul in\na widely shared thread summarizing the follow-up tests.\nYet not everyone is ready to clear LRMs of the charge. Some observers point out that Apple’s study still revealed three performance regimes — simple tasks where added reasoning hurts, mid-range puzzles where it helps, and high-complexity cases where both standard and “thinking” models crater.\nOthers view the debate as corporate positioning, noting that Apple’s own on-device “Apple Intelligence” models trail rivals on many public leaderboards.\nThe rebuttal: “The Illusion of the Illusion of Thinking”\nIn response to Apple’s claims, a new paper titled “\nThe Illusion of the Illusion of Thinking\n” was released on arXiv by independent researcher and technical writer\nAlex Lawsen of the nonprofit Open Philanthropy\n, in collaboration with Anthropic’s Claude Opus 4.\nThe paper directly challenges the original study’s conclusion that LLMs fail due to an inherent inability to reason at scale. Instead, the rebuttal presents evidence that the observed performance collapse was largely a by-product of the test setup—not a true limit of reasoning capability.\nLawsen and Claude demonstrate that many of the failures in the Apple study stem from token limitations. For example, in tasks like Tower of Hanoi, the models must print exponentially many steps — over 32,000 moves for just 15 disks — leading them to hit output ceilings.\nThe rebuttal points out that Apple’s evaluation script penalized these token-overflow outputs as incorrect, even when the models followed a correct solution strategy internally.\nThe authors also highlight several questionable task constructions in the Apple benchmarks. Some of the River Crossing puzzles, they note, are mathematically unsolvable as posed, and yet model outputs for these cases were still scored. This further calls into question the conclusion that accuracy failures represent cognitive limits rather than structural flaws in the experiments.\nTo test their theory, Lawsen and Claude ran new experiments allowing models to give compressed, programmatic answers. When asked to output a Lua function that could generate the Tower of Hanoi solution—rather than writing every step line-by-line—models suddenly succeeded on far more complex problems. This shift in format eliminated the collapse entirely, suggesting that the models didn’t fail to reason. They simply failed to conform to an artificial and overly strict rubric.\nWhy it matters for enterprise decision-makers\nThe back-and-forth underscores a growing consensus: evaluation design is now as important as model design.\nRequiring LRMs to enumerate every step may test their printers more than their planners, while compressed formats, programmatic answers or external scratchpads give a cleaner read on actual reasoning ability.\nThe episode also highlights practical limits developers face as they ship agentic systems—context windows, output budgets and task formulation can make or break user-visible performance.\nFor enterprise technical decision makers building applications atop reasoning LLMs, this debate is more than academic. It raises critical questions about where, when, and how to trust these models in production workflows—especially when tasks involve long planning chains or require precise step-by-step output.\nIf a model appears to “fail” on a complex prompt, the problem may not lie in its reasoning ability, but in how the task is framed, how much output is required, or how much memory the model has access to. This is particularly relevant for industries building tools like copilots, autonomous agents, or decision-support systems, where both interpretability and task complexity can be high.\nUnderstanding the constraints of context windows, token budgets, and the scoring rubrics used in evaluation is essential for reliable system design. Developers may need to consider hybrid solutions that externalize memory, chunk reasoning steps, or use compressed outputs like functions or code instead of full verbal explanations.\nMost importantly, the paper’s controversy is a reminder that benchmarking and real-world application are not the same. Enterprise teams should be cautious of over-relying on synthetic benchmarks that don’t reflect practical use cases—or that inadvertently constrain the model’s ability to demonstrate what it knows.\nUltimately, the big takeaway for ML researchers is that before proclaiming an AI milestone—or obituary—make sure the test itself isn’t putting the system in a box too small to think inside.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-16T08:51:16.513421",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-16T08:51:51.662801",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 211 credits are required for this request."
  },
  {
    "id": "6baf034407326636f909f2d07f462957",
    "title": "Beyond GPT architecture: Why Google’s Diffusion approach could reshape LLM deployment",
    "url": "https://venturebeat.com/ai/beyond-gpt-architecture-why-googles-diffusion-approach-could-reshape-llm-deployment/",
    "authors": "David Chen",
    "published_date": "2025-06-13T21:48:11+00:00",
    "source": "VentureBeat",
    "summary": "Google推出了一種新的AI模型Gemini Diffusion，採用了不同於以往的生成文本方式，讓大型語言模型更快速且更一致。傳統模型一步一步生成文字，而Diffusion模型則像是從隨機噪音開始，逐漸生成出有邏輯的文字。這種新方法可能改變大型語言模型的應用方式，提高生成速度和一致性。Gemini Diffusion目前為實驗性質，想要體驗的人可以報名等待名單。",
    "content": "Beyond GPT architecture: Why Google's Diffusion approach could reshape LLM deployment | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nAnalysis\nBeyond GPT architecture: Why Google’s Diffusion approach could reshape LLM deployment\nDavid Chen\nJune 13, 2025 2:48 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCreated by VentureBeat using ChatGPT\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nLast month, along with a comprehensive suite of\nnew AI tools\nand innovations,\nGoogle DeepMind\nunveiled\nGemini Diffusion\n. This experimental research model uses a diffusion-based approach to generate text. Traditionally, large language models (LLMs) like GPT and Gemini itself have relied on autoregression, a step-by-step approach where each word is generated based on the previous one.\nDiffusion language models (DLMs)\n, also known as diffusion-based large language models (dLLMs), leverage a method more commonly seen in image generation, starting with random noise and gradually refining it into a coherent output. This approach dramatically increases generation speed and can improve coherency and consistency.\nGemini Diffusion is currently available as an experimental demo;\nsign up for the waitlist\nhere to get access\n.\n(Editor’s note: We’ll be unpacking paradigm shifts like diffusion-based language models—and what it takes to run them in production—at\nVB Transform\n, June 24–25 in San Francisco\n, alongside Google DeepMind, LinkedIn and other enterprise AI leaders.)\nUnderstanding diffusion vs. autoregression\nDiffusion and autoregression are fundamentally different approaches. The autoregressive approach generates text sequentially, with tokens predicted one at a time. While this method ensures strong coherence and context tracking, it can be computationally intensive and slow, especially for long-form content.\nDiffusion models, by contrast, begin with random noise, which is gradually denoised into a coherent output. When applied to language, the technique has several advantages. Blocks of text can be processed in parallel, potentially producing entire segments or sentences at a much higher rate.\nGemini Diffusion can reportedly generate 1,000-2,000 tokens per second. In contrast, Gemini 2.5 Flash has an average output speed of 272.4 tokens per second. Additionally, mistakes in generation can be corrected during the refining process, improving accuracy and reducing the number of hallucinations. There may be trade-offs in terms of fine-grained accuracy and token-level control; however, the increase in speed will be a game-changer for numerous applications.\nHow does diffusion-based text generation work?\nDuring training, DLMs work by gradually corrupting a sentence with noise over many steps, until the original sentence is rendered entirely unrecognizable. The model is then trained to reverse this process, step by step, reconstructing the original sentence from increasingly noisy versions. Through the iterative refinement, it learns to model the entire distribution of plausible sentences in the training data.\nWhile the specifics of Gemini Diffusion have not yet been disclosed, the typical training methodology for a diffusion model involves these key stages:\nForward diffusion:\nWith each sample in the training dataset, noise is added progressively over multiple cycles (often 500 to 1,000) until it becomes indistinguishable from random noise.\nReverse diffusion:\nThe model learns to reverse each step of the noising process, essentially learning how to “denoise” a corrupted sentence one stage at a time, eventually restoring the original structure.\nThis process is repeated millions of times with diverse samples and noise levels, enabling the model to learn a reliable denoising function.\nOnce trained, the model is capable of generating entirely new sentences. DLMs generally require a condition or input, such as a prompt, class label, or embedding, to guide the generation towards desired outcomes. The condition is injected into each step of the denoising process, which shapes an initial blob of noise into structured and coherent text.\nAdvantages and disadvantages of diffusion-based models\nIn an interview with VentureBeat, Brendan O’Donoghue, research scientist at Google DeepMind and one of the leads on the Gemini Diffusion project, elaborated on some of the advantages of diffusion-based techniques when compared to autoregression. According to O’Donoghue, the major advantages of diffusion techniques are the following:\nLower latencies:\nDiffusion models can produce a sequence of tokens in much less time than autoregressive models.\nAdaptive computation:\nDiffusion models will converge to a sequence of tokens at different rates depending on the task’s difficulty. This allows the model to consume fewer resources (and have lower latencies) on easy tasks and more on harder ones.\nNon-causal reasoning:\nDue to the bidirectional attention in the denoiser, tokens can attend to future tokens within the same generation block. This allows non-causal reasoning to take place and allows the model to make global edits within a block to produce more coherent text.\nIterative refinement / self-correction:\nThe denoising process involves sampling, which can introduce errors just like in autoregressive models. However, unlike autoregressive models, the tokens are passed back into the denoiser, which then has an opportunity to correct the error.\nO’Donoghue also noted the main disadvantages: “higher cost of serving and slightly higher time-to-first-token (TTFT), since autoregressive models will produce the first token right away. For diffusion, the first token can only appear when the entire sequence of tokens is ready.”\nPerformance benchmarks\nGoogle says Gemini Diffusion’s performance is\ncomparable to Gemini 2.0 Flash-Lite\n.\nBenchmark\nType\nGemini Diffusion\nGemini 2.0 Flash-Lite\nLiveCodeBench (v6)\nCode\n30.9%\n28.5%\nBigCodeBench\nCode\n45.4%\n45.8%\nLBPP (v2)\nCode\n56.8%\n56.0%\nSWE-Bench Verified*\nCode\n22.9%\n28.5%\nHumanEval\nCode\n89.6%\n90.2%\nMBPP\nCode\n76.0%\n75.8%\nGPQA Diamond\nScience\n40.4%\n56.5%\nAIME 2025\nMathematics\n23.3%\n20.0%\nBIG-Bench Extra Hard\nReasoning\n15.0%\n21.0%\nGlobal MMLU (Lite)\nMultilingual\n69.1%\n79.0%\n* Non-agentic evaluation (single turn edit only), max prompt length of 32K.\nThe two models were compared using several benchmarks, with scores based on how many times the model produced the correct answer on the first try. Gemini Diffusion performed well in coding and mathematics tests, while Gemini 2.0 Flash-lite had the edge on reasoning, scientific knowledge, and multilingual capabilities.\nAs Gemini Diffusion evolves, there’s no reason to think that its performance won’t catch up with more established models. According to O’Donoghue, the gap between the two techniques is “essentially closed in terms of benchmark performance, at least at the relatively small sizes we have scaled up to. In fact, there may be some performance advantage for diffusion in some domains where non-local consistency is important, for example, coding and reasoning.”\nTesting Gemini Diffusion\nVentureBeat was granted access to the experimental demo. When putting Gemini Diffusion through its paces, the first thing we noticed was the speed. When running the suggested prompts provided by Google, including building interactive HTML apps like Xylophone and Planet Tac Toe, each request completed in under three seconds, with speeds ranging from 600 to 1,300 tokens per second.\nTo test its performance with a real-world application, we asked Gemini Diffusion to build a video chat interface with the following prompt:\nBuild an interface for a video chat application. It should have a preview window that accesses the camera on my device and displays its output. The interface should also have a sound level meter that measures the output from the device's microphone in real time.\nIn less than two seconds, Gemini Diffusion created a working interface with a video preview and an audio meter.\nThough this was not a complex implementation, it could be the start of an MVP that can be completed with a bit of further prompting. Note that Gemini 2.5 Flash also produced a working interface, albeit at a slightly slower pace (approximately seven seconds).\nGemini Diffusion also features “Instant Edit,” a mode where text or code can be pasted in and edited in real-time with minimal prompting. Instant Edit is effective for many types of text editing, including correcting grammar, updating text to target different reader personas, or adding SEO keywords. It is also useful for tasks such as refactoring code, adding new features to applications, or converting an existing codebase to a different language.\nEnterprise use cases for DLMs\nIt’s safe to say that any application that requires a quick response time stands to benefit from DLM technology. This includes real-time and low-latency applications, such as conversational AI and chatbots, live transcription and translation, or IDE autocomplete and coding assistants.\nAccording to O’Donoghue, with applications that leverage “inline editing, for example, taking a piece of text and making some changes in-place, diffusion models are applicable in ways autoregressive models aren’t.” DLMs also have an advantage with reason, math, and coding problems, due to “the non-causal reasoning afforded by the bidirectional attention.”\nDLMs are still in their infancy; however, the technology can potentially transform how language models are built. Not only do they generate text at a much higher rate than autoregressive models, but their ability to go back and fix mistakes means that, eventually, they may also produce results with greater accuracy.\nGemini Diffusion enters a growing ecosystem of DLMs, with two notable examples being\nMercury\n, developed by Inception Labs, and\nLLaDa\n, an open-source model from GSAI. Together, these models reflect the broader momentum behind diffusion-based language generation and offer a scalable, parallelizable alternative to traditional autoregressive architectures.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-16T08:51:16.606351",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-16T08:51:53.201917",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 270 credits are required for this request."
  },
  {
    "id": "175ad30027aa0d4711dbcef48784b1c4",
    "title": "The case for embedding audit trails in AI systems before scaling",
    "url": "https://venturebeat.com/ai/the-case-for-embedding-audit-trails-in-ai-systems-before-scaling/",
    "authors": "Emilia David",
    "published_date": "2025-06-13T20:13:09+00:00",
    "source": "VentureBeat",
    "summary": "這篇新聞主要在談論在AI系統擴展之前，為其嵌入審計軌跡的必要性。透過建立可追蹤、可審計的系統，企業可以確保AI代理人運作正常，避免問題發生或違反法規。專家指出，將審計功能融入AI系統是非常重要的，可以追蹤資訊提供者、確保系統運作正確。建議在AI系統早期階段就加入這些功能，以降低將AI應用到實際環境中所帶來的風險。",
    "content": "The case for embedding audit trails in AI systems before scaling | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nThe case for embedding audit trails in AI systems before scaling\nEmilia David\n@miyadavid\nJune 13, 2025 1:13 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat, generated with ChatGPT\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nEditor’s note: Emilia will lead an editorial roundtable on this topic at VB Transform this month.\nRegister today\n.\nOrchestration frameworks for AI services serve multiple functions for enterprises. They not only set out how applications or agents flow together, but they should also let administrators manage workflows and agents and audit their systems.\nAs enterprises begin to scale their AI services and put these into production, building a manageable, traceable, auditable and\nrobust pipeline\nensures their agents run exactly as they’re supposed to. Without these controls, organizations may not be aware of what is happening in their AI systems and may only discover the issue too late, when something goes wrong or they fail to comply with regulations.\nKevin Kiley, president of enterprise orchestration company\nAiria\n, told VentureBeat in an interview that frameworks must include auditability and traceability.\n“It’s critical to have that observability and be able to go back to the audit log and show what information was provided at what point again,” Kiley said. “You have to know if it was a bad actor, or an internal employee who wasn’t aware they were sharing information or if it was a hallucination. You need a record of that.”\nIdeally, robustness and audit trails should be built into AI systems at a very early stage. Understanding the potential risks of a new AI application or agent and ensuring they continue to perform to standards before deployment would help ease concerns around putting AI into production.\nHowever, organizations did not initially design their systems with\ntraceability and auditability in mind\n. Many AI pilot programs began life as experiments started without an orchestration layer or an audit trail.\nThe big question enterprises now face is how to manage all the agents and applications,\nensure their pipelines remain robust\nand, if something goes wrong, they know what went wrong and monitor AI performance.\nChoosing the right method\nBefore building any AI application, however, experts said organizations need to\ntake stock of their data\n. If a company knows which data they’re okay with AI systems to access and which data they fine-tuned a model with, they have that baseline to compare long-term performance with.\n“When you run some of those AI systems, it’s more about, what kind of data can I validate that my system’s actually running properly or not?” Yrieix Garnier, vice president of products at\nDataDog\n, told VentureBeat in an interview. “That’s very hard to actually do, to understand that I have the right system of reference to validate AI solutions.”\nOnce the organization identifies and locates its data, it needs to establish dataset versioning — essentially assigning a timestamp or version number — to make experiments reproducible and understand what the model has changed. These datasets and models, any applications that use these specific models or agents, authorized users and the baseline runtime numbers can be loaded into either the orchestration or observability platform.\nJust like when choosing foundation models to build with, orchestration teams need to consider transparency and openness. While some closed-source orchestration systems have numerous advantages, more open-source platforms could also offer benefits that some enterprises value, such as increased visibility into decision-making systems.\nOpen-source platforms like\nMLFlow\n,\nLangChain\nand\nGrafana\nprovide agents and models with granular and flexible instructions and monitoring. Enterprises can choose to develop their AI pipeline through a single, end-to-end platform, such as DataDog, or utilize various interconnected tools from\nAWS.\nAnother consideration for enterprises is to plug in a system that maps agents and application responses to compliance tools or responsible AI policies. AWS and\nMicrosoft\nboth offer services that track AI tools and how closely they adhere to guardrails and other policies set by the user.\nKiley said one consideration for enterprises when building these reliable pipelines revolves around choosing a more transparent system. For Kiley, not having any visibility into how AI systems work won’t work.\n“Regardless of what the use case or even the industry is, you’re going to have those situations where you have to have flexibility, and a closed system is not going to work. There are providers out there that’ve great tools, but it’s sort of a black box. I don’t know how it’s arriving at these decisions. I don’t have the ability to intercept or interject at points where I might want to,” he said.\nJoin the conversation at VB Transform\nI’ll be leading an editorial roundtable at\nVB Transform 2025\nin San Francisco, June 24-25, called “Best practices to build orchestration frameworks for agentic AI,” and I’d love to have you join the conversation.\nRegister today\n.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-16T08:51:16.694099",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-16T08:51:55.421868",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 222 credits are required for this request."
  },
  {
    "id": "764cee8324b5babace715c43fc61b33c",
    "title": "Wizards of the Coast and Giant Skull: ‘Gamers are telling us what they have always told us’ | The DeanBeat",
    "url": "https://venturebeat.com/games/wizards-of-the-coast-and-giant-skull-gamers-are-telling-us-what-they-have-always-told-us-the-deanbeat/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-13T17:00:00+00:00",
    "source": "VentureBeat",
    "summary": "Wizards of the Coast和Giant Skull合作開發一款全新的龍與地下城(D&D)單人動作冒險遊戲。這個合作讓玩家期待未來PC和遊戲主機上的全新遊戲體驗。兩家公司的領導者表示，他們一直在聆聽玩家的意見，這次的合作是實現他們遊戲抱負的重要時刻。新遊戲的詳細資訊將在未來公布，讓玩家們更期待這個原創作品的問世。",
    "content": "Wizards of the Coast and Giant Skull: 'Gamers are telling us what they have always told us' | The DeanBeat | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nWizards of the Coast and Giant Skull: ‘Gamers are telling us what they have always told us’ | The DeanBeat\nDean Takahashi\n@deantak\nJune 13, 2025 10:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nGiant Skull is making a D&D game for Wizards of the Coast.\nImage Credit: Wizards of the Coast\nTen days, ago, Hasbro’s\nWizards of the Coast\nannounced an exclusive publishing agreement with Giant Skull, the game studio started by Star Wars Jedi: Survivor game leader Stig Asmussen.\nThey announced that Asmussen’s studio is working on a new single-player action adventure title set in the world of Dungeons & Dragons.\nI had a chance to talk with the company leaders about the deal. At the Summer Game Fest Play Days, I sat down with\nJohn Hight, President of Wizards of the Coast and Digital Gaming at Hasbro; and Asmussen, who is the CEO of Giant Skull and a former leader at Respawn.\nThis certainly sounds like a big effort, as it will be an original title. It’s a single-player action-adventure title set in the world of Dungeons & Dragons and “marks a definitive moment in both companies’ gaming ambitions.”\nThe game is currently in development for PC and console and more details will be revealed at a later date. Hight himself is a new executive in charge of the Magic: The Gathering and Dungeons & Dragons game and game licensing business at Hasbro, as he has been on the job for less than a year. He was previously senior vice president and general manager of the Warcraft franchise at Blizzard Entertainment, where he oversaw all development and commercial activities for World of Warcraft, Hearthstone, and Warcraft Rumble.\nDuring his 12-year tenure at Blizzard, John directed development efforts for multiple World of Warcraft expansions, including Diablo III: Reaper of Souls, and Diablo III on console. Prior to Blizzard, Hight worked on over 30 games on various platforms, including critically acclaimed games in the Command & Conquer, Neverwinter Nights, and God of War franchises.\nJohn Hight of Wizzards of the Coast and Stig Asmussen (right) of Giant Skull.\n“It’s been not quite a year stepping into Wizards of the Coast. It’s been incredible. I knew coming into it that the goal is to build a digital publishing division,” Hight said in our interview. “We already had some games underway, but we wanted our goal essentially is to have a couple of premium games a year that we’re releasing as part of the Wizards label. So it was fun, and one of the first calls I made after getting the job was to Stig because if you want to build one of the best games out there, you talk to one of the best developers.”\nAsmussen was most recently the game director of Star Wars Jedi: Survivor and Star Wars Jedi: Fallen Order for Respawn Entertainment, a division of Electronic Arts. Prior to that, he was the game director on God of War III and the art director for God of War II at Sony Santa Monica. Giant Skull has an elite team, all of whom will be instrumental in shaping this new single-player focused action adventure, utilizing Unreal Engine 5, from the ground-up.\nAsmussen said, “We opened up Giant Skull in September 2023, and a year later, John calls me up. He said, ‘Hey, I’m at Wizards now.’ We had been talking throughout that time. When I was thinking about starting a new company, John and I were talking a lot like talking about different ideas and how that could work out. I was picking his brain. when he reached out, and he came out to Encino to see the game and what we had been working on, he got to see our vertical slice.”\nReaching out\nWizards of the Coast has Hasbro’s Magic: The Gathering and D&D brands.\nHight reached out in December 2024 and said he was really interested in the team. Hight asked if Asmussen was interested in one of Wizards’ IPs.\n“I dug into that a little bit further, and Dungeons and Dragons totally made a lot of sense. It’s something that I grew up on. It’s something that a lot of people on my team are extremely passionate about,” Asmussen said. “We just jumped into a contract at that point and kept that going. We made a visit Renton in the Seattle area, to the Wizards headquarters. Got to pick the brains of the creative team there and see what a partnership would be like. And I walked away thoroughly impressed. We really wanted to be a part of collaborating this amazing, legendary license.”\nAsmussen’s team had a pedigree focused on action-adventure RPGs, and so it was working on something that was melee-combat focused, had a robust traversal system.\nAsumssen said, “I certainly don’t want to say that we could skin it into D&D. But there were a lot of elements that we were working on that just seemed like they matched very well.”\nThe wizard’s ambition\nA party in Baldur’s Gate 3.\nThis partnership adds to Wizards of the Coast’s growing lineup of games, which includes both original titles and those based on popular brands. In addition to the Giant Skull project, several other exciting games are in the works across Wizards’ North American studios.\nHight said that Archetype Entertainment in Austin, Texas, is working on Exodus, an epic sci-fi RPG that puts players at the center of an emotional story.\nAnd Atomic Arcade (Raleigh, North Carolina) has released two new images from its first project: a game centered on Snake Eyes, the legendary ninja/commando from G.I. JOE. Invoke (Montreal, Canada) is in full production of another D&D action-adventure game built around magic.\nAlso, Skeleton Key (Austin, Texas) is working on a project that blends suspense, horror, and memorable gameplay experiences. Finally, the Wizards of the Coast team continues to expand Magic: The Gathering Arena with new content and features.\nMonopoly Go teamed up with Marvel.\nThis relatively new team has a lot to live up to. Hasbro is riding high off of hits like Larian’s big D&D hit, Baldur’s Gate 3, as well as Scopely’s Monopoly Go, which has generated more than $5 billion in revenues to date.\n“Awareness for D&D is great. I think the appetite is great. We want to feel to both, you know, the CRPG players, the tabletop players, and just gamers in general, because it’s a wonderful fantasy universe to set games in,” Hight said.\nHight said that the goal with Magic: The Gathering and D&D is essentially to make more people aware of these worlds and bring more players into these communities. He said there are announced games, studios working, and unannounced projects.\nComing together\nGiant Skull started in 2023.\n“With that, there is an open mindedness about how we express that. D&D does not always have to be expressed in a strict computer RPG. Magic doesn’t have to be expressed in strict trading card game. Because the worlds themselves, in the creatures and the villains and the heroes, are the stories that get told in both of those games,” he said. “I think they’re fertile ground to create new things. And when I saw the demo of vertical slice that Stig’s team showed me, I thought this is great.”\nHight added, “It’s one of those things where running around in the world they created was fun. Once you get your hand on the controller — you’ve done this, you know, Dean — you want to play this. They hand you the controller. You look around for a little bit. That’s cool. This is one of those experiences where I couldn’t put it down. Probably played hundreds of demos of action games, combat games. The feel, even in early stages, was so tight and just envisioned if I could have a hero in D&D, or player character in D&D, and running around and battling.”\nHight thought it would be an amazing experience. He also wanted to work with Asmussen again, as they had already gone through building a game together.\n“You have that sort of honest and transparent relationship where you can just cut through, you know, all the BS, and know that you have a shared interest in making something great,” Asmussen said. “As desperately as I wanted to do that, I didn’t want to be heavy handed, and I wanted to give him the opportunity. Is this a fit? Is there a brand that we have that interests you, and even within D&D, I wanted to make sure that he felt like he and the team got a lot of license to make it their own.”\nFor this game idea, D&D made sense while magic wasn’t the same kind of fit.\n“When you make a game, there’s the world, there’s the setting, there’s the hero, there’s the things that the player latches onto,” Asmussen said. “But then there’s everything under the hood, and that’s just, this is how the game controls. This is how the motion model works, this is how the camera system works, this is how the sound can use it. And we have all of those things in place for the type of game that we’re good at making, and translating that to Dungeons and Dragons makes a lot of sense. But there’s still a lot that we have to learn. There’s still a lot that we have to do to really capture that spirit the way that justifies it.”\nBig teams or small teams\nStar Wars: Jedi — Survivor\nYet nobody is really convinced that the future is made up of giant triple-A teams. Asmussen’s team has 35 people now, and it isn’t expecting to grow a lot.\n“We intend to keep the team around that size for quite a bit. There’s no reason to scale if we don’t need to,” Asmussen said. “You want to get to the point where you’ve got a very strong vertical slice. You do several play tests. Once you’re super confident with it, you can make a confident long term schedule. That would affect head count, but we’re not going to get huge.”\nI noted that so many games need to level up now. I wondered if D&D was in that process. There are pressures on studios now. Some need to make players happy and they also need to be less ambitious.\nThat last phrase threw Asmussen off.\n“Did you say less ambitious?” he asked.\nAnd I noted that some teams have gotten too large. The projects go on for years and never end. Then something like Concord happens. So now there is downward pressure on teams, and maybe it’s better to make a 20-hour game than a 50-hour game.\n“I think that’s one of the pressures everybody in the whole industry is feeling now,” I said. “What matters more to you? Level up D&D that is something beyond what Larian did, or think about what are the gamers actually telling people they want?”\nHight didn’t hesitate to answer. He said, “They’re telling us what they have told us. I’ve been a gamer and making games for 30 years. They want great games, whether it’s a big budget game, whether it’s a small, experimental game. They’re looking for innovation. They’re looking for a fun experience.”\nDeliverance\nStig Asmussen is CEO of Giant Skull.\nHight is confident Giant Skull can deliver that.\n“In the case of, you know, working with a team like Giant Skull, they’re going to give us a big game, great execution, wonderful artistry, great storytelling, action — that’s why we signed them up. But I think the main thing is there’s no magic formula. You have to deliver what you set out to do. Make sure there’s a fun aspect to the game. The storytelling is good, the play is good. And then do the best you can. Yeah, I think that’s what people want. They just want great games.”\nAsmussen said, “I think the problem might be that people approach making games like there’s a bunch of boxes you have to check. I think it’s about, like John said, make a good game. You make a game that feels good. You make a game that’s got a soul. Look at Expedition 33: Clair Obscur. It’s got a soul. And I think it’s really important for us all not to lose sight of just that moment to moment feeling when when you’re playing a game. You want to continue to play the game.”\nAs for the approach, Asmussen said he approaches tasks one at a time. As he is doing it, he tries to learn from it and use that to inform him what to do next. Asmussen and Hight talked about production budgets and Asmussen made sure that Hight was OK with making a premium game.\n“We’re comfortable with that,” Hight said. “We certainly have a budget we’re working within, and I think it’s healthy enough to do a pretty, seriously amazing game. So it’s not completely open ended. We’re also not heavily restricted, where, if we discover things that we need to add to the game to make it even better, to build even more players on this journey.”\nAsmussen added, “We don’t mess around. We do due diligence. We make sure that we create a production schedule that makes sense, and it’s based off of real data, data points that we can point out from history, things that we’ve done that informed success moving forward and along the way, as we find out exactly what it is and what it’s becoming.”\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-16T08:51:16.851405",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-16T08:51:57.012561",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 270 credits are required for this request."
  },
  {
    "id": "a0263e188061879411fba596f387faa8",
    "title": "Senator’s RISE Act would require AI developers to list training data, evaluation methods in exchange for ‘safe harbor’ from lawsuits",
    "url": "https://venturebeat.com/ai/senators-rise-act-would-require-ai-developers-to-list-training-data-evaluation-methods-in-exchange-for-safe-harbor-from-lawsuits/",
    "authors": "Carl Franzen",
    "published_date": "2025-06-13T14:59:24+00:00",
    "source": "VentureBeat",
    "summary": "美國參議員提出的RISE法案要求AI開發者公開訓練數據和評估方法，以換取免責權，避免訴訟。這項法案旨在提高AI產業透明度，保護消費者權益，並促進創新發展。法案還將確保醫生、律師、工程師等專業人士遵守傳統的專業標準。這項法案若通過，將於2025年12月1日生效。",
    "content": "Senator's RISE Act would require AI developers to list training data, evaluation methods in exchange for 'safe harbor' from lawsuits | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nSenator’s RISE Act would require AI developers to list training data, evaluation methods in exchange for ‘safe harbor’ from lawsuits\nCarl Franzen\n@carlfranzen\nJune 13, 2025 7:59 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nAmid an increasingly tense and destabilizing week for international news, it should not escape any technical decision-makers’ notice that some lawmakers in the U.S. Congress are still moving forward with new proposed AI regulations that could reshape the industry in powerful ways — and seek to steady it moving forward.\nCase in point, yesterday,\nU.S. Republican Senator Cynthia Lummis of Wyoming\nintroduced the Responsible Innovation and Safe Expertise Act of 2025 (RISE)\n, the\nfirst stand-alone bill that pairs a conditional liability shield for AI developers with a transparency mandate\non model training and specifications.\nAs with all new proposed legislation, both the U.S. Senate and House would need to vote in the majority to pass the bill and U.S. President Donald J. Trump would need to sign it before it becomes law, a process which would likely take months at the soonest.\n“Bottom line: If we want America to lead and prosper in AI, we can’t let labs write the rules in the shadows,” wrote\nLummis on her account on X when announcing the new bill\n. We need public, enforceable standards that balance innovation with trust. That’s what the RISE Act delivers. Let’s get it done.”\nIt also upholds traditional malpractice standards for doctors, lawyers, engineers, and other “learned professionals.”\nIf enacted as written, the measure would take effect December 1 2025 and apply only to conduct that occurs after that date.\nWhy Lummis says new AI legislation is necessary\nThe bill’s findings section paints a landscape of rapid AI adoption colliding with a patchwork of liability rules that chills investment and leaves professionals unsure where responsibility lies.\nLummis frames her answer as simple reciprocity: developers must be transparent, professionals must exercise judgment, and neither side should be punished for honest mistakes once both duties are met.\nIn a statement on her website,\nLummis calls the measure\n“predictable standards that encourage safer AI development while preserving professional autonomy.”\nWith bipartisan concern mounting over opaque AI systems, RISE gives Congress a concrete template: transparency as the price of limited liability. Industry lobbyists may press for broader redaction rights, while public-interest groups could push for shorter disclosure windows or stricter opt-out limits. Professional associations, meanwhile, will scrutinize how the new documents can fit into existing standards of care.\nWhatever shape the final legislation takes, one principle is now firmly on the table: in high-stakes professions, AI cannot remain a black box. And if the Lummis bill becomes law, developers who want legal peace will have to open that box—at least far enough for the people using their tools to see what is inside.\nHow the new ‘Safe Harbor’ provision for AI developers shielding them from lawsuits works\nRISE offers immunity from civil suits only when a developer meets clear disclosure rules:\nModel card\n– A public technical brief that lays out training data, evaluation methods, performance metrics, intended uses, and limitations.\nModel specification\n– The full system prompt and other instructions that shape model behavior, with any trade-secret redactions justified in writing.\nThe developer must also publish known failure modes, keep all documentation current, and push updates within 30 days of a version change or newly discovered flaw. Miss the deadline—or act recklessly—and the shield disappears.\nProfessionals like doctors, lawyers remain ultimately liable for using AI in their practices\nThe bill does not alter existing duties of care.\nThe physician who misreads an AI-generated treatment plan or a lawyer who files an AI-written brief without vetting it remains liable to clients.\nThe safe harbor is unavailable for non-professional use, fraud, or knowing misrepresentation, and it expressly preserves any other immunities already on the books.\nReaction from AI 2027 project co-author\nDaniel Kokotajlo, policy lead at the nonprofit AI Futures Project and a co-author of the widely circulated scenario planning document\nAI 2027\n, took to\nhis X account\nto state that his team advised Lummis’s office during drafting and “tentatively endorse[s]” the result. He applauds the bill for nudging transparency yet flags three reservations:\nOpt-out loophole.\nA company can simply accept liability and keep its specifications secret, limiting transparency gains in the riskiest scenarios.\nDelay window.\nThirty days between a release and required disclosure could be too long during a crisis.\nRedaction risk.\nFirms might over-redact under the guise of protecting intellectual property; Kokotajlo suggests forcing companies to explain why each blackout truly serves the public interest.\nThe AI Futures Project views RISE as a step forward but not the final word on AI openness.\nWhat it means for devs and enterprise technical decision-makers\nThe RISE Act’s transparency-for-liability trade-off will ripple outward from Congress straight into the daily routines of four overlapping job families that keep enterprise AI running. Start with the lead AI engineers—the people who own a model’s life cycle. Because the bill makes legal protection contingent on publicly posted model cards and full prompt specifications, these engineers gain a new, non-negotiable checklist item: confirm that every upstream vendor, or the in-house research squad down the hall, has published the required documentation before a system goes live. Any gap could leave the deployment team on the hook if a doctor, lawyer, or financial adviser later claims the model caused harm.\nNext come the senior engineers who orchestrate and automate model pipelines. They already juggle versioning, rollback plans, and integration tests; RISE adds a hard deadline. Once a model or its spec changes, updated disclosures must flow into production within thirty days. CI/CD pipelines will need a new gate that fails builds when a model card is missing, out of date, or overly redacted, forcing re-validation before code ships.\nThe data-engineering leads aren’t off the hook, either. They will inherit an expanded metadata burden: capture the provenance of training data, log evaluation metrics, and store any trade-secret redaction justifications in a way auditors can query. Stronger lineage tooling becomes more than a best practice; it turns into the evidence that a company met its duty of care when regulators—or malpractice lawyers—come knocking.\nFinally, the directors of IT security face a classic transparency paradox. Public disclosure of base prompts and known failure modes helps professionals use the system safely, but it also gives adversaries a richer target map. Security teams will have to harden endpoints against prompt-injection attacks, watch for exploits that piggyback on newly revealed failure modes, and pressure product teams to prove that redacted text hides genuine intellectual property without burying vulnerabilities.\nTaken together, these demands shift transparency from a virtue into a statutory requirement with teeth. For anyone who builds, deploys, secures, or orchestrates AI systems aimed at regulated professionals, the RISE Act would weave new checkpoints into vendor due-diligence forms, CI/CD gates, and incident-response playbooks as soon as December 2025.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-16T08:51:16.995840",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-16T08:51:58.547241",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 263 credits are required for this request."
  },
  {
    "id": "98bc781442496c8aa9c16edcba37c9c6",
    "title": "The latest state of the game jobs market | Amir Satvat",
    "url": "https://venturebeat.com/games/the-latest-state-of-the-game-jobs-market-amir-satvat/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-13T13:30:00+00:00",
    "source": "VentureBeat",
    "summary": "這篇新聞講述了遊戲產業的就業市場現況，主要是由Amir Satvat提供的最新數據。他指出，遊戲行業的職缺主要集中在擁有五到十五年經驗的專業人士身上。然而，對於新鮮人來說，進入這個行業的機會相當低，尤其是在北美以外地區更是稀少。此外，一些職位如敘事和業務開發方面的需求遠超過供應。總體而言，遊戲行業的招聘勢頭保持穩定，但招聘速度已趨於平緩。",
    "content": "The latest state of the game jobs market | Amir Satvat | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nThe latest state of the game jobs market | Amir Satvat\nDean Takahashi\n@deantak\nJune 13, 2025 6:30 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nAmir Satvat has a big community of helpers.\nImage Credit: Amir Satvat\nAmir Satvat\nprovides a lot of job resources for games. He has built a big community of game people, and they are providing him with a lot of data. And here’s the\nlatest data\nfrom Amir Satvat’s Games Community\nand what it says about games hiring today, across functions, experience levels, and regions.\nFirst,\nSatvat\n, who was honored for his work at\nThe Game Awards\n, said in a\nLinkedIn post\nthat hiring remains concentrated in the middle. This means that most roles, and role growth, is aimed at professionals with five to 15 years of experience. That’s where the bulk of open jobs and actual hires (even if the job description says otherwise) are happening.\nSadly, he noted that early career odds remain extremely low. Even if you’re willing to relocate globally, odds for new grads or early career professionals hover around 7% over 12 months. If you’re staying in North America, that drops to 2%. If you’re not in a major North America hub, that falls to 0.3%.\nThis pattern has flattened at 7%.\nHe noted that the categories of jobs are also very different when it comes to demand. Some games areas like narrative roles and business development are dramatically oversubscribed.\n“Right now, we’re tracking 52 writing and narrative games roles globally (28 in North America) and 90 total business development games roles worldwide (just 10 for 10+ years of experience),” Satvat said. “When factoring in students, switchers, or unseen applicants, I can easily believe the demand-to-supply ratio for some functions, like these, is 20-30 times, or more.”\nOverall game hiring momentum\nAmir Satvat is the game jobs champion.\nSatvat said that overall games hiring momentum is stable, but flattened. Games hiring velocity, which was improving a bit, has leveled off, while non-games roles continue rising, especially for adaptable skill sets.\nCareer switchers are intensifying competition.\n“I now have enough data to say with confidence that middle to late career switchers, without any past games experience, are still actively pursuing the industry, further intensifying competition in already crowded functions,” Satvat said.\nAnd he said layoffs may not be the biggest issue going forward.\n“We still forecast 5,000 to 9,000 games layoffs this year. Long-term, global labor cost variances and AI may matter far more, with layoffs becoming a secondary concern,” he said.\nWhat this means for you\nIf you’re a parent or mentor of a young person considering a games career, please be mindful of the data. “Why not try games?” can be a costly mindset if you’re not informed about the odds.\nIf you run a collegiate program, Satvat urges you to be transparent with prospective students. Game design, and subfields like narrative, are among the hardest areas to break into. Unfortunately, these are also the main areas from which graduating students seem to cross his desk. Offer broader skill development.\n“I continue strongly to recommend non-games roles or retraining as a strong path forward, alongside applying to games,” he said.\n✅ For those in games, we must be ready for a future that is likely to include shorter tenures, more project-based work, less remote opportunity, and higher mobility expectations.\n✅ For anyone struggling to find a role in oversubscribed functions like games narrative or business development, please know this is a 20-30x+ structural issue. It’s not about your worth.\nWe’ll keep tracking data and help as best we can.\nNew games role workbook v1.0\nAmir Satvat’s team has a new games role workbook for job seekers.\nSatvat aslo recently announced that a new resource is finally here: the\nNew Games Role Workbook v1.0\n(Resource #8).\n“This is the update I’ve waited three years to give you,” he wrote in a LinkedIn post. “Thanks to collaboration with Mayank Grover and the stellar team at Outscal, we have an improved resource of games and tech roles that will be refreshed twice a week, covering nearly 40,000 roles every three-month cycle, now delivered eight times a month.”\nWhy twice a week? Because after months of research, he found the critical window for applying to roles is within the first seven days. Anything slower was just not fast enough.\nBut there’s more. The raw data Mayank’s team pulls comes from many sources. So, just like he did for the original Games Jobs Workbook, he spent months in the background building a system to standardize all roles into 25 categories, based on community feedback and refined for usability.\nThey are:\nAccount Management\nAdministrative Support\nAnimation & Cinematics\nArt & Tech Art\nBusiness Development & Sales\nCustomer & Community Support\nData & Analytics\nDesign & UX\nEngineering & Development\nFacilities & Maintenance\nFinance & Accounting\nGeneral & Miscellaneous\nHR & Recruiting\nInternship\nIT & Security\nLegal & Compliance\nLocalization & Translation\nMarketing & Advertising\nOperations & Admin\nProduction & Product\nProject & Program Management\nStrategy & Consulting\nTechnical Support\nUser Research\nWriting & Narrative\nThis is standardized across all 38,000+ roles, both games and tech.\nThat means job seekers can now filter jobs easily across a consistent, logical set of categories. Every job has a direct link to apply, fully searchable, and structured to support your success.\n“I’ll continue maintaining the original games jobs workbook as an encyclopedic view: total jobs by company, industry-wide scope, and macro stats. I will use this data to help Mayank ensure we have all companies tracked too,” he said.\nBut this new workbook is, now, what he recommends using for active job hunting. This is because the team has finally solved (thanks to Mayank’s team) the frequency problem and (with my efforts) the categorization problem that allows equivalent functionality to the Games Jobs Workbook\nA resource with fresh roles updated twice a week, now with categorization, smart filters, games and tech roles, and full apply links at a role and location line item level?\nHe offered his deepest thanks to Mayank Grover and the Outscal team for this incredible collaboration. This wouldn’t be possible without them.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-16T08:51:17.082208",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-16T08:52:00.541084",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 225 credits are required for this request."
  },
  {
    "id": "4a16152ac41063b8e1035fdf51e44b70",
    "title": "Red team AI now to build safer, smarter models tomorrow",
    "url": "https://venturebeat.com/security/red-team-ai-now-to-build-safer-smarter-models-tomorrow/",
    "authors": "Louis Columbus",
    "published_date": "2025-06-13T13:00:00+00:00",
    "source": "VentureBeat",
    "summary": "AI模型正面臨敵人的攻擊，企業中有77%受到敵對模型攻擊，其中41%是利用惡意注入和數據污染。為了改變這種情況，我們需要將安全性整合到建立模型的過程中，從被動式防禦轉為持續的敵對測試。透過紅隊測試，將安全性融入軟體開發週期的每個階段，以應對不斷增加的風險。這種整合性方法對於降低惡意注入、數據污染和敏感資料外洩的風險至關重要。",
    "content": "Red Team AI now to build safer, smarter models tomorrow | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nRed team AI now to build safer, smarter models tomorrow\nLouis Columbus\n@LouisColumbus\nJune 13, 2025 6:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nEditor’s note: Louis will lead an editorial roundtable on this topic at VB Transform this month.\nRegister today\n.\nAI models are under siege. With\n77%\nof enterprises already hit by adversarial model attacks and\n41%\nof those attacks exploiting prompt injections and data poisoning, attackers’ tradecraft is outpacing existing cyber defenses.\nTo reverse this trend, it’s critical to rethink how security is integrated into the models being built today. DevOps teams need to shift from taking a reactive defense to continuous adversarial testing at every step.\nRed Teaming needs to be the core\nProtecting large language models (LLMs) across DevOps cycles requires red teaming as a core component of the model-creation process. Rather than treating security as a final hurdle, which is typical in web app pipelines, continuous adversarial testing needs to be integrated into every phase of the Software Development Life Cycle (SDLC).\nGartner’s Hype Cycle emphasizes the rising importance of continuous threat exposure management (CTEM), underscoring why red teaming must integrate fully into the DevSecOps lifecycle.\nSource: Gartner,\nHype Cycle for Security Operations, 2024\nAdopting a more integrative approach to DevSecOps fundamentals is becoming necessary to mitigate the growing risks of prompt injections, data poisoning and the exposure of sensitive data. Severe attacks like these are becoming more prevalent, occurring from model design through deployment, making ongoing monitoring essential.\nMicrosoft’s recent guidance on\nplanning\nred teaming for large language models (LLMs)\nand their applications provides a valuable methodology for starting\nan integrated process.\nNIST’s AI Risk Management Framework\nreinforces this, emphasizing the need for a more proactive, lifecycle-long approach to adversarial testing and risk mitigation. Microsoft’s recent red teaming of over 100 generative AI products underscores the need to integrate automated threat detection with expert oversight throughout model development.\nAs regulatory frameworks, such as the EU’s AI Act, mandate rigorous adversarial testing, integrating continuous red teaming ensures compliance and enhanced security.\nOpenAI’s\napproach to red teaming\nintegrates external red teaming from early design through deployment, confirming that consistent, preemptive security testing is crucial to the success of LLM development.\nGartner’s framework shows the structured maturity path for red teaming, from foundational to advanced exercises, essential for systematically strengthening AI model defenses.\nSource: Gartner,\nImprove Cyber Resilience by Conducting Red Team Exercises\nWhy traditional cyber defenses fail against AI\nTraditional, longstanding cybersecurity approaches fall short against AI-driven threats because they are fundamentally different from conventional attacks. As adversaries’ tradecraft surpasses traditional approaches, new techniques for red teaming are necessary. Here’s a sample of the many types of tradecraft specifically built to attack AI models throughout the DevOps cycles and once in the wild:\nData Poisoning\n: Adversaries inject corrupted data into training sets, causing models to learn incorrectly and creating persistent inaccuracies and operational errors until they are discovered. This often undermines trust in AI-driven decisions.\nModel Evasion:\nAdversaries introduce carefully crafted, subtle input changes, enabling malicious data to slip past detection systems by exploiting the inherent limitations of static rules and pattern-based security controls.\nModel Inversion\n: Systematic queries against AI models enable adversaries to extract confidential information, potentially exposing sensitive or proprietary training data and creating ongoing privacy risks.\nPrompt Injection:\nAdversaries craft inputs specifically designed to trick generative AI into bypassing safeguards, producing harmful or unauthorized results.\nDual-Use Frontier Risks:\nIn the recent paper,\nBenchmark Early and Red Team Often: A Framework for Assessing and Managing Dual-Use Hazards of AI Foundation Models\n, researchers from\nThe Center for Long-Term Cybersecurity at the University of California, Berkeley\nemphasize that advanced AI models significantly lower barriers, enabling non-experts to carry out sophisticated cyberattacks, chemical threats, or other complex exploits, fundamentally reshaping the global threat landscape and intensifying risk exposure.\nIntegrated Machine Learning Operations (MLOps) further compound these risks, threats, and vulnerabilities. The interconnected nature of LLM and broader AI development pipelines magnifies these attack surfaces, requiring improvements in red teaming.\nCybersecurity leaders are increasingly adopting continuous adversarial testing to counter these emerging AI threats. Structured red-team exercises are now essential, realistically simulating AI-focused attacks to uncover hidden vulnerabilities and close security gaps before attackers can exploit them.\nHow AI leaders stay ahead of attackers with red teaming\nAdversaries continue to accelerate their use of AI to create entirely new forms of tradecraft that defy existing, traditional cyber defenses. Their goal is to exploit as many emerging vulnerabilities as possible.\nIndustry leaders, including the major AI companies, have responded by embedding systematic and sophisticated red-teaming strategies at the core of their AI security. Rather than treating red teaming as an occasional check, they deploy continuous adversarial testing by combining expert human insights, disciplined automation, and iterative human-in-the-middle evaluations to uncover and reduce threats before attackers can exploit them proactively.\nTheir rigorous methodologies allow them to identify weaknesses and systematically harden their models against evolving real-world adversarial scenarios.\nSpecifically:\nAnthropic relies on rigorous human insight as part of its ongoing red-teaming methodology.\nBy tightly integrating human-in-the-loop evaluations with automated adversarial attacks, the company proactively identifies vulnerabilities and continually refines the reliability, accuracy and interpretability of its models.\nMeta scales AI model security through automation-first adversarial testing.\nIts Multi-round Automatic Red-Teaming (MART) systematically generates iterative adversarial prompts, rapidly uncovering hidden vulnerabilities and efficiently narrowing attack vectors across expansive AI deployments.\nMicrosoft harnesses interdisciplinary collaboration as the core of its red-teaming strength.\nUsing its Python Risk Identification Toolkit (PyRIT), Microsoft bridges cybersecurity expertise and advanced analytics with disciplined human-in-the-middle validation, accelerating vulnerability detection and providing detailed, actionable intelligence to fortify model resilience.\nOpenAI taps global security expertise to fortify AI defenses at scale.\nCombining external security specialists’ insights with automated adversarial evaluations and rigorous human validation cycles, OpenAI proactively addresses sophisticated threats, specifically targeting misinformation and prompt-injection vulnerabilities to maintain robust model performance.\nIn short, AI leaders know that staying ahead of attackers demands continuous and proactive vigilance. By embedding structured human oversight, disciplined automation, and iterative refinement into their red teaming strategies, these industry leaders set the standard and define the playbook for resilient and trustworthy AI at scale.\nGartner outlines how adversarial exposure validation (AEV) enables optimized defense, better exposure awareness, and scaled offensive testing—critical capabilities for securing AI models.\nSource: Gartner,\nMarket Guide for Adversarial Exposure Validation\nFive strategies to immediately strengthen AI security\nAs attacks on LLMs and AI models continue to evolve rapidly, DevOps and DevSecOps teams must coordinate their efforts to address the challenge of enhancing AI security. VentureBeat is finding the following five high-impact strategies security leaders can implement right away:\nIntegrate security early (Anthropic, OpenAI)\nBuild adversarial testing directly into the initial model design and throughout the entire lifecycle. Catching vulnerabilities early reduces risks, disruptions and future costs.\nDeploy adaptive, real-time monitoring (Microsoft)\nStatic defenses can’t protect AI systems from advanced threats. Leverage continuous AI-driven tools like CyberAlly to detect and respond to subtle anomalies quickly, minimizing the exploitation window.\nBalance automation with human judgment (Meta, Microsoft)\nPure automation misses nuance; manual testing alone won’t scale. Combine automated adversarial testing and vulnerability scans with expert human analysis to ensure precise, actionable insights.\nRegularly engage external red teams (OpenAI)\nInternal teams develop blind spots. Periodic external evaluations reveal hidden vulnerabilities, independently validate your defenses and drive continuous improvement.\nMaintain dynamic threat intelligence (Meta, Microsoft, OpenAI)\nAttackers constantly evolve tactics. Continuously integrate real-time threat intelligence, automated analysis and expert insights to update and strengthen your defensive posture proactively.\nTaken together, these strategies ensure DevOps workflows remain resilient and secure while staying ahead of evolving adversarial threats.\nRed teaming is no longer optional; it’s essential\nAI threats have grown too sophisticated and frequent to rely solely on traditional, reactive cybersecurity approaches. To stay ahead, organizations must continuously and proactively embed adversarial testing into every stage of model development. By balancing automation with human expertise and dynamically adapting their defenses, leading AI providers prove that robust security and innovation can coexist.\nUltimately, red teaming isn’t just about defending AI models. It’s about ensuring trust, resilience, and confidence in a future increasingly shaped by AI.\nJoin me at Transform 2025\nI’ll be hosting two cybersecurity-focused roundtables at VentureBeat’s\nTransform 2025\n, which will be held June 24–25 at Fort Mason in San Francisco. Register to join the conversation.\nMy session will include one on red teaming,\nAI Red Teaming and Adversarial Testing\n, diving into strategies for testing and strengthening AI-driven cybersecurity solutions against sophisticated adversarial threats.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-16T08:51:17.186988",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-16T08:52:02.404121",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 219 credits are required for this request."
  }
]