[
  {
    "id": "4c0083c72363294de62dc8e5247244ac",
    "title": "How much information do LLMs really memorize? Now we know, thanks to Meta, Google, Nvidia and Cornell",
    "url": "https://venturebeat.com/ai/how-much-information-do-llms-really-memorize-now-we-know-thanks-to-meta-google-nvidia-and-cornell/",
    "authors": "Carl Franzen",
    "published_date": "2025-06-05T15:35:34+00:00",
    "source": "VentureBeat AI",
    "summary": "最新研究顯示，像GPT這樣的AI模型每個參數大約只能記住3.6個位元的資訊。這項研究是由Meta、Google、Nvidia和康奈爾大學合作完成的。這個發現有助於我們了解語言模型到底有多少資訊能力，對未來AI發展有重要意義。",
    "content": "Using a clever solution, researchers find GPT-style models have a fixed memorization capacity of approximately 3.6 bits per parameter.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-05T23:50:24.651440",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-05T23:50:38.583016",
    "audio_file": "4c0083c72363294de62dc8e5247244ac.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/4c0083c72363294de62dc8e5247244ac.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-05T23:50:51.174913"
  },
  {
    "id": "b8dbd28afc5a3e5d1d6e3d5e5ca0b7cd",
    "title": "Databricks and Noma tackle CISOs’ AI nightmares around inference vulnerabilities",
    "url": "https://venturebeat.com/security/databricks-noma-tackle-cisos-ai-inference-nightmare/",
    "authors": "Louis Columbus",
    "published_date": "2025-06-05T14:13:05+00:00",
    "source": "VentureBeat AI",
    "summary": "Databricks和Noma攜手解決CISO在AI推論階段面臨的安全夢魘，專注於保護企業免受入侵、資料外洩和模型破解等威脅。他們結合了32百萬美元的資金，致力填補企業AI部署中存在的安全漏洞。這個合作夥伴關係將提供即時威脅分析、進階推論層保護和主動的AI紅隊測試，讓組織能夠安全自信地加速AI應用。",
    "content": "Databricks, Noma Tackle CISOs’ AI Inference Nightmare | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nExclusive\nDatabricks and Noma tackle CISOs’ AI nightmares around inference vulnerabilities\nLouis Columbus\n@LouisColumbus\nJune 5, 2025 7:13 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nCISOs know precisely where their AI nightmare unfolds fastest. It’s inference, the vulnerable stage where live models meet real-world data, leaving enterprises exposed to prompt injection, data leaks, and model jailbreaks.\nDatabricks Ventures\nand\nNoma Security\nare confronting these inference-stage threats head-on. Backed by a fresh $32 million Series A round led by Ballistic Ventures and Glilot Capital, with strong support from Databricks Ventures, the partnership aims to address the critical security gaps that have hindered enterprise AI deployments.\n“The number one reason enterprises hesitate to deploy AI at scale fully is security,” said Niv Braun, CEO of Noma Security, in an exclusive interview with VentureBeat. “With Databricks, we’re embedding real-time threat analytics, advanced inference-layer protections, and proactive AI red teaming directly into enterprise workflows. Our joint approach enables organizations to accelerate their AI ambitions safely and confidently finally,” Braun said.\nSecuring AI inference demands real-time analytics and runtime defense, Gartner finds\nTraditional cybersecurity prioritizes perimeter defenses, leaving AI inference vulnerabilities dangerously overlooked. Andrew Ferguson, Vice President at Databricks Ventures, highlighted this critical security gap in an exclusive interview with VentureBeat, emphasizing customer urgency regarding inference-layer security. “Our customers clearly indicated that securing AI inference in real-time is crucial, and Noma uniquely delivers that capability,” Ferguson said. “Noma directly addresses the inference security gap with continuous monitoring and precise runtime controls.”\nBraun expanded on this critical need. “We built our runtime protection specifically for increasingly complex AI interactions,” Braun explained. “Real-time threat analytics at the inference stage ensure enterprises maintain robust runtime defenses, minimizing unauthorized data exposure and adversarial model manipulation.”\nGartner’s recent analysis confirms that enterprise demand for advanced AI\nTrust, Risk, and Security Management (TRiSM)\ncapabilities is surging. Gartner predicts that through 2026, over\n80%\nof unauthorized AI incidents will result from internal misuse rather than external threats, reinforcing the urgency for integrated governance and real-time AI security.\nGartner’s AI TRiSM framework illustrates comprehensive security layers essential for managing enterprise AI risk effectively. (Source: Gartner)\nNoma’s proactive red teaming aims to ensure AI integrity from the outset\nNoma’s proactive red teaming approach is strategically central to identifying vulnerabilities long before AI models reach production, Braun told VentureBeat. By simulating sophisticated adversarial attacks during pre-production testing, Noma exposes and addresses risks early, significantly enhancing the robustness of runtime protection.\nDuring his interview with VentureBeat, Braun elaborated on the strategic value of proactive red teaming: “Red teaming is essential. We proactively uncover vulnerabilities pre-production, ensuring AI integrity from day one.”\n“Reducing time to production without compromising security requires avoiding over-engineering. We design testing methodologies that directly inform runtime protections, helping enterprises move securely and efficiently from testing to deployment”, Braun advised.\nBraun elaborated further on the complexity of modern AI interactions and the depth required in proactive red teaming methods. He stressed that this process must evolve alongside increasingly sophisticated AI models, particularly those of the generative type: “Our runtime protection was specifically built to handle increasingly complex AI interactions,” Braun explained. “Each detector we employ integrates multiple security layers, including advanced NLP models and language-modeling capabilities, ensuring we provide comprehensive security at every inference step.”\nThe red team exercises not only validate the models but also strengthen enterprise confidence in deploying advanced AI systems safely at scale, directly aligning with the expectations of leading enterprise Chief Information Security Officers (CISOs).\nHow Databricks and Noma Block Critical AI Inference Threats\nSecuring AI inference from emerging threats has become a top priority for CISOs as enterprises scale their AI model pipelines. “The number one reason enterprises hesitate to deploy AI at scale fully is security,” emphasized Braun. Ferguson echoed this urgency, noting, “Our customers have clearly indicated securing AI inference in real-time is critical, and Noma uniquely delivers on that need.”\nTogether, Databricks and Noma offer integrated, real-time protection against sophisticated threats, including prompt injection, data leaks, and model jailbreaks, while aligning closely with standards such as Databricks’ DASF 2.0 and OWASP guidelines for robust governance and compliance.\nThe table below summarizes key AI inference threats and how the Databricks-Noma partnership mitigates them:\nThreat Vector\nDescription\nPotential Impact\nNoma-Databricks Mitigation\nPrompt Injection\nMalicious inputs are overriding model instructions.\nUnauthorized data exposure and harmful content generation.\nPrompt scanning with multilayered detectors (Noma); Input validation via DASF 2.0 (Databricks).\nSensitive Data Leakage\nAccidental exposure of confidential data.\nCompliance breaches, loss of intellectual property.\nReal-time sensitive data detection and masking (Noma); Unity Catalog governance and encryption (Databricks).\nModel Jailbreaking\nBypassing embedded safety mechanisms in AI models.\nGeneration of inappropriate or malicious outputs.\nRuntime jailbreak detection and enforcement (Noma); MLflow model governance (Databricks).\nAgent Tool Exploitation\nMisuse of integrated AI agent functionalities.\nUnauthorized system access and privilege escalation.\nReal-time monitoring of agent interactions (Noma); Controlled deployment environments (Databricks).\nAgent Memory Poisoning\nInjection of false data into persistent agent memory.\nCompromised decision-making, misinformation.\nAI-SPM integrity checks and memory security (Noma); Delta Lake data versioning (Databricks).\nIndirect Prompt Injection\nEmbedding malicious instructions in trusted inputs.\nAgent hijacking, unauthorized task execution.\nReal-time input scanning for malicious patterns (Noma); Secure data ingestion pipelines (Databricks).\nHow Databricks Lakehouse architecture supports AI governance and security\nDatabricks’ Lakehouse architecture combines the structured governance capabilities of traditional data warehouses with the scalability of data lakes, centralizing analytics, machine learning, and AI workloads within a single, governed environment.\nBy embedding governance directly into the data lifecycle, Lakehouse architecture addresses compliance and security risks, particularly during the inference and runtime stages, aligning closely with industry frameworks such as OWASP and MITRE ATLAS.\nDuring our interview, Braun highlighted the platform’s alignment with the stringent regulatory demands he’s seeing in sales cycles and with existing customers. “We automatically map our security controls onto widely adopted frameworks like OWASP and MITRE ATLAS. This allows our customers to confidently comply with critical regulations such as the EU AI Act and ISO 42001. Governance isn’t just about checking boxes. It’s about embedding transparency and compliance directly into operational workflows”.\nDatabricks Lakehouse integrates governance and analytics to securely manage AI workloads. (Source: Gartner)\nHow Databricks and Noma plan to secure enterprise AI at scale\nEnterprise AI adoption is accelerating, but as deployments expand, so do security risks, especially at the model inference stage.\nThe partnership between Databricks and Noma Security addresses this directly by providing integrated governance and real-time threat detection, with a focus on securing AI workflows from development through production.\nFerguson explained the rationale behind this combined approach clearly: “Enterprise AI requires comprehensive security at every stage, especially at runtime. Our partnership with Noma integrates proactive threat analytics directly into AI operations, giving enterprises the security coverage they need to scale their AI deployments confidently”.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-05T23:50:24.985405",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-05T23:50:41.796108",
    "audio_file": "b8dbd28afc5a3e5d1d6e3d5e5ca0b7cd.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/b8dbd28afc5a3e5d1d6e3d5e5ca0b7cd.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-05T23:50:52.233758"
  },
  {
    "id": "341ad5be765bd43158904be9e92d885a",
    "title": "Stop guessing why your LLMs break: Anthropic’s new tool shows you exactly what goes wrong",
    "url": "https://venturebeat.com/ai/stop-guessing-why-your-llms-break-anthropics-new-tool-shows-you-exactly-what-goes-wrong/",
    "authors": "Ben Dickson",
    "published_date": "2025-06-04T22:39:09+00:00",
    "source": "VentureBeat AI",
    "summary": "Anthropic推出新工具，讓你不再猜測大型語言模型(LLMs)出錯的原因，直接顯示問題所在。這個工具可以幫助開發者和研究人員了解和控制模型的內部運作，解決企業面對LLMs不可預測性的問題。透過追蹤模型內部活動，生成屬性圖，解析模型處理資訊並產生輸出的過程，幫助精細調整LLMs。這項技術讓人更了解AI模型的內在邏輯，提升對模型運作的掌握和控制能力。",
    "content": "Stop guessing why your LLMs break: Anthropic's new tool shows you exactly what goes wrong | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nStop guessing why your LLMs break: Anthropic’s new tool shows you exactly what goes wrong\nBen Dickson\n@BenDee983\nJune 4, 2025 3:39 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage credit: VentureBeat with Ideogram\nJoin our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage.\nLearn More\nLarge language models (LLMs) are transforming how enterprises operate, but their “black box” nature often leaves enterprises grappling with unpredictability. Addressing this critical challenge,\nAnthropic\nrecently open-sourced its\ncircuit tracing tool\n, allowing developers and researchers to directly understand and control models’ inner workings.\nThis tool allows investigators to investigate unexplained errors and unexpected behaviors in open-weight models. It can also help with granular fine-tuning of LLMs for specific internal functions.\nUnderstanding the AI’s inner logic\nThis circuit tracing tool works based on “\nmechanistic interpretability\n,” a burgeoning field dedicated to understanding how AI models function based on their internal activations rather than merely observing their inputs and outputs.\nWhile Anthropic’s\ninitial research on circuit tracing\napplied this methodology to their own\nClaude 3.5 Haiku model\n, the open-sourced tool extends this capability to open-weights models. Anthropic’s team has already used the tool to trace circuits in models like Gemma-2-2b and Llama-3.2-1b and has released a\nColab notebook\nthat helps use the library on open models.\nThe core of the tool lies in generating attribution graphs, causal maps that trace the interactions between features as the model processes information and generates an output. (Features are internal activation patterns of the model that can be roughly mapped to understandable concepts.) It is like obtaining a detailed wiring diagram of an AI’s internal thought process. More importantly, the tool enables “intervention experiments,” allowing researchers to directly modify these internal features and observe how changes in the AI’s internal states impact its external responses, making it possible to debug models.\nThe tool integrates with\nNeuronpedia\n, an open platform for understanding and experimentation with neural networks.\nCircuit tracing on Neuronpedia (source: Anthropic blog)\nPracticalities and future impact for enterprise AI\nWhile Anthropic’s circuit tracing tool is a great step toward explainable and controllable AI, it has practical challenges, including high memory costs associated with running the tool and the inherent complexity of interpreting the detailed attribution graphs.\nHowever, these challenges are typical of cutting-edge research. Mechanistic interpretability is a big area of research, and most big AI labs are developing models to investigate the inner workings of large language models. By open-sourcing the circuit tracing tool, Anthropic will enable the community to develop interpretability tools that are more scalable, automated, and accessible to a wider array of users, opening the way for practical applications of all the effort that is going into understanding LLMs.\nAs the tooling matures, the ability to understand why an LLM makes a certain decision can translate into practical benefits for enterprises.\nCircuit tracing explains how LLMs perform sophisticated multi-step reasoning. For example, in their study, the researchers were able to trace how a model inferred “Texas” from “Dallas” before arriving at “Austin” as the capital. It also revealed advanced planning mechanisms, like a model pre-selecting rhyming words in a poem to guide line composition. Enterprises can use these insights to analyze how their models tackle complex tasks like data analysis or legal reasoning. Pinpointing internal planning or reasoning steps allows for targeted optimization, improving efficiency and accuracy in complex business processes.\nSource: Anthropic\nFurthermore, circuit tracing offers better clarity into numerical operations. For example, in their study, the researchers uncovered how models handle arithmetic, like 36+59=95, not through simple algorithms but via parallel pathways and “lookup table” features for digits. For example, enterprises can use such insights to audit internal computations leading to numerical results, identify the origin of errors and implement targeted fixes to ensure data integrity and calculation accuracy within their open-source LLMs.\nFor global deployments, the tool provides insights into multilingual consistency. Anthropic’s previous research shows that models employ both language-specific and abstract, language-independent “universal mental language” circuits, with larger models demonstrating greater generalization. This can potentially help debug localization challenges when deploying models across different languages.\nFinally, the tool can help combat hallucinations and improve factual grounding. The research revealed that models have “default refusal circuits” for unknown queries, which are suppressed by “known answer” features. Hallucinations can occur when this inhibitory circuit “misfires.”\nSource: Anthropic\nBeyond debugging existing issues, this mechanistic understanding unlocks new avenues for\nfine-tuning LLMs\n. Instead of merely adjusting output behavior through trial and error, enterprises can identify and target the specific internal mechanisms driving desired or undesired traits. For instance, understanding how a model’s “Assistant persona” inadvertently incorporates hidden reward model biases, as shown in Anthropic’s research, allows developers to precisely re-tune the internal circuits responsible for alignment, leading to more robust and ethically consistent AI deployments.\nAs LLMs increasingly integrate into critical enterprise functions, their transparency, interpretability and control become increasingly critical. This new generation of tools can help bridge the gap between AI’s powerful capabilities and human understanding, building foundational trust and ensuring that enterprises can deploy AI systems that are reliable, auditable, and aligned with their strategic objectives.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-05T23:50:25.329381",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-05T23:50:45.613719",
    "audio_file": "341ad5be765bd43158904be9e92d885a.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/341ad5be765bd43158904be9e92d885a.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-05T23:50:53.673918"
  }
]