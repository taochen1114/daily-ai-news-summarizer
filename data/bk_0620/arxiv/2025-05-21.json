[
  {
    "id": "2505.11584",
    "title": "LLM Agents Are Hypersensitive to Nudges",
    "url": "https://arxiv.org/abs/2505.11584",
    "authors": "Manuel Cherep, Pattie Maes, Nikhil Singh",
    "categories": [
      "cs.AI"
    ],
    "published_date": "Tue, 20 May 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2505.11584v1 Announce Type: new  Abstract: LLMs are being set loose in complex, real-world environments involving sequential decision-making and tool use. Often, this involves making choices on behalf of human users. However, not much is known about the distribution of such choices, and how susceptible they are to different choice architectures. We perform a case study with a few such LLM models on a multi-attribute tabular decision-making problem, under canonical nudges such as the default option, suggestions, and information highlighting, as well as additional prompting strategies. We show that, despite superficial similarities to human choice distributions, such models differ in subtle but important ways. First, they show much higher susceptibility to the nudges. Second, they diverge in points earned, being affected by factors like the idiosyncrasy of available prizes. Third, they diverge in information acquisition strategies: e.g. incurring substantial cost to reveal too much information, or selecting without revealing any. Moreover, we show that simple prompt strategies like zero-shot chain of thought (CoT) can shift the choice distribution, and few-shot prompting with human data can induce greater alignment. Yet, none of these methods resolve the sensitivity of these models to nudges. Finally, we show how optimal nudges optimized with a human resource-rational model can similarly increase LLM performance for some models. All these findings suggest that behavioral tests are needed before deploying models as agents or assistants acting on behalf of users in complex environments.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-05-21T10:24:35.577833",
    "summary": "這篇研究發現，當人工智慧模型在複雜環境中做決策時，對於一些微妙的影響非常敏感，像是預設選項、建議或資訊突顯等。這些模型在做決策時，容易受到這些影響而做出不同於人類的選擇。研究者也發現透過一些提示策略，可以改變模型的選擇分佈，但仍無法完全解決模型對這些影響的敏感度。因此，研究建議在將這些模型應用在複雜環境中之前，需要進行行為測試。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-21T10:24:40.473925",
    "audio_file": "2505.11584.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2505.11584.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-21T10:25:13.242266"
  },
  {
    "id": "2505.11610",
    "title": "Foundation Models for AI-Enabled Biological Design",
    "url": "https://arxiv.org/abs/2505.11610",
    "authors": "Asher Moldwin, Amarda Shehu",
    "categories": [
      "cs.AI",
      "cs.LG",
      "q-bio.BM",
      "q-bio.GN"
    ],
    "published_date": "Tue, 20 May 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2505.11610v1 Announce Type: new  Abstract: This paper surveys foundation models for AI-enabled biological design, focusing on recent developments in applying large-scale, self-supervised models to tasks such as protein engineering, small molecule design, and genomic sequence design. Though this domain is evolving rapidly, this survey presents and discusses a taxonomy of current models and methods. The focus is on challenges and solutions in adapting these models for biological applications, including biological sequence modeling architectures, controllability in generation, and multi-modal integration. The survey concludes with a discussion of open problems and future directions, offering concrete next-steps to improve the quality of biological sequence generation.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-05-21T10:24:35.577861",
    "summary": "這篇論文探討了在人工智慧技術下，如何應用大規模、自我監督的模型來進行生物設計，主要聚焦於蛋白質工程、小分子設計和基因組序列設計等任務。作者們提出了一個對目前模型和方法進行分類的調查，並討論了在生物應用中適應這些模型所面臨的挑戰和解決方案。最後，他們提出了未來研究方向和解決問題的具體步驟，以提高生物序列生成的質量。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-21T10:24:42.531758",
    "audio_file": "2505.11610.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2505.11610.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-21T10:25:14.344245"
  },
  {
    "id": "2505.11611",
    "title": "Probing the Vulnerability of Large Language Models to Polysemantic Interventions",
    "url": "https://arxiv.org/abs/2505.11611",
    "authors": "Bofan Gong, Shiyang Lai, Dawn Song",
    "categories": [
      "cs.AI",
      "cs.CL",
      "cs.CR"
    ],
    "published_date": "Tue, 20 May 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2505.11611v1 Announce Type: new  Abstract: Polysemanticity -- where individual neurons encode multiple unrelated features -- is a well-known characteristic of large neural networks and remains a central challenge in the interpretability of language models. At the same time, its implications for model safety are also poorly understood. Leveraging recent advances in sparse autoencoders, we investigate the polysemantic structure of two small models (Pythia-70M and GPT-2-Small) and evaluate their vulnerability to targeted, covert interventions at the prompt, feature, token, and neuron levels. Our analysis reveals a consistent polysemantic topology shared across both models. Strikingly, we demonstrate that this structure can be exploited to mount effective interventions on two larger, black-box instruction-tuned models (LLaMA3.1-8B-Instruct and Gemma-2-9B-Instruct). These findings suggest not only the generalizability of the interventions but also point to a stable and transferable polysemantic structure that could potentially persist across architectures and training regimes.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-05-21T10:24:35.577885",
    "summary": "這篇研究探討大型語言模型對多義詞干擾的脆弱性，發現即使是小型模型也存在多義性結構，可被利用進行干擾。研究者利用稀疏自編碼器分析模型結構，發現這種多義結構在不同模型中普遍存在，並可跨架構和訓練方式轉移。這項研究揭示了語言模型的安全性挑戰，對於模型的解釋性和安全性有重要啟示。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-21T10:24:44.525803",
    "audio_file": "2505.11611.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2505.11611.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-21T10:25:15.397972"
  },
  {
    "id": "2505.11612",
    "title": "Heart2Mind: Human-Centered Contestable Psychiatric Disorder Diagnosis System using Wearable ECG Monitors",
    "url": "https://arxiv.org/abs/2505.11612",
    "authors": "Hung Nguyen, Alireza Rahimi, Veronica Whitford, H\\'el\\`ene Fournier, Irina Kondratova, Ren\\'e Richard, Hung Cao",
    "categories": [
      "cs.AI",
      "cs.HC"
    ],
    "published_date": "Tue, 20 May 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2505.11612v1 Announce Type: new  Abstract: Psychiatric disorders affect millions globally, yet their diagnosis faces significant challenges in clinical practice due to subjective assessments and accessibility concerns, leading to potential delays in treatment. To help address this issue, we present Heart2Mind, a human-centered contestable psychiatric disorder diagnosis system using wearable electrocardiogram (ECG) monitors. Our approach leverages cardiac biomarkers, particularly heart rate variability (HRV) and R-R intervals (RRI) time series, as objective indicators of autonomic dysfunction in psychiatric conditions. The system comprises three key components: (1) a Cardiac Monitoring Interface (CMI) for real-time data acquisition from Polar H9/H10 devices; (2) a Multi-Scale Temporal-Frequency Transformer (MSTFT) that processes RRI time series through integrated time-frequency domain analysis; (3) a Contestable Diagnosis Interface (CDI) combining Self-Adversarial Explanations (SAEs) with contestable Large Language Models (LLMs). Our MSTFT achieves 91.7% accuracy on the HRV-ACC dataset using leave-one-out cross-validation, outperforming state-of-the-art methods. SAEs successfully detect inconsistencies in model predictions by comparing attention-based and gradient-based explanations, while LLMs enable clinicians to validate correct predictions and contest erroneous ones. This work demonstrates the feasibility of combining wearable technology with Explainable Artificial Intelligence (XAI) and contestable LLMs to create a transparent, contestable system for psychiatric diagnosis that maintains clinical oversight while leveraging advanced AI capabilities. Our implementation is publicly available at: https://github.com/Analytics-Everywhere-Lab/heart2mind.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-05-21T10:24:35.577905",
    "summary": "這篇論文提出了一個名為Heart2Mind的系統，利用可穿戴心電圖監測器幫助診斷精神疾病。透過心率變異性和R-R間隔時間序列作為客觀指標，幫助檢測精神狀況的自主功能異常。系統包含三個主要組件：心臟監測界面、多尺度時間頻率轉換器和可爭議診斷界面，結合了可解釋人工智慧和可爭議的大型語言模型，提供透明且可爭議的診斷系統。這項研究展示了結合可穿戴科技和XAI技術，創造出一個具有高度透明度和爭議性的精神疾病診斷系統的可行性。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-21T10:24:47.571493",
    "audio_file": "2505.11612.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2505.11612.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-21T10:25:16.715677"
  },
  {
    "id": "2505.11614",
    "title": "Using Reinforcement Learning to Train Large Language Models to Explain Human Decisions",
    "url": "https://arxiv.org/abs/2505.11614",
    "authors": "Jian-Qiao Zhu, Hanbo Xie, Dilip Arumugam, Robert C. Wilson, Thomas L. Griffiths",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published_date": "Tue, 20 May 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2505.11614v1 Announce Type: new  Abstract: A central goal of cognitive modeling is to develop models that not only predict human behavior but also provide insight into the underlying cognitive mechanisms. While neural network models trained on large-scale behavioral data often achieve strong predictive performance, they typically fall short in offering interpretable explanations of the cognitive processes they capture. In this work, we explore the potential of pretrained large language models (LLMs) to serve as dual-purpose cognitive models--capable of both accurate prediction and interpretable explanation in natural language. Specifically, we employ reinforcement learning with outcome-based rewards to guide LLMs toward generating explicit reasoning traces for explaining human risky choices. Our findings demonstrate that this approach produces high-quality explanations alongside strong quantitative predictions of human decisions.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-05-21T10:24:35.577928",
    "summary": "這篇研究利用強化學習來訓練大型語言模型，幫助解釋人類的決策過程。他們發現透過預先訓練的大型語言模型，可以同時提供準確的預測和易於理解的說明，特別是在解釋人類冒險選擇時。這項研究的價值在於開創了一個新的途徑，讓機器能夠以自然語言生成清晰的推理過程，同時也能準確預測人類的決策。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-21T10:24:50.536301",
    "audio_file": "2505.11614.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2505.11614.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-21T10:25:17.703060"
  },
  {
    "id": "2505.11618",
    "title": "Benchmarking Spatiotemporal Reasoning in LLMs and Reasoning Models: Capabilities and Challenges",
    "url": "https://arxiv.org/abs/2505.11618",
    "authors": "Pengrui Quan, Brian Wang, Kang Yang, Liying Han, Mani Srivastava",
    "categories": [
      "cs.AI",
      "cs.LG",
      "eess.SP"
    ],
    "published_date": "Tue, 20 May 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2505.11618v1 Announce Type: new  Abstract: Spatiotemporal reasoning plays a key role in Cyber-Physical Systems (CPS). Despite advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs), their capacity to reason about complex spatiotemporal signals remains underexplored. This paper proposes a hierarchical SpatioTemporal reAsoning benchmaRK, STARK, to systematically evaluate LLMs across three levels of reasoning complexity: state estimation (e.g., predicting field variables, localizing and tracking events in space and time), spatiotemporal reasoning over states (e.g., inferring spatial-temporal relationships), and world-knowledge-aware reasoning that integrates contextual and domain knowledge (e.g., intent prediction, landmark-aware navigation). We curate 26 distinct spatiotemporal tasks with diverse sensor modalities, comprising 14,552 challenges where models answer directly or by Python Code Interpreter. Evaluating 3 LRMs and 8 LLMs, we find LLMs achieve limited success in tasks requiring geometric reasoning (e.g., multilateration or triangulation), particularly as complexity increases. Surprisingly, LRMs show robust performance across tasks with various levels of difficulty, often competing or surpassing traditional first-principle-based methods. Our results show that in reasoning tasks requiring world knowledge, the performance gap between LLMs and LRMs narrows, with some LLMs even surpassing LRMs. However, the LRM o3 model continues to achieve leading performance across all evaluated tasks, a result attributed primarily to the larger size of the reasoning models. STARK motivates future innovations in model architectures and reasoning paradigms for intelligent CPS by providing a structured framework to identify limitations in the spatiotemporal reasoning of LLMs and LRMs.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-05-21T10:24:35.577950",
    "summary": "這篇論文主要探討在大型語言模型（LLMs）和大型推理模型（LRMs）中，對於複雜時空訊號的推理能力進行評估。研究提出了一個名為STARK的層次化時空推理基準，通過26個不同的時空任務來評估模型的表現。結果顯示，LLMs在需要幾何推理的任務上表現有限，而LRMs則在各種難度的任務中表現穩健。研究發現，在需要世界知識的推理任務中，LLMs和LRMs之間的性能差距逐漸縮小。這項研究有助於未來智能CPS模型架構和推理範式的創新。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-21T10:24:53.630677",
    "audio_file": "2505.11618.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2505.11618.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-21T10:25:19.512309"
  },
  {
    "id": "2505.11646",
    "title": "FLOW-BENCH: Towards Conversational Generation of Enterprise Workflows",
    "url": "https://arxiv.org/abs/2505.11646",
    "authors": "Evelyn Duesterwald, Siyu Huo, Vatche Isahagian, K. R. Jayaram, Ritesh Kumar, Vinod Muthusamy, Punleuk Oum, Debashish Saha, Gegi Thomas, Praveen Venkateswaran",
    "categories": [
      "cs.AI",
      "cs.SE"
    ],
    "published_date": "Tue, 20 May 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2505.11646v1 Announce Type: new  Abstract: Business process automation (BPA) that leverages Large Language Models (LLMs) to convert natural language (NL) instructions into structured business process artifacts is becoming a hot research topic. This paper makes two technical contributions -- (i) FLOW-BENCH, a high quality dataset of paired natural language instructions and structured business process definitions to evaluate NL-based BPA tools, and support bourgeoning research in this area, and (ii) FLOW-GEN, our approach to utilize LLMs to translate natural language into an intermediate representation with Python syntax that facilitates final conversion into widely adopted business process definition languages, such as BPMN and DMN. We bootstrap FLOW-BENCH by demonstrating how it can be used to evaluate the components of FLOW-GEN across eight LLMs of varying sizes. We hope that FLOW-GEN and FLOW-BENCH catalyze further research in BPA making it more accessible to novice and expert users.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-05-21T10:24:35.577969",
    "summary": "這篇論文提出了一個名為「FLOW-BENCH」的資料集，幫助企業將自然語言指令轉換成結構化的業務流程文件，這是一個熱門的研究領域。他們開發了一個名為「FLOW-GEN」的方法，利用大型語言模型將自然語言轉換成Python語法，再進一步轉換成常用的業務流程定義語言。這個研究希望透過「FLOW-GEN」和「FLOW-BENCH」來推動業務流程自動化領域的研究，讓初學者和專家都能更容易地使用相關工具。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-21T10:24:56.168478",
    "audio_file": "2505.11646.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2505.11646.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-21T10:25:20.574496"
  },
  {
    "id": "2505.11661",
    "title": "Learning from Less: Guiding Deep Reinforcement Learning with Differentiable Symbolic Planning",
    "url": "https://arxiv.org/abs/2505.11661",
    "authors": "Zihan Ye, Oleg Arenz, Kristian Kersting",
    "categories": [
      "cs.AI"
    ],
    "published_date": "Tue, 20 May 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2505.11661v1 Announce Type: new  Abstract: When tackling complex problems, humans naturally break them down into smaller, manageable subtasks and adjust their initial plans based on observations. For instance, if you want to make coffee at a friend's place, you might initially plan to grab coffee beans, go to the coffee machine, and pour them into the machine. Upon noticing that the machine is full, you would skip the initial steps and proceed directly to brewing. In stark contrast, state of the art reinforcement learners, such as Proximal Policy Optimization (PPO), lack such prior knowledge and therefore require significantly more training steps to exhibit comparable adaptive behavior. Thus, a central research question arises: \\textit{How can we enable reinforcement learning (RL) agents to have similar ``human priors'', allowing the agent to learn with fewer training interactions?} To address this challenge, we propose differentiable symbolic planner (Dylan), a novel framework that integrates symbolic planning into Reinforcement Learning. Dylan serves as a reward model that dynamically shapes rewards by leveraging human priors, guiding agents through intermediate subtasks, thus enabling more efficient exploration. Beyond reward shaping, Dylan can work as a high level planner that composes primitive policies to generate new behaviors while avoiding common symbolic planner pitfalls such as infinite execution loops. Our experimental evaluations demonstrate that Dylan significantly improves RL agents' performance and facilitates generalization to unseen tasks.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-05-21T10:24:35.577990",
    "summary": "這篇論文提出了一個名為Dylan的新框架，將符號式規劃引入強化學習，讓AI能夠像人類一樣在解決問題時分解成小任務，並根據觀察調整計劃。透過Dylan，AI能夠更有效地探索環境，提高學習效率，並在未知任務上有更好的表現。這項研究的核心創新在於結合符號式規劃和強化學習，讓AI具備類似「人類先驗知識」，幫助AI在少量互動中學習。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-21T10:24:58.636640",
    "audio_file": "2505.11661.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2505.11661.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-21T10:25:21.604111"
  },
  {
    "id": "2505.11698",
    "title": "Conditional Deep Generative Models for Belief State Planning",
    "url": "https://arxiv.org/abs/2505.11698",
    "authors": "Antoine Bigeard, Anthony Corso, Mykel Kochenderfer",
    "categories": [
      "cs.AI"
    ],
    "published_date": "Tue, 20 May 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2505.11698v1 Announce Type: new  Abstract: Partially observable Markov decision processes (POMDPs) are used to model a wide range of applications, including robotics, autonomous vehicles, and subsurface problems. However, accurately representing the belief is difficult for POMDPs with high-dimensional states. In this paper, we propose a novel approach that uses conditional deep generative models (cDGMs) to represent the belief. Unlike traditional belief representations, cDGMs are well-suited for high-dimensional states and large numbers of observations, and they can generate an arbitrary number of samples from the posterior belief. We train the cDGMs on data produced by random rollout trajectories and show their effectiveness in solving a mineral exploration POMDP with a large and continuous state space. The cDGMs outperform particle filter baselines in both task-agnostic measures of belief accuracy as well as in planning performance.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-05-21T10:24:35.578010",
    "summary": "這篇論文提出了一種新方法，使用條件深度生成模型來有效表示部分可觀察馬可夫決策過程中的信念狀態。這種方法特別適合應對高維度狀態和大量觀察的情況，能夠產生後驗信念的任意數量樣本。經過訓練後，這些模型在解決具有大型連續狀態空間的礦產勘探問題時表現優異，不僅在信念準確性方面優於基準粒子濾波器，而且在規劃性能上也表現出色。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-21T10:25:01.167406",
    "audio_file": "2505.11698.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2505.11698.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-21T10:25:22.565153"
  },
  {
    "id": "2505.11701",
    "title": "DMN-Guided Prompting: A Low-Code Framework for Controlling LLM Behavior",
    "url": "https://arxiv.org/abs/2505.11701",
    "authors": "Shaghayegh Abedi, Amin Jalali",
    "categories": [
      "cs.AI"
    ],
    "published_date": "Tue, 20 May 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2505.11701v1 Announce Type: new  Abstract: Large Language Models (LLMs) have shown considerable potential in automating decision logic within knowledge-intensive processes. However, their effectiveness largely depends on the strategy and quality of prompting. Since decision logic is typically embedded in prompts, it becomes challenging for end users to modify or refine it. Decision Model and Notation (DMN) offers a standardized graphical approach for defining decision logic in a structured, user-friendly manner. This paper introduces a DMN-guided prompting framework that breaks down complex decision logic into smaller, manageable components, guiding LLMs through structured decision pathways. We implemented the framework in a graduate-level course where students submitted assignments. The assignments and DMN models representing feedback instructions served as inputs to our framework. The instructor evaluated the generated feedback and labeled it for performance assessment. Our approach demonstrated promising results, outperforming chain-of-thought (CoT) prompting. Students also responded positively to the generated feedback, reporting high levels of perceived usefulness in a survey based on the Technology Acceptance Model.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-05-21T10:24:35.578027",
    "summary": "這篇論文提出了一個新的方法，利用決策模型來引導大型語言模型（LLMs）進行決策，讓使用者更容易控制和修改模型的行為。研究者設計了一個以決策模型為基礎的框架，將複雜的決策邏輯分解成易管理的部分，引導LLMs按照結構化的決策路徑進行。在研究中，學生提交作業，並使用框架生成反饋，結果表現優異且獲得正面回饋。這個方法比傳統的提示方法表現更好，也受到學生高度肯定。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-05-21T10:25:05.669965",
    "audio_file": "2505.11701.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2505.11701.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-05-21T10:25:23.677324"
  }
]