[
  {
    "id": "2504.17929",
    "title": "ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing",
    "url": "https://arxiv.org/abs/2504.17929",
    "authors": "Ayesha Siddique, Khurram Khalil, Khaza Anuarul Hoque",
    "categories": [
      "cs.AI",
      "cs.AR"
    ],
    "published_date": "Mon, 28 Apr 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2504.17929v1 Announce Type: new  Abstract: Explainable artificial intelligence (XAI) enhances AI system transparency by framing interpretability as an optimization problem. However, this approach often necessitates numerous iterations of computationally intensive operations, limiting its applicability in real-time scenarios. While recent research has focused on XAI hardware acceleration on FPGAs and TPU, these methods do not fully address energy efficiency in real-time settings. To address this limitation, we propose XAIedge, a novel framework that leverages approximate computing techniques into XAI algorithms, including integrated gradients, model distillation, and Shapley analysis. XAIedge translates these algorithms into approximate matrix computations and exploits the synergy between convolution, Fourier transform, and approximate computing paradigms. This approach enables efficient hardware acceleration on TPU-based edge devices, facilitating faster real-time outcome interpretations. Our comprehensive evaluation demonstrates that XAIedge achieves a $2\\times$ improvement in energy efficiency compared to existing accurate XAI hardware acceleration techniques while maintaining comparable accuracy. These results highlight the potential of XAIedge to significantly advance the deployment of explainable AI in energy-constrained real-time applications.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:09.494804",
    "summary": "這篇論文提出了一個名為「XAIedge」的新框架，結合了近似運算技術和解釋性人工智慧（XAI）演算法，能在邊緣裝置上進行高效的硬體加速，讓AI系統更透明且能夠即時解釋結果。研究顯示，XAIedge相較於現有的XAI硬體加速技術，能提升2倍的能源效率，同時保持相當的準確性，有助於在能源受限的即時應用中推動可解釋AI的應用。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:20.393794",
    "audio_file": "2504.17929.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2504.17929.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:44:22.249054"
  },
  {
    "id": "2504.17967",
    "title": "LLM Agent Swarm for Hypothesis-Driven Drug Discovery",
    "url": "https://arxiv.org/abs/2504.17967",
    "authors": "Kevin Song, Andrew Trotter, Jake Y. Chen",
    "categories": [
      "cs.AI"
    ],
    "published_date": "Mon, 28 Apr 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2504.17967v1 Announce Type: new  Abstract: Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM \"agents\" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:09.494815",
    "summary": "這篇論文介紹了一個名為PharmaSwarm的新方法，利用多個專門的語言模型代理人來加速藥物發現過程。這些代理人能夠提出、驗證和改進有關新藥物靶點和前導化合物的假設。透過結合不同功能，如基因組和表現分析、生物醫學知識圖譜、通路豐富度和網路模擬等，並透過共享記憶層不斷優化系統，PharmaSwarm可以加速轉譯研究，提供高信心的假設，比傳統方法更有效率。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:23.065724",
    "audio_file": "2504.17967.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2504.17967.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:44:32.529364"
  },
  {
    "id": "2504.18007",
    "title": "Differential Privacy-Driven Framework for Enhancing Heart Disease Prediction",
    "url": "https://arxiv.org/abs/2504.18007",
    "authors": "Yazan Otoum, Amiya Nayak",
    "categories": [
      "cs.AI",
      "cs.CR",
      "cs.LG"
    ],
    "published_date": "Mon, 28 Apr 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2504.18007v1 Announce Type: new  Abstract: With the rapid digitalization of healthcare systems, there has been a substantial increase in the generation and sharing of private health data. Safeguarding patient information is essential for maintaining consumer trust and ensuring compliance with legal data protection regulations. Machine learning is critical in healthcare, supporting personalized treatment, early disease detection, predictive analytics, image interpretation, drug discovery, efficient operations, and patient monitoring. It enhances decision-making, accelerates research, reduces errors, and improves patient outcomes. In this paper, we utilize machine learning methodologies, including differential privacy and federated learning, to develop privacy-preserving models that enable healthcare stakeholders to extract insights without compromising individual privacy. Differential privacy introduces noise to data to guarantee statistical privacy, while federated learning enables collaborative model training across decentralized datasets. We explore applying these technologies to Heart Disease Data, demonstrating how they preserve privacy while delivering valuable insights and comprehensive analysis. Our results show that using a federated learning model with differential privacy achieved a test accuracy of 85%, ensuring patient data remained secure and private throughout the process.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:09.494825",
    "summary": "這篇論文主要探討如何在保護病患隱私的同時，利用機器學習技術來預測心臟疾病。他們運用差分隱私和聯邦學習等方法，讓醫療相關人員可以從數據中獲取洞察，同時確保個人隱私不被侵犯。研究結果顯示，他們開發的模型在測試中達到85%的準確率，確保病患數據在整個過程中保持安全和私密。這項研究有助於提升醫療決策的準確性，同時保護病患的隱私資料。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:27.002914",
    "audio_file": "2504.18007.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2504.18007.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:44:43.455052"
  },
  {
    "id": "2504.18039",
    "title": "MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind",
    "url": "https://arxiv.org/abs/2504.18039",
    "authors": "Zheng Zhang, Nuoqian Xiao, Qi Chai, Deheng Ye, Hao Wang",
    "categories": [
      "cs.AI"
    ],
    "published_date": "Mon, 28 Apr 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2504.18039v1 Announce Type: new  Abstract: Large Language Model (LLM) agents have demonstrated impressive capabilities in social deduction games (SDGs) like Werewolf, where strategic reasoning and social deception are essential. However, current approaches remain limited to textual information, ignoring crucial multimodal cues such as facial expressions and tone of voice that humans naturally use to communicate. Moreover, existing SDG agents primarily focus on inferring other players' identities without modeling how others perceive themselves or fellow players. To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a testbed and present MultiMind, the first framework integrating multimodal information into SDG agents. MultiMind processes facial expressions and vocal tones alongside verbal content, while employing a Theory of Mind (ToM) model to represent each player's suspicion levels toward others. By combining this ToM model with Monte Carlo Tree Search (MCTS), our agent identifies communication strategies that minimize suspicion directed at itself. Through comprehensive evaluation in both agent-versus-agent simulations and studies with human players, we demonstrate MultiMind's superior performance in gameplay. Our work presents a significant advancement toward LLM agents capable of human-like social reasoning across multimodal domains.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:09.494833",
    "summary": "這篇論文提出了一個名為MultiMind的新框架，讓AI在狼人殺等社交推理遊戲中能夠更好地理解人類的溝通方式。MultiMind不僅考慮文字訊息，還分析臉部表情、語氣等多模態訊號，同時透過心智理論模型來推測玩家之間的懷疑程度。透過這種方式，AI能夠制定更有效的溝通策略，提升在遊戲中的表現。這項研究對於讓AI在多模態領域展現出人類般的社交推理能力有著重大的意義。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:29.857356",
    "audio_file": "2504.18039.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2504.18039.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:44:54.822155"
  },
  {
    "id": "2504.18096",
    "title": "Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation",
    "url": "https://arxiv.org/abs/2504.18096",
    "authors": "Xiang Li, Haixu Ma, Guanyong Wu, Shi Mu, Chen Li, Shunpan Liang",
    "categories": [
      "cs.AI",
      "cs.LG"
    ],
    "published_date": "Mon, 28 Apr 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2504.18096v1 Announce Type: new  Abstract: Medication recommendation is crucial in healthcare, offering effective treatments based on patient's electronic health records (EHR). Previous studies show that integrating more medication-related knowledge improves medication representation accuracy. However, not all medications encompass multiple types of knowledge data simultaneously. For instance, some medications provide only textual descriptions without structured data. This imbalance in data availability limits the performance of existing models, a challenge we term the \"bucket effect\" in medication recommendation. Our data analysis uncovers the severity of the \"bucket effect\" in medication recommendation. To fill this gap, we introduce a cross-modal medication encoder capable of seamlessly aligning data from different modalities and propose a medication recommendation framework to integrate Multiple types of Knowledge, named MKMed. Specifically, we first pre-train a cross-modal encoder with contrastive learning on five knowledge modalities, aligning them into a unified space. Then, we combine the multi-knowledge medication representations with patient records for recommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets demonstrate that MKMed mitigates the \"bucket effect\" in data, and significantly outperforms state-of-the-art baselines in recommendation accuracy and safety.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:09.494842",
    "summary": "這篇論文提出了一個名為MKMed的新方法，用來改善藥物推薦中的資料不平衡問題，稱為「桶子效應」。他們使用跨模態藥物編碼器將不同類型的知識資料整合在一起，提高了推薦的準確性和安全性。經過實驗證明，MKMed在MIMIC-III和MIMIC-IV數據集上表現優異，超越了現有模型。這個方法的創新之處在於整合多種知識來提升藥物推薦的效果，對於醫療保健領域具有重要價值。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:32.556760",
    "audio_file": "2504.18096.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2504.18096.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:45:05.852800"
  },
  {
    "id": "2504.18443",
    "title": "Pseudo-Boolean Proof Logging for Optimal Classical Planning",
    "url": "https://arxiv.org/abs/2504.18443",
    "authors": "Simon Dold, Malte Helmert, Jakob Nordstr\\\"om, Gabriele R\\\"oger, Tanja Schindler",
    "categories": [
      "cs.AI"
    ],
    "published_date": "Mon, 28 Apr 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2504.18443v1 Announce Type: new  Abstract: We introduce lower-bound certificates for classical planning tasks, which can be used to prove the unsolvability of a task or the optimality of a plan in a way that can be verified by an independent third party. We describe a general framework for generating lower-bound certificates based on pseudo-Boolean constraints, which is agnostic to the planning algorithm used.   As a case study, we show how to modify the $A^{*}$ algorithm to produce proofs of optimality with modest overhead, using pattern database heuristics and $h^\\textit{max}$ as concrete examples. The same proof logging approach works for any heuristic whose inferences can be efficiently expressed as reasoning over pseudo-Boolean constraints.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:09.494850",
    "summary": "這篇論文提出了一種新方法，可以用來證明傳統規劃任務的不可解性或計畫的最佳性，並且可以被獨立第三方驗證。簡單來說，就是可以用一種通用的方式生成證明，來確認規劃任務的最佳解。研究者們展示了如何修改$A^{*}$演算法，以較小的開銷產生最佳性證明，並以模式資料庫啟發式和$h^\\textit{max}$為具體例子。這種證明記錄方法對任何能夠有效地表達為虛偽布林約束的推理的啟發式都適用。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:35.556121",
    "audio_file": "2504.18443.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2504.18443.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:45:17.555515"
  },
  {
    "id": "2504.18453",
    "title": "Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation",
    "url": "https://arxiv.org/abs/2504.18453",
    "authors": "Peiyuan Jing, Kinhei Lee, Zhenxuan Zhang, Huichi Zhou, Zhengqing Yuan, Zhifan Gao, Lei Zhu, Giorgos Papanastasiou, Yingying Fang, Guang Yang",
    "categories": [
      "cs.AI",
      "cs.CL"
    ],
    "published_date": "Mon, 28 Apr 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2504.18453v1 Announce Type: new  Abstract: Radiology report generation is critical for efficiency but current models lack the structured reasoning of experts, hindering clinical trust and explainability by failing to link visual findings to precise anatomical locations. This paper introduces BoxMed-RL, a groundbreaking unified training framework for generating spatially verifiable and explainable radiology reports. Built on a large vision-language model, BoxMed-RL revolutionizes report generation through two integrated phases: (1) In the Pretraining Phase, we refine the model via medical concept learning, using Chain-of-Thought supervision to internalize the radiologist-like workflow, followed by spatially verifiable reinforcement, which applies reinforcement learning to align medical findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze the pretrained weights and train a downstream adapter to ensure fluent and clinically credible reports. This framework precisely mimics radiologists' workflow, compelling the model to connect high-level medical concepts with definitive anatomical evidence. Extensive experiments on public datasets demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR and ROUGE-L metrics compared to state-of-the-art methods. An average 5% improvement in large language model-based metrics further underscores BoxMed-RL's robustness in generating high-quality radiology reports.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:09.494859",
    "summary": "這篇論文提出了一個名為BoxMed-RL的新方法，幫助AI模型像放射科醫師一樣理性思考，生成可靠的放射學報告。透過結合鏈式思維和強化學習，讓模型能夠準確連結醫學概念和解剖位置，提高報告的可靠性和可解釋性。實驗結果顯示，BoxMed-RL相較於現有方法，在報告品質上有顯著提升，進一步確立其在放射學報告生成領域的價值和創新性。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:38.313818",
    "audio_file": "2504.18453.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2504.18453.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:45:28.640791"
  },
  {
    "id": "2504.18530",
    "title": "Scaling Laws For Scalable Oversight",
    "url": "https://arxiv.org/abs/2504.18530",
    "authors": "Joshua Engels, David D. Baek, Subhash Kantamneni, Max Tegmark",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG"
    ],
    "published_date": "Mon, 28 Apr 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2504.18530v1 Announce Type: new  Abstract: Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, we propose a framework that quantifies the probability of successful oversight as a function of the capabilities of the overseer and the system being overseen. Specifically, our framework models oversight as a game between capability-mismatched players; the players have oversight-specific and deception-specific Elo scores that are a piecewise-linear function of their general intelligence, with two plateaus corresponding to task incompetence and task saturation. We validate our framework with a modified version of the game Nim and then apply it to four oversight games: \"Mafia\", \"Debate\", \"Backdoor Code\" and \"Wargames\". For each game, we find scaling laws that approximate how domain performance depends on general AI system capability (using Chatbot Arena Elo as a proxy for general capability). We then build on our findings in a theoretical study of Nested Scalable Oversight (NSO), a process in which trusted models oversee untrusted stronger models, which then become the trusted models in the next step. We identify conditions under which NSO succeeds and derive numerically (and in some cases analytically) the optimal number of oversight levels to maximize the probability of oversight success. In our numerical examples, the NSO success rate is below 52% when overseeing systems that are 400 Elo points stronger than the baseline overseer, and it declines further for overseeing even stronger systems.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:09.494868",
    "summary": "這篇論文研究了一種名為「可擴展監督」的方法，讓較弱的人工智慧系統監督更強大的系統，以控制未來超智能系統。他們提出了一個框架，量化了成功監督的機率，取決於監督者和被監督系統的能力。經過一系列遊戲模擬和理論研究，他們找出了最佳的監督層級數量，以最大化成功監督的機率。研究發現，當監督的系統比基準監督者強400 Elo分時，可擴展監督的成功率低於52％，並對監督更強大的系統的成功率進一步下降。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:41.496254",
    "audio_file": "2504.18530.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2504.18530.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:45:39.853714"
  },
  {
    "id": "2504.18536",
    "title": "Adapting Probabilistic Risk Assessment for AI",
    "url": "https://arxiv.org/abs/2504.18536",
    "authors": "Anna Katariina Wisakanto, Joe Rogero, Avyay M. Casheekar, Richard Mallah",
    "categories": [
      "cs.AI",
      "cs.CY",
      "cs.LG",
      "cs.SY",
      "eess.SY",
      "stat.AP"
    ],
    "published_date": "Mon, 28 Apr 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2504.18536v1 Announce Type: new  Abstract: Modern general-purpose artificial intelligence (AI) systems present an urgent risk management challenge, as their rapidly evolving capabilities and potential for catastrophic harm outpace our ability to reliably assess their risks. Current methods often rely on selective testing and undocumented assumptions about risk priorities, frequently failing to make a serious attempt at assessing the set of pathways through which Al systems pose direct or indirect risks to society and the biosphere. This paper introduces the probabilistic risk assessment (PRA) for AI framework, adapting established PRA techniques from high-reliability industries (e.g., nuclear power, aerospace) for the new challenges of advanced AI. The framework guides assessors in identifying potential risks, estimating likelihood and severity, and explicitly documenting evidence, underlying assumptions, and analyses at appropriate granularities. The framework's implementation tool synthesizes the results into a risk report card with aggregated risk estimates from all assessed risks. This systematic approach integrates three advances: (1) Aspect-oriented hazard analysis provides systematic hazard coverage guided by a first-principles taxonomy of AI system aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk pathway modeling analyzes causal chains from system aspects to societal impacts using bidirectional analysis and incorporating prospective techniques; and (3) Uncertainty management employs scenario decomposition, reference scales, and explicit tracing protocols to structure credible projections with novelty or limited data. Additionally, the framework harmonizes diverse assessment methods by integrating evidence into comparable, quantified absolute risk estimates for critical decisions. We have implemented this as a workbook tool for AI developers, evaluators, and regulators, available on the project website.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:09.494879",
    "summary": "這篇論文提出了一個新的概率風險評估框架，專門針對現代通用人工智慧系統可能帶來的風險挑戰。這個框架引入了從高可靠性產業（如核能、航太）中建立的概率風險評估技術，幫助評估人員識別潛在風險、估計發生機率和嚴重性，並明確記錄證據、基本假設和分析。這個系統性方法整合了三個進步：（1）方面導向危害分析提供系統性危害覆蓋；（2）風險途徑建模分析從系統方面到社會影響的因果鏈；（3）不確定性管理結構可信預測。這個框架",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:45.078712",
    "audio_file": "2504.18536.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2504.18536.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:45:52.809279"
  },
  {
    "id": "2504.17792",
    "title": "My Precious Crash Data: Barriers and Opportunities in Encouraging Autonomous Driving Companies to Share Safety-Critical Data",
    "url": "https://arxiv.org/abs/2504.17792",
    "authors": "Hauke Sandhaus, Angel Hsing-Chi Hwang, Wendy Ju, Qian Yang",
    "categories": [
      "cs.HC",
      "cs.AI",
      "cs.DB"
    ],
    "published_date": "Mon, 28 Apr 2025 00:00:00 -0400",
    "source": "ArXiv CS.AI",
    "content": "arXiv:2504.17792v1 Announce Type: cross  Abstract: Safety-critical data, such as crash and near-crash records, are crucial to improving autonomous vehicle (AV) design and development. Sharing such data across AV companies, academic researchers, regulators, and the public can help make all AVs safer. However, AV companies rarely share safety-critical data externally. This paper aims to pinpoint why AV companies are reluctant to share safety-critical data, with an eye on how these barriers can inform new approaches to promote sharing. We interviewed twelve AV company employees who actively work with such data in their day-to-day work. Findings suggest two key, previously unknown barriers to data sharing: (1) Datasets inherently embed salient knowledge that is key to improving AV safety and are resource-intensive. Therefore, data sharing, even within a company, is fraught with politics. (2) Interviewees believed AV safety knowledge is private knowledge that brings competitive edges to their companies, rather than public knowledge for social good. We discuss the implications of these findings for incentivizing and enabling safety-critical AV data sharing, specifically, implications for new approaches to (1) debating and stratifying public and private AV safety knowledge, (2) innovating data tools and data sharing pipelines that enable easier sharing of public AV safety data and knowledge; (3) offsetting costs of curating safety-critical data and incentivizing data sharing.",
    "content_type": "academic",
    "processed": true,
    "fetch_date": "2025-04-28T12:43:09.494889",
    "summary": "這篇論文探討自駕車公司為何不願意分享事故相關資料，發現兩大障礙：一是資料中蘊含重要知識且耗費資源，內部分享也充滿政治考量；二是公司認為安全知識是競爭優勢，不是為社會福祉而公開。研究指出這些發現對於鼓勵自駕車安全資料分享有重要啟示，包括如何辯論和區分公共與私人安全知識，以及創新資料工具和分享管道，降低維護資料成本並激勵分享。",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-04-28T12:43:47.557149",
    "audio_file": "2504.17792.mp3",
    "audio_path": "/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/data/audio/articles/2504.17792.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-04-28T12:46:05.352515"
  }
]