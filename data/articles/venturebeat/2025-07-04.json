[
  {
    "id": "662bbc53073607c784bc1f74dbbd5c79",
    "title": "Sakana AI’s TreeQuest: Deploy multi-model teams that outperform individual LLMs by 30%",
    "url": "https://venturebeat.com/ai/sakana-ais-treequest-deploy-multi-model-teams-that-outperform-individual-llms-by-30/",
    "authors": "Ben Dickson",
    "published_date": "2025-07-03T22:00:19+00:00",
    "source": "VentureBeat",
    "summary": "Sakana AI提出了一個新技術，讓多個大型語言模型合作，形成一個AI夢幻團隊，比單一模型表現提升了30%。企業可以結合不同模型的優勢，讓AI系統更強大。這種集體智慧的方法讓AI系統像人類團隊一樣合作，解決更複雜的問題。透過結合不同模型的智慧，AI系統可以解決單一模型難以克服的問題。",
    "content": "Sakana AI's TreeQuest: Deploy multi-model teams that outperform individual LLMs by 30% | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nSakana AI’s TreeQuest: Deploy multi-model teams that outperform individual LLMs by 30%\nBen Dickson\n@BenDee983\nJuly 3, 2025 3:00 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage credit: VentureBeat with ChatGPT\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nJapanese AI lab\nSakana AI\nhas introduced a new technique that allows multiple large language models (LLMs) to cooperate on a single task, effectively creating a “dream team” of AI agents. The method, called\nMulti-LLM AB-MCTS\n, enables models to perform trial-and-error and combine their unique strengths to solve problems that are too complex for any individual model.\nFor enterprises, this approach provides a means to develop more robust and capable AI systems. Instead of being locked into a single provider or model, businesses could dynamically leverage the best aspects of different frontier models, assigning the right AI for the right part of a task to achieve superior results.\nThe power of collective intelligence\nFrontier AI models are evolving rapidly. However, each model has its own distinct strengths and weaknesses derived from its unique training data and architecture. One might excel at coding, while another excels at creative writing. Sakana AI’s researchers argue that these differences are not a bug, but a feature.\n“We see these biases and varied aptitudes not as limitations, but as precious resources for creating collective intelligence,” the researchers state in their\nblog post\n. They believe that just as humanity’s greatest achievements come from diverse teams, AI systems can also achieve more by working together. “By pooling their intelligence, AI systems can solve problems that are insurmountable for any single model.”\nThinking longer at inference time\nSakana AI’s new algorithm is an “inference-time scaling” technique (also referred to as “\ntest-time scaling\n”), an area of research that has become very popular in the past year. While most of the focus in AI has been on “training-time scaling” (making models bigger and training them on larger datasets), inference-time scaling improves performance by allocating more computational resources after a model is already trained.\nOne common approach involves using reinforcement learning to prompt models to generate longer, more detailed\nchain-of-thought\n(CoT) sequences, as seen in popular models such as OpenAI o3 and\nDeepSeek-R1\n. Another, simpler method is repeated sampling, where the model is given the same prompt multiple times to generate a variety of potential solutions, similar to a brainstorming session. Sakana AI’s work combines and advances these ideas.\n“Our framework offers a smarter, more strategic version of Best-of-N (aka repeated sampling),” Takuya Akiba, research scientist at Sakana AI and co-author of the paper, told VentureBeat. “It complements reasoning techniques like long CoT through RL. By dynamically selecting the search strategy and the appropriate LLM, this approach maximizes performance within a limited number of LLM calls, delivering better results on complex tasks.”\nHow adaptive branching search works\nThe core of the new method is an algorithm called Adaptive Branching Monte Carlo Tree Search (AB-MCTS). It enables an LLM to effectively perform trial-and-error by intelligently balancing two different search strategies: “searching deeper” and “searching wider.” Searching deeper involves taking a promising answer and repeatedly refining it, while searching wider means generating completely new solutions from scratch. AB-MCTS combines these approaches, allowing the system to improve a good idea but also to pivot and try something new if it hits a dead end or discovers another promising direction.\nTo accomplish this, the system uses\nMonte Carlo Tree Search\n(MCTS), a decision-making algorithm famously used by\nDeepMind’s AlphaGo\n. At each step, AB-MCTS uses probability models to decide whether it’s more strategic to refine an existing solution or generate a new one.\nDifferent test-time scaling strategies Source: Sakana AI\nThe researchers took this a step further with Multi-LLM AB-MCTS, which not only decides “what” to do (refine vs. generate) but also “which” LLM should do it. At the start of a task, the system doesn’t know which model is best suited for the problem. It begins by trying a balanced mix of available LLMs and, as it progresses, learns which models are more effective, allocating more of the workload to them over time.\nPutting the AI ‘dream team’ to the test\nThe researchers tested their Multi-LLM AB-MCTS system on the\nARC-AGI-2 benchmark\n. ARC (Abstraction and Reasoning Corpus) is designed to test a human-like ability to solve novel visual reasoning problems, making it notoriously difficult for AI.\nThe team used a combination of frontier models, including\no4-mini\n,\nGemini 2.5 Pro\n, and DeepSeek-R1.\nThe collective of models was able to find correct solutions for over 30% of the 120 test problems, a score that significantly outperformed any of the models working alone. The system demonstrated the ability to dynamically assign the best model for a given problem. On tasks where a clear path to a solution existed, the algorithm quickly identified the most effective LLM and used it more frequently.\nAB-MCTS vs individual models Source: Sakana AI\nMore impressively, the team observed instances where the models solved problems that were previously impossible for any single one of them. In one case, a solution generated by the o4-mini model was incorrect. However, the system passed this flawed attempt to DeepSeek-R1 and Gemini-2.5 Pro, which were able to analyze the error, correct it, and ultimately produce the right answer.\n“This demonstrates that Multi-LLM AB-MCTS can flexibly combine frontier models to solve previously unsolvable problems, pushing the limits of what is achievable by using LLMs as a collective intelligence,” the researchers write.\nAB-MTCS can select different models at different stages of solving a problem Source: Sakana AI\n“In addition to the individual pros and cons of each model, the tendency to hallucinate can vary significantly among them,” Akiba said. “By creating an ensemble with a model that is less likely to hallucinate, it could be possible to achieve the best of both worlds: powerful logical capabilities and strong groundedness. Since hallucination is a major issue in a business context, this approach could be valuable for its mitigation.”\nFrom research to real-world applications\nTo help developers and businesses apply this technique, Sakana AI has released the underlying algorithm as an open-source framework called\nTreeQuest\n, available under an Apache 2.0 license (usable for commercial purposes). TreeQuest provides a flexible API, allowing users to implement Multi-LLM AB-MCTS for their own tasks with custom scoring and logic.\n“While we are in the early stages of applying AB-MCTS to specific business-oriented problems, our research reveals significant potential in several areas,” Akiba said.\nBeyond the ARC-AGI-2 benchmark, the team was able to successfully apply AB-MCTS to tasks like complex algorithmic coding and improving the accuracy of machine learning models.\n“AB-MCTS could also be highly effective for problems that require iterative trial-and-error, such as optimizing performance metrics of existing software,” Akiba said. “For example, it could be used to automatically find ways to improve the response latency of a web service.”\nThe release of a practical, open-source tool could pave the way for a new class of more powerful and reliable enterprise AI applications.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe Briefing for Tech Decision-Makers\nStay ahead in AI, data, and security with VB Daily—trusted by 100K+ industry leaders.\nSubscribe Here\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-04T15:07:00.469770",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-04T15:07:15.136603",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/662bbc53073607c784bc1f74dbbd5c79.mp3",
    "audio_file": "audio/articles/662bbc53073607c784bc1f74dbbd5c79.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-07-04T15:08:07.100650",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "c9838a52cf59b05589b01e7c31f531ee",
    "title": "Dust hits $6M ARR helping enterprises build AI agents that actually do stuff instead of just talking",
    "url": "https://venturebeat.com/ai/dust-hits-6m-arr-helping-enterprises-build-ai-agents-that-actually-do-stuff-instead-of-just-talking/",
    "authors": "Michael Nuñez",
    "published_date": "2025-07-03T17:00:00+00:00",
    "source": "VentureBeat",
    "summary": "一家名為Dust的人工智慧平台幫助企業建立AI代理人，不僅能對話，還能實際完成業務流程，年收入達到600萬美元，是去年的6倍。這顯示企業AI採用已從簡單的聊天機器人轉向能在業務應用中採取具體行動的系統。Dust的AI代理人不僅能回答問題，還能自動創建GitHub問題、安排日曆會議、更新客戶記錄，甚至根據內部編碼標準推送代碼審查，同時保持企業級安全協議。",
    "content": "Dust hits $6M ARR helping enterprises build AI agents that actually do stuff instead of just talking | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nDust hits $6M ARR helping enterprises build AI agents that actually do stuff instead of just talking\nMichael Nuñez\n@MichaelFNunez\nJuly 3, 2025 10:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nDust\n, a two-year-old artificial intelligence platform that helps enterprises build AI agents capable of completing entire business workflows, has reached $6 million in annual revenue — a six-fold increase from $1 million just one year ago. The company’s rapid growth signals a shift in enterprise AI adoption from simple chatbots toward sophisticated systems that can take concrete actions across business applications.\nThe San Francisco-based startup announced Thursday that it has been selected as part of Anthropic’s “Powered by Claude” ecosystem, highlighting a new category of AI companies building specialized enterprise tools on top of frontier language models rather than developing their own AI systems from scratch.\n“Users want more than just conversational interfaces,” said Gabriel Hubert, CEO and co-founder of Dust, in an interview with VentureBeat. “Instead of generating a draft, they want to create the actual document automatically. Rather than getting meeting summaries, they need CRM records updated without manual intervention.”\nDust’s platform goes far beyond the chatbot-style AI tools that dominated early enterprise adoption. Instead of simply answering questions, Dust’s AI agents can automatically create GitHub issues, schedule calendar meetings, update customer records, and even push code reviews based on internal coding standards–all while maintaining enterprise-grade security protocols.\nHow AI agents turn sales calls into automated GitHub tickets and CRM updates\nThe company’s approach becomes clear through a concrete example Hubert described: a business-to-business sales company using multiple Dust agents to process sales call transcripts. One agent analyzes which sales arguments resonated with prospects and automatically updates battle cards in Salesforce. Simultaneously, another agent identifies customer feature requests, maps them to the product roadmap, and in some cases, automatically generates GitHub tickets for small features deemed ready for development.\n“Each call transcript is going to be analyzed by multiple agents,” Hubert explained. “You’ll have a sales battle card optimizer agent that’s going to look at the arguments the salesperson made, which ones were powerful and seem to resonate with the prospect, and that’s going to go and feed into a process on the Salesforce side.”\nThis level of automation is enabled by the\nModel Context Protocol (MCP)\n, a new standard developed by Anthropic that allows AI systems to securely connect with external data sources and applications. Guillaume Princen, Head of EMEA at Anthropic, described MCP as “like a USB-C connector between AI models and apps,” enabling agents to access company data while maintaining security boundaries.\nWhy Claude and MCP are powering the next wave of enterprise AI automation\nDust’s success reflects broader changes in how enterprises are approaching AI implementation. Rather than building custom models, companies like Dust are leveraging increasingly capable foundation models — particularly Anthropic’s Claude 4 suite — and combining them with specialized orchestration software.\n“We just want to give our customers access to the best models,” Hubert said. “And I think right now, Anthropic is early in the lead, especially on coding related models.” The company charges customers $40-50 per user per month and serves thousands of workspaces ranging from small startups to large enterprises with thousands of employees.\nAnthropic’s Claude models have seen particularly strong adoption for coding tasks, with the company reporting 300% growth in Claude Code usage over the past four weeks following the release of its latest Claude 4 models. “Opus 4 is the most powerful model for coding in the world,” Princen noted. “We were already leading the coding race. We’re reinforcing that.”\nEnterprise security gets complex when AI agents can actually take action\nThe shift toward AI agents that can take real actions across business systems introduces new security complexities that didn’t exist with simple chatbot implementations. Dust addresses this through what Hubert calls a “native permissioning layer” that separates data access rights from agent usage rights.\n“Permission creation, as well as data & tool management is part of the onboarding experience to mitigate sensitive data exposure when AI agents operate across multiple business systems,” the company explains in technical documentation. This becomes critical when agents have the ability to create GitHub issues, update CRM records, or modify documents across an organization’s technology stack.\nThe company implements enterprise-grade infrastructure with Anthropic’s Zero Data Retention policies, ensuring that sensitive business information processed by AI agents isn’t stored by the model provider. This addresses a key concern for enterprises considering AI adoption at scale.\nThe rise of AI-native startups building on foundation models instead of creating their own\nDust’s growth is part of what Anthropic calls an emerging ecosystem of “AI native startups”—companies that fundamentally couldn’t exist without advanced AI capabilities. These firms are building businesses not by developing their own AI models, but by creating sophisticated applications on top of existing foundation models.\n“These companies have a very, very strong sense of what their end customers need and want for that specific use case,” Princen explained. “We’re providing the tools for them to kind of build and adapt their product to those specific customers and use cases they’re looking for.”\nThis approach represents a significant shift in the AI industry’s structure. Instead of every company needing to develop its own AI capabilities, specialized platforms like Dust can provide the orchestration layer that makes powerful AI models useful for specific business applications.\nWhat Dust’s $6M revenue growth signals about the future of enterprise software\nThe success of companies like Dust suggests that the enterprise AI market is moving beyond the experimental phase toward practical implementation. Rather than replacing human workers wholesale, these systems are designed to eliminate routine tasks and context-switching between applications, allowing employees to focus on higher-value activities.\n“By providing universal AI primitives that make all company workflows more intelligent as well as a proper permissioning system, we are setting the foundations for an agent operating system that is future-proof,” Hubert said.\nThe company’s customer base includes organizations convinced that AI will fundamentally change business operations. “The common thread between all customers is that they’re pretty stemmed towards the future and convinced that this technology is going to change a lot of things,” Hubert noted.\nAs AI models become more capable and protocols like MCP mature, the distinction between AI tools that simply provide information and those that take action is likely to become a key differentiator in the enterprise market. Dust’s rapid revenue growth suggests that businesses are willing to pay premium prices for AI systems that can complete real work rather than just assist with it.\nThe implications extend beyond individual companies to the broader structure of enterprise software. If AI agents can seamlessly integrate and automate workflows across disconnected business applications, it could reshape how organizations think about software procurement and workflow design—potentially reducing the complexity that has long plagued enterprise technology stacks.\nPerhaps the most telling sign of this transformation is how naturally Hubert describes AI agents not as tools, but as digital employees that show up to work every day. In a business world that has spent decades connecting systems with APIs and integration platforms, companies like Dust are proving that the future might not require connecting everything—just teaching AI to navigate the chaos we’ve already built.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe Briefing for Tech Decision-Makers\nStay ahead in AI, data, and security with VB Daily—trusted by 100K+ industry leaders.\nSubscribe Here\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-04T15:07:00.729874",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-04T15:07:19.689400",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/c9838a52cf59b05589b01e7c31f531ee.mp3",
    "audio_file": "audio/articles/c9838a52cf59b05589b01e7c31f531ee.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-07-04T15:08:14.305519",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "607f0974fed82d021371c239a6f145eb",
    "title": "HOLY SMOKES! A new, 200% faster DeepSeek R1-0528 variant appears from German lab TNG Technology Consulting GmbH",
    "url": "https://venturebeat.com/ai/holy-smokes-a-new-200-faster-deepseek-r1-0528-variant-appears-from-german-lab-tng-technology-consulting-gmbh/",
    "authors": "Carl Franzen",
    "published_date": "2025-07-03T13:32:44+00:00",
    "source": "VentureBeat",
    "summary": "德國TNG Technology Consulting GmbH實驗室推出全新DeepSeek R1-0528變種，比原版快200%。這個新模型名為DeepSeek-TNG R1T2 Chimera，效率更高、速度更快，並且產生的答案更為簡潔，這意味著推理速度更快、計算成本更低。這項創新受到AI開發者的熱烈迴響，有望為AI領域帶來更多進步。",
    "content": "HOLY SMOKES! A new, 200% faster DeepSeek R1-0528 variant appears from German lab TNG Technology Consulting GmbH | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nHOLY SMOKES! A new, 200% faster DeepSeek R1-0528 variant appears from German lab TNG Technology Consulting GmbH\nCarl Franzen\n@carlfranzen\nJuly 3, 2025 6:32 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nIt’s been a little more than a month since Chinese AI startup DeepSeek, an offshoot of Hong Kong-based High-Flyer Capital Management, released the\nlatest version of its hit open source model DeepSeek, R1-0528.\nLike its predecessor, DeepSeek-R1 — which\nrocked the AI and global business communities\nwith how cheaply it was trained and how well it performed on reasoning tasks, all available to developers and enterprises for free — R1-0528 is already being adapted and remixed by other AI labs and developers, thanks in large part to its permissive Apache 2.0 license.\nThis week, the 24-year-old German firm\nTNG Technology Consulting GmbH released one\nsuch adaptation:\nDeepSeek-TNG R1T2 Chimera\n, the latest model in its Chimera large language model (LLM) family. R1T2 delivers a notable boost in efficiency and speed, scoring at upwards of\n90% of R1-0528’s intelligence benchmark scores\n, while generating answers with\nless than 40% of R1-0528’s output token count\n.\nThat means it produces shorter responses, translating directly into\nfaster inference and lower compute costs\n. On the model card TNG released for its new R1T2 on the AI code sharing community Hugging Face, the company states that it is “about 20% faster than the regular R1” (the one released back in January) “and more than twice as fast as R1-0528” (the May official update from DeepSeek).\nAlready, the response has been incredibly positive from the AI developer community. “DAMN! DeepSeek R1T2 – 200% faster than R1-0528 & 20% faster than R1,” wrote Vaibhav (VB) Srivastav, a senior leader at Hugging Face,\non X\n. “Significantly better than R1 on GPQA & AIME 24, made via Assembly of Experts with DS V3, R1 & R1-0528 — and it’s MIT-licensed, available on Hugging Face.”\nThis gain is made possible by TNG’s Assembly-of-Experts (AoE) method — a technique for building LLMs by selectively merging the weight tensors (internal parameters) from multiple pre-trained models that TNG described in a\npaper published in May\non arXiv, the non-peer reviewed open access online journal.\nA successor to the original R1T Chimera, R1T2 introduces a new “Tri-Mind” configuration that integrates three parent models: DeepSeek-R1-0528, DeepSeek-R1, and DeepSeek-V3-0324. The result is a model engineered to maintain high reasoning capability while significantly reducing inference cost.\nR1T2 is constructed without further fine-tuning or retraining. It inherits the reasoning strength of R1-0528, the structured thought patterns of R1, and the concise, instruction-oriented behavior of V3-0324 — delivering a more efficient, yet capable model for enterprise and research use.\nHow Assembly-of-Experts (AoE) Differs from Mixture-of-Experts (MoE)\nMixture-of-Experts (MoE) is an architectural design in which different components, or “experts,” are conditionally activated per input. In MoE LLMs like DeepSeek-V3 or Mixtral, only a subset of the model’s expert layers (e.g., 8 out of 256) are active during any given token’s forward pass. This allows very large models to achieve higher parameter counts and specialization while keeping inference costs manageable — because only a fraction of the network is evaluated per token.\nAssembly-of-Experts (AoE) is a model merging technique, not an architecture. It’s used to create a new model from multiple pre-trained MoE models by selectively interpolating their weight tensors.\nThe “experts” in AoE refer to the model components being merged — typically the routed expert tensors within MoE layers — not experts dynamically activated at runtime.\nTNG’s implementation of AoE focuses primarily on merging routed expert tensors — the part of a model most responsible for specialized reasoning — while often retaining the more efficient shared and attention layers from faster models like V3-0324. This approach enables the resulting Chimera models to inherit reasoning strength without replicating the verbosity or latency of the strongest parent models.\nPerformance and Speed: What the Benchmarks Actually Show\nAccording to benchmark comparisons presented by TNG, R1T2 achieves between\n90% and 92%\nof the reasoning performance of its most intelligent parent, DeepSeek-R1-0528, as measured by AIME-24, AIME-25, and GPQA-Diamond test sets.\nHowever, unlike DeepSeek-R1-0528 — which tends to produce long, detailed answers due to its extended chain-of-thought reasoning — R1T2 is designed to be much more concise. It delivers similarly intelligent responses while using significantly fewer words.\nRather than focusing on raw processing time or tokens-per-second, TNG measures “speed” in terms of\noutput token count per answer\n— a practical proxy for both cost and latency. According to benchmarks shared by TNG, R1T2 generates responses using\napproximately 40% of the tokens\nrequired by R1-0528.\nThat translates to a\n60% reduction in output length\n, which directly reduces inference time and compute load, speeding up responses by 2X, or 200%.\nWhen compared to the original DeepSeek-R1, R1T2 is also around\n20% more concise on average\n, offering meaningful gains in efficiency for high-throughput or cost-sensitive deployments.\nThis efficiency does not come at the cost of intelligence. As shown in the benchmark chart presented in TNG’s technical paper, R1T2 sits in a desirable zone on the intelligence vs. output cost curve. It preserves reasoning quality while minimizing verbosity — an outcome critical to enterprise applications where inference speed, throughput, and cost all matter.\nDeployment Considerations and Availability\nR1T2 is released under a permissive MIT License and is available now on Hugging Face, meaning it is open source and available to be used and built into commercial applications.\nTNG notes that while the model is well-suited for general reasoning tasks, it is not currently recommended for use cases requiring function calling or tool use, due to limitations inherited from its DeepSeek-R1 lineage. These may be addressed in future updates.\nThe company also advises European users to assess compliance with the EU AI Act, which comes into effect on August 2, 2025.\nEnterprises operating in the EU should review relevant provisions or consider halting model use after that date if requirements cannot be met.\nHowever, U.S. companies operating domestically and servicing U.S.-based users, or those of other nations, are\nnot\nsubject to the terms of the EU AI Act, which should give them considerable flexibility when using and deploying this free, speedy open source reasoning model. If they service users in the E.U., some\nprovisions of the EU Act will still apply\n.\nTNG has already made prior Chimera variants available through platforms like OpenRouter and Chutes, where they reportedly processed billions of tokens daily. The release of R1T2 represents a further evolution in this public availability effort.\nAbout TNG Technology Consulting GmbH\nFounded in January 2001,\nTNG Technology Consulting GmbH\nis based in Bavaria, Germany, and employs over 900 people, with a high concentration of PhDs and technical specialists.\nThe company focuses on software development, artificial intelligence, and DevOps/cloud services, serving major enterprise clients across industries such as telecommunications, insurance, automotive, e-commerce, and logistics.\nTNG operates as a values-based consulting partnership. Its unique structure, grounded in operational research and self-management principles, supports a culture of technical innovation.\nIt actively contributes to open-source communities and research, as demonstrated through public releases like R1T2 and the publication of its Assembly-of-Experts methodology.\nWhat It Means for Enterprise Technical Decision-Makers\nFor CTOs, AI platform owners, engineering leads, and IT procurement teams, R1T2 introduces tangible benefits and strategic options:\nLower Inference Costs\n: With fewer output tokens per task, R1T2 reduces GPU time and energy consumption, translating directly into infrastructure savings — especially important in high-throughput or real-time environments.\nHigh Reasoning Quality Without Overhead\n: It preserves much of the reasoning power of top-tier models like R1-0528, but without their long-windedness. This is ideal for structured tasks (math, programming, logic) where concise answers are preferable.\nOpen and Modifiable\n: The MIT License allows full deployment control and customization, enabling private hosting, model alignment, or further training within regulated or air-gapped environments.\nEmerging Modularity\n: The AoE approach suggests a future where models are built modularly, allowing enterprises to assemble specialized variants by recombining strengths of existing models, rather than retraining from scratch.\nCaveats\n: Enterprises relying on function-calling, tool use, or advanced agent orchestration should note current limitations, though future Chimera updates may address these gaps.\nTNG encourages researchers, developers, and enterprise users to explore the model, test its behavior, and provide feedback. The R1T2 Chimera is available at\nhuggingface.co/tngtech/DeepSeek-TNG-R1T2-Chimera\n, and technical inquiries can be directed to\nresearch@tngtech.com\n.\nFor technical background and benchmark methodology, TNG’s research paper is available at\narXiv:2506.14794\n.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nThe Briefing for Tech Decision-Makers\nStay ahead in AI, data, and security with VB Daily—trusted by 100K+ industry leaders.\nSubscribe Here\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-04T15:07:00.986168",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-04T15:07:22.763997",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/607f0974fed82d021371c239a6f145eb.mp3",
    "audio_file": "audio/articles/607f0974fed82d021371c239a6f145eb.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-07-04T15:08:22.229778",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  }
]