[
  {
    "id": "5bab8d942db663bfbd853a81c32aabb6",
    "title": "Chinese startup Z.ai launches powerful open source GLM-4.5 model family with PowerPoint creation",
    "url": "https://venturebeat.com/ai/chinese-startup-z-ai-launches-powerful-open-source-glm-4-5-model-family-with-powerpoint-creation/",
    "authors": "Carl Franzen",
    "published_date": "2025-07-28T23:33:40+00:00",
    "source": "VentureBeat",
    "summary": "一家名為Z.ai的中國初創公司推出了強大的開源GLM-4.5模型家族，並搭配PowerPoint製作功能。這兩個新的模型被視為AI推理、行為和編碼的首選解決方案，表現優異。GLM-4.5甚至在一些測試中超越了美國領先的專有模型，而GLM-4.5-Air也表現出色。這兩個模型還具有雙重操作模式，可根據情況進行複雜推理或即時回應。此外，它們還能從單個標題或提示自動生成完整的PowerPoint簡報。",
    "content": "Chinese startup Z.ai launches powerful open source GLM-4.5 model family with PowerPoint creation | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nChinese startup Z.ai launches powerful open source GLM-4.5 model family with PowerPoint creation\nCarl Franzen\n@carlfranzen\nJuly 28, 2025 4:33 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nAnother week in the summer of 2025 has begun, and in a continuation of the\ntrend from last week\n, with it arrives more powerful Chinese open source AI models.\nLittle-known (at least to us here in the West) Chinese startup\nZ.ai has introduced two new open source LLMs\n—\nGLM-4.5\nand\nGLM-4.5-Air\n— casting them as go-to solutions for AI reasoning, agentic behavior, and coding.\nAnd according to\nZ.ai’s blog post\n, the models perform near the top of the pack of other proprietary LLM leaders in the U.S.\nFor example, the flagship GLM-4.5 matches or outperforms leading proprietary models like\nClaude 4 Sonnet\n,\nClaude 4 Opus\n, and\nGemini 2.5 Pro\non evaluations such as\nBrowseComp\n,\nAIME24\n, and\nSWE-bench Verified\n, while ranking third overall across a dozen competitive tests.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nIts lighter-weight sibling, GLM-4.5-Air, also performs within the top six, offering strong results relative to its smaller scale.\nBoth models feature dual operation modes: a thinking mode for complex reasoning and tool use, and a non-thinking mode for instant response scenarios. They can a\nutomatically generate complete PowerPoint presentations from a single title or prompt,\nmaking them useful for meeting preparation, education, and internal reporting.\nThey further offer creative writing, emotionally aware copywriting, and script generation to create branded content for social media and the web. Moreover, z.ai says they support virtual character development and turn-based dialogue systems for customer support, roleplaying, fan engagement, or digital persona storytelling.\nWhile both models support reasoning, coding, and agentic capabilities, GLM-4.5-Air is designed for teams seeking a lighter-weight, more cost-efficient alternative with faster inference and lower resource requirements.\nZ.ai\nalso lists several specialized models in the GLM-4.5 family on its API\n, including\nGLM-4.5-X\nand\nGLM-4.5-AirX\nfor ultra-fast inference, and\nGLM-4.5-Flash\n, a free variant optimized for coding and reasoning tasks.\nThey’re available now to use directly on\nZ.ai\nand through the\nZ.ai application programming interface (API)\nfor developers to connect to third-party apps, and their code is available on HuggingFace and ModelScope. The company also provides multiple integration routes, including support for inference via vLLM and SGLang.\nLicensing and API pricing\nGLM-4.5 and GLM-4.5-Air are released under the\nApache 2.0 license\n, a permissive and commercially friendly open-source license.\nThis allows developers and organizations to freely\nuse, modify, self-host, fine-tune, and redistribute\nthe models for both research and commercial purposes.\nFor those who don’t want to download the model code or weights and self-host or deploy on their own, z.ai’s cloud-based API offers the model for the following prices.\nGLM-4.5\n:\n$0.60\n/\n$2.20 per 1 million input/output tokens\nGLM-4.5-Air\n:\n$0.20 /\n$1.10 per 1M input/output tokens\nA\nCNBC article on the models\nreported that z.ai would charge only $0.11 / $0.28 per million input/output tokens, which is also supported by a Chinese graphic the company posted on its API documentation for the “Air model.”\nHowever, this appears to be the case only for inputting up to 32,000 tokens and outputting 200 tokens at a single time. (Recall tokens are the numerical designations the LLM uses to represent different semantic concepts and word components, the LLM’s native language, with each token translating to a word or portion of a word).\nIn fact, the Chinese graphic reveals far more detailed pricing for both models per batches of tokens inputted/outputted. I’ve tried to translate it below:\nAnother note: since z.ai is based in China, those in the West who are focused on data sovereignty will want to due diligence through internal policies to pursue using the API, as it may be subject to Chinese content restrictions.\nCompetitive performance on third-party benchmarks, approaching that of leading closed/proprietary LLMs\nGLM-4.5 ranks third across 12 industry benchmarks measuring agentic, reasoning, and coding performance—trailing only OpenAI’s GPT-4 and xAI’s Grok 4. GLM-4.5-Air, its more compact sibling, lands in sixth position.\nIn agentic evaluations, GLM-4.5 matches Claude 4 Sonnet in performance and exceeds Claude 4 Opus in web-based tasks. It achieves a 26.4% accuracy on the BrowseComp benchmark, compared to Claude 4 Opus’s 18.8%. In the reasoning category, it scores competitively on tasks such as MATH 500 (98.2%), AIME24 (91.0%), and GPQA (79.1%).\nFor coding, GLM-4.5 posts a 64.2% success rate on SWE-bench Verified and 37.5% on Terminal-Bench. In pairwise comparisons, it outperforms Qwen3-Coder with an 80.8% win rate and beats Kimi K2 in 53.9% of tasks. Its agentic coding ability is enhanced by integration with tools like Claude Code, Roo Code, and CodeGeex.\nThe model also leads in tool-calling reliability, with a success rate of 90.6%, edging out Claude 4 Sonnet and the new-ish Kimi K2.\nPart of the wave of open source Chinese LLMs\nThe release of GLM-4.5 arrives amid a surge of competitive open-source model launches in China, most notably from\nAlibaba’s Qwen Team\n.\nIn the span of a single week, Qwen released\nfour new open-source LLMs\n, including the reasoning-focused\nQwen3-235B-A22B-Thinking-2507\n, which now tops or matches leading models such as OpenAI’s o4-mini and Google’s Gemini 2.5 Pro on reasoning benchmarks like AIME25, LiveCodeBench, and GPQA.\nThis week, Alibaba continued the trend with\nthe release of Wan 2.2\n, a powerful new open source video model.\nAlibaba’s new models are, like z.ai, licensed under\nApache 2.0\n, allowing commercial usage, self-hosting, and integration into proprietary systems.\nThe broad availability and permissive licensing of Alibaba’s offerings and Chinese startup Moonshot before it with its Kimi K2 model reflects an ongoing strategic effort by Chinese AI companies to position open-source infrastructure as a viable alternative to closed U.S.-based models.\nIt also places pressure on the U.S.-based model provider efforts to compete in open source.\nMeta has been on a hiring spree\nafter its Llama 4 model family debuted earlier this year to a mixed response from the AI community,\nincluding a hefty dose of criticism\nfor what some AI power users saw as benchmark gaming and inconsistent performance.\nMeanwhile, OpenAI co-founder and CEO Sam Altman recently announced that OpenAI’s long-awaited and much-hyped frontier open source LLM — its first since before ChatGPT launched in late 2022 —\nwould be delayed\nfrom its originally planned July release to an as-yet unspecified later date.\nArchitecture and training lessons revealed\nGLM-4.5 is built with 355 billion total and 32 billion active parameters. Its counterpart, GLM-4.5-Air, offers a lighter-weight design at 106 billion total and 12 billion active parameters.\nBoth use a Mixture-of-Experts (MoE) architecture, optimized with loss-free balance routing, sigmoid gating, and increased depth for enhanced reasoning.\nThe self-attention block includes Grouped-Query Attention and a higher number of attention heads. A Multi-Token Prediction (MTP) layer enables speculative decoding during inference.\nPre-training spans 22 trillion tokens split between general-purpose and code/reasoning corpora. Mid-training adds 1.1 trillion tokens from repo-level code data, synthetic reasoning inputs, and long-context/agentic sources.\nZ.ai’s post-training process for GLM-4.5 relied upon a reinforcement learning phase powered by its in-house RL infrastructure,\nslime\n, which separates data generation and model training processes to optimize throughput on agentic tasks.\nAmong the techniques they used were mixed-precision rollouts and adaptive curriculum learning.\nThe former help the model train faster and more efficiently by using lower-precision math when generating data, without sacrificing much accuracy.\nMeanwhile, adaptive curriculum learning means the model starts with easier tasks and gradually moves to harder ones, helping it learn more complex tasks gradually over time.\nGLM-4.5’s architecture prioritizes computational efficiency. According to\nCNBC\n, Z.ai CEO\nZhang Peng\nstated that the model runs on just eight\nNvidia H20 GPUs\n— custom silicon designed for the Chinese market to comply with U.S. export controls. That’s roughly half the hardware requirement of DeepSeek’s comparable models.\nInteractive demos\nZ.ai highlights full-stack development, slide creation, and interactive artifact generation as demonstration\nareas on its blog post\n.\nExamples include a Flappy Bird clone,\nPokémon Pokédex web app\n, and slide decks built from structured documents or web queries.\nUsers can interact with these features on the Z.ai chat platform or through API integration.\nCompany background and market position\nZ.ai was founded in 2019 under the name Zhipu, and has since grown into one of China’s most prominent AI startups, according to\nCNBC\n.\nThe company has raised over $1.5 billion from investors including Alibaba, Tencent, Qiming Venture Partners, and municipal funds from Hangzhou and Chengdu, with additional backing from Aramco-linked Prosperity7 Ventures.\nIts GLM-4.5 launch coincides with the World Artificial Intelligence Conference in Shanghai, where multiple Chinese firms showcased advancements. Z.ai was also named in a June OpenAI report highlighting Chinese progress in AI, and has since been added to a U.S. entity list limiting business with American firms.\nWhat it means for enterprise technical decision-makers\nFor senior AI engineers, data engineers, and AI orchestration leads tasked with building, deploying, or scaling language models in production, the GLM-4.5 family’s release under the\nApache 2.0 license\npresents a meaningful shift in options.\nThe model offers performance that rivals top proprietary systems across reasoning, coding, and agentic benchmarks — yet comes with full weight access, commercial usage rights, and flexible deployment paths, including cloud, private, or on-prem environments.\nFor those managing LLM lifecycles — whether leading model fine-tuning, orchestrating multi-stage pipelines, or integrating models with internal tools — GLM-4.5 and GLM-4.5-Air reduce barriers to testing and scaling.\nThe models support standard OpenAI-style interfaces and tool-calling formats, making it easier to evaluate in sandboxed environments or drop into existing agent frameworks.\nGLM-4.5 also supports\nstreaming output, context caching, and structured JSON responses\n, enabling smoother integration with enterprise systems and real-time interfaces. For teams building autonomous tools, its deep thinking mode provides more precise control over multi-step reasoning behavior.\nFor teams under budget constraints or those seeking to avoid vendor lock-in, the pricing structure undercuts major proprietary alternatives like DeepSeek and Kimi K2. This matters for organizations where usage volume, long-context tasks, or data sensitivity make open deployment a strategic necessity.\nFor professionals in AI infrastructure and orchestration, such as those implementing CI/CD pipelines, monitoring models in production, or managing GPU clusters, GLM-4.5’s support for vLLM, SGLang, and mixed-precision inference aligns with current best practices in efficient, scalable model serving. Combined with open-source RL infrastructure (slime) and a modular training stack, the model’s design offers flexibility for tuning or extending in domain-specific environments.\nIn short, GLM-4.5’s launch gives enterprise teams a viable, high-performing foundation model they can\ncontrol, adapt, and scale\n, without being tied to proprietary APIs or pricing structures. It’s a compelling option for teams balancing innovation, performance, and operational constraints.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nIs your ai infrastructure ready for what's next?\nExplore how four enterprises built AI infrastructure that cuts costs, modernizes systems, and scales performance—fast. In this interactive experience, see what they changed, why it worked, and how you can apply it to your own strategy.\nLearn More",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-29T12:03:44.526176",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-29T12:03:59.702515",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/5bab8d942db663bfbd853a81c32aabb6.mp3",
    "audio_file": "audio/articles/5bab8d942db663bfbd853a81c32aabb6.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-07-29T12:04:36.197504",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "714b2855f45542b783897f1edec5547f",
    "title": "No more links, no more scrolling—The browser is becoming an AI Agent",
    "url": "https://venturebeat.com/ai/no-more-links-no-more-scrolling-the-browser-is-becoming-an-ai-agent/",
    "authors": "Taryn Plumb",
    "published_date": "2025-07-28T23:23:53+00:00",
    "source": "VentureBeat",
    "summary": "這篇新聞講的是未來瀏覽器將會變成AI助手，不再需要點連結或滾動網頁，只要問問題就能直接找到需要的資訊。OpenAI預計推出一款新的AI瀏覽器，可以幫我們找資料並直接連結，改變網頁瀏覽方式。這代表著搜尋引擎將不再只是提供連結，而是直接幫忙找到答案。企業需要重新思考線上策略，因為SEO可能會變得不再重要。",
    "content": "No more links, no more scrolling—The browser is becoming an AI Agent | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nAnalysis\nNo more links, no more scrolling—The browser is becoming an AI Agent\nTaryn Plumb\n@taryn_plumb\nJuly 28, 2025 4:23 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nVentureBeat/Midjourney\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nRumors that\nOpenAI\nis\nset to release\na gen AI-powered web browser to rival\nAlphabet\n‘s Google Chrome have amped up excitement about the future of search and how AI will fundamentally change how we browse the web.\nIn this seeming next phase of the internet, search engines won’t just point to information; intelligent agents will find it for us and even act on it.\n“This isn’t just about better answers; it’s about redefining the interface between humans and the web,” Ja-Naé Duane, a\nBrown University\nfaculty member and\nMIT CISR research fellow\n, told VentureBeat. “By embedding a conversational, task-completing AI into the browser itself, OpenAI is signaling the end of search as we know it.”\nWhat exactly is gen AI-powered search?\nGen AI-powered search is fundamentally different from traditional search, as it not only fetches the most relevant links in response to a query, but summarizes and directly links to them. Users won’t have to scroll URLs, websites or databases to get the information they need. For enterprises, this means that SEO may eventually become obsolete, so they must fundamentally rethink their online strategy.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nPresumably, OpenAI’s goal is to keep users inside\nGPT-like interfaces\nas long as possible. A dedicated browser would allow the company to directly integrate products such as\nOperator\n, which handles repetitive browser tasks.\nThe latter, ultimately, is the future of AI-powered search, experts say: Agents that fetch information for users and get to know their habits, interests and goals.\n“We’re moving into an era where the browser doesn’t just respond, it anticipates,” said Duane. “The future of search is not about finding, it’s about fulfilling.”\nThe current gen AI-powered search landscape\nWhenever OpenAI enters the gen AI-powered search space, it will face a slate of competition, including from\nPerplexity\n,\nDia\n,\nArc\n,\nAndi\n,\nBagoodex\n,\nKomo\nYou.com\nand others.\nNotably, Perplexity’s\nComet\nwas launched earlier this month, but is currently only available to customers on the $200-per-month tier. The company says it will roll out the browser to additional users on an invite-only basis, and eventually make it free.\nPerplexity is “excellent for deep research,” noted Wyatt Mayham of\nNorthwest AI Consulting\n, but its current price tag gears it toward power users, not the mass market.\nPerplexity is “fast, task-oriented” and being increasingly adopted in knowledge work, noted Johnny Hughes, co-founder and CMO at marketing and advertising firm\nAvenue Z\n. “The issue? Source transparency and trust are still hit or miss,” he said. You.com, Arc and others also have good user interface (UI) experimentation, but “lack scale, funding or core differentiators.”\nDia, meanwhile, as Mayham put it, is “rethinking the browser from scratch with modular AI features, but faces the uphill battle of adoption in a space dominated by incumbents.” And, its intent-sensitive automation is also more constrained.\nIncumbents have also taken steps to compete. Chrome has introduced AI Mode and Bing offers Copilot search, while Firefox, DuckDuckGo and others have incorporated AI chatbots and sidebars, as well as integrated AI summaries into search results. Still, these are more conservative and remain closer to traditional assistive search, and are beholden to ad revenue models and legacy UX.\nOpenAI’s potential advantage in search\nWhat could set\nChatGPT\napart from the others is its strong market share, deep industry partnerships — and the fact that it has 500 million weekly active users.\nExperts say one advantage is its task-oriented nature.\n“Instead of giving you a list of links, their upcoming browser agent aims to complete actions (book a flight, order groceries, handle forms),” said Mayham of Northwest AI Consulting. “That’s a different model than Google’s ad-driven approach and has major implications for how discovery happens online.”\nIt is indeed a “big shift in mental models,” agreed Hughes of Avenue Z. Google was built to index and rank, while OpenAI is engineered to understand, synthesize and serve intent-based outcomes.\n“They’re not trying to ‘crawl the web,’ they’re trying to comprehend it,” he said, emphasizing that today’s users are searching for direct answers, not just links.\nOpenAI’s advantage over rivals is its massive developer ecosystem, built-in user behavior via ChatGPT and direct feedback loops from billions of prompts. Where Perplexity functions as a powerful agentic assistant, and Gemini augments search with context and extensions, “OpenAI is positioned to become the OS layer of the internet,” said Hughes.\nBut can OpenAI really topple Google?\nThe browser wars have been ongoing for years, and Chrome remains the far-and-away dominant player.\nAccording to marketing intelligence firm\nDatos\n, the tech giant maintained a 90.15% share of the U.S. user base and 92.49% in Europe between Q1 2024 and Q1 2025. By contrast, ChatGPT accounted for just 0.29% of desktop events in the U.S. and 0.32% in Europe.\n“Short of a miracle, I have a hard time seeing any new browser having any kind of material impact on Google’s browser dominance for quite some time, if at all,” said Eli Goodman, Datos’ CEO and co-founder.\nAI tools will show value in areas including summarization, research acceleration and “mitigating tab fatigue,” he said. “But an existential threat to Google? Not yet.”\nFor AI browsers to truly disrupt the market, they’ll need to prove that their end-to-end experience is not just faster or smarter, but consistently more useful than what users already know, he noted.\nChatGPT is strong at answering well-formed questions using its internal knowledge and language reasoning, but it lacks access to real-time, long-tail and less-indexed web content, said Vladyslav Hamolia, AI product lead at Mac app builder\nMacPaw\n.\n“This is where a traditional browser-plus-search engine still plays a key role, surfacing newly published pages, live prices, event-specific updates or in-depth technical documentation,” said Hamolia. “The browser is not just a UI layer; it’s a gateway to navigating and filtering a vast, dynamic web that models alone cannot fully absorb.”\nGoogle remains dominant in crawling depth (with two decades of crawling infrastructure), semantic understanding of web structure (sitemaps, structured data) and personalized relevance, he noted.\nBrian Jackson, principal research director at\nInfo-Tech Research Group\n, pointed out that Chrome users also likely use Gmail, Google Calendar, Google Docs and other Google platforms. “OpenAI and Perplexity don’t have that same gamut of services.”\nHowever, if their\nAI agents\ncan begin replacing more Google tools beyond search, they can win some market share.\n“We also have to consider what strategy OpenAI and Perplexity take with their browsers,” said Jackson. “Right now, Perplexity makes Comet available only for its paying users, so at the moment, it’s more of an added value to draw in subscribers rather than trying to win browser market share.”\nAdvantages and disadvantages of AI web browsers\nThe advantages of AI search may not be truly seen for some time, said Info-Tech’s Jackson.\nWhile Comet touts its ability to summarize and translate every page instantly, that’s not so different from what can be accomplished with Chrome — especially once you consider its extensive library of available extensions.\n“These AI browsers will literally be trying to interpret the goal of users,” he said. “They will make suggestions, offer to automate routine tasks, find product comparisons or source multiple quotes for services. “Browsers could transition from being mere windows to web content to agentic assistants that help users achieve their digital goals.”\nOn the other hand, resistance to new technology is always a factor, he pointed out. Users who reject the AI summaries they see in core search will likely also reject the notion that AI should be at the forefront of browsing.\n“The early days of user experience will be important here, and if we see browsers recommending that users put glue on pizza or other silly things like that, it won’t help with adoption,” said Jackson.\nAnother distinguishing factor with AI search is models’ ability to persist memory across sessions and assist with task execution in-browser.\n“The risk, however, is user trust,” said Kaveh Vahdat, founder and president at fractional CMO agency\nRiseOpp\n. “A browser that thinks and remembers raises legitimate privacy concerns unless boundaries are clearly defined.”\nMoving from static search bars to dynamic AI interfaces that learn, adapt and integrate with internal systems also introduces new exposure points, especially when proprietary data is surfaced by models operating across public and private content, he noted. Enterprises must be prepared to revisit access controls and ensure AI agents align with governance and compliance standards.\n“These tools are converging in functionality but diverging in user control,” said Vahdat. “The key differentiator may not be capability, but how well each platform balances autonomy with transparency.”\nWhat enterprises should do now\nWhether sooner or later, how should enterprises prepare for a new search environment where SEO is no longer relevant?\nThink of your site as a reference point for AI systems, advised Mayham of Northwest AI Consulting. Content should be clear, factual and structured so AI tools can easily surface information. Also, prepare for conversational commerce by ensuring product data and checkout flows are API-friendly and that AI agents can complete transactions without friction.\nAdditionally, invest in brand authority; if AI cites sources, it’ll use a brand name, not just keywords. “Brand trust is critical,” said Mayham, and is achieved by being featured on other authoritative websites or reviewed well on review platforms.\n“Enterprises should stop thinking in blue links and start building content that answers, reasons and resonates,” agreed Avenue Z’s Hughes.\nThis means:\nStructuring content with AI comprehension in mind (schema, embeddings, FAQs)\nPrioritizing expert-driven, evergreen content that large language models (LLMs) trust\nDiversifying beyond Google (social search, TikTok SEO, YouTube, voice)\nTraining internal teams on prompt engineering and AI integration\nUltimately, it is critical to make the customer experience interoperable with agentic AI, emphasized Brown University’s Duane.\n“Soon, users won’t be browsing; they’ll be delegating,” she said. “You need to prepare your systems not just to be found, but to be understood by AI.”\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nIs your ai infrastructure ready for what's next?\nExplore how four enterprises built AI infrastructure that cuts costs, modernizes systems, and scales performance—fast. In this interactive experience, see what they changed, why it worked, and how you can apply it to your own strategy.\nLearn More",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-29T12:03:44.839167",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-29T12:04:02.206472",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/714b2855f45542b783897f1edec5547f.mp3",
    "audio_file": "audio/articles/714b2855f45542b783897f1edec5547f.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-07-29T12:04:46.278523",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "b7e303fa5cf8bec072118a5764eb4418",
    "title": "Anthropic throttles Claude rate limits, devs call foul",
    "url": "https://venturebeat.com/ai/anthropic-throttles-claude-rate-limits-devs-call-foul/",
    "authors": "Emilia David",
    "published_date": "2025-07-28T22:18:31+00:00",
    "source": "VentureBeat",
    "summary": "Anthropic宣布對Claude訂閱者實施每週使用限制，稱有些用戶一直在全天候運行Claude，尤其是在使用Claude Code產品時。這項新政策將於8月28日開始實施，並與目前的5小時限制同時存在。然而，許多開發者對此反應負面，認為這樣的舉措不公平地懲罰了更多人。Anthropic表示，他們的目標是給開發者充分的Claude使用權限，但也發現有些用戶違反政策，影響了其他人的使用體驗。",
    "content": "Anthropic throttles Claude rate limits, devs call foul | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nAnthropic throttles Claude rate limits, devs call foul\nEmilia David\n@miyadavid\nJuly 28, 2025 3:18 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat, gennerated with MidJourney\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nAnthropic\nannounced today it would introduce weekly rate limits for Claude subscribers, claiming that some users have been running Claude 24/7, with the majority of usage centered\naround its Claude Code product\n.\nOverall weekly limits will begin on August 28 and will be in conjunction with the current 5-hour limits. Anthropic said the throttling will only affect 5% of its total users.\nNot surprisingly, many developers and other users reacted negatively to the news, claiming that the move unfairly punishes more people for the actions of a few. The move also raises the question of how enterprises hoping to run more long-running projects could reach their usage limits much faster.\n“Claude Code has experienced unprecedented demand since launch. We designed our plans to give developers generous access to Claude, and while most users operate within normal patterns, we’ve also seen policy violations like account sharing and reselling access, which affects performance for everyone,” Anthropic said in a statement sent to VentureBeat.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nIt added in an email sent to Claude subscribers that it also noticed “advanced usage patterns like running Claude 24/7 in the background that are impacting system capacity for all.”\nAnthropic added that it continues to support “long running use cases through other options in the future, but until then, weekly limits will help us maintain reliable service for everyone.”\nThe new rate limits\nAnthropic did not specify what the rate limits are, but said most Claude Max 20x users “can expect 240-480 hours of Sonnet 4 and 24-40 hours of Opus 4 within their weekly rate limits.” Heavy users of the Opus model or those who run multiple instances of Claude Code simultaneously can reach these limits sooner. The company insisted that “most users won’t notice any difference, the weekly limits are designed to support typical daily use across your projects.”\nFor users that do hit the weekly usage limit, they can buy more usage “at standard API rates to continue working without interruption.” Many enterprises may already have an agreement with Anthropic around rate limits, but some organizations may be using one of the subscription tiers with Claude. This could mean companies needing to buy more usage access to run some projects.\nThe additional rate limits come as users experienced reliability issues with Claude, which Anthropic acknowledged. The company stated that it is working on addressing any remaining issues over the next few days.\nAnthropic has been making waves in the developer community, even helping push for the\nubiquity of AI coding tools\n. In June, the company\ntransformed the Claude AI\nassistant into a no-code platform for all users and launched a\nfinancial services-specific\nversion of Claude for the Enterprise tier.\nRate limits exist to ensure that model providers and chat platforms have the bandwidth to respond to user prompts. Although some companies, such as\nGoogle, have\nslowly removed limits\nfor specific models, others, including\nOpenAI\nand Anthropic, offer different tiers of rate limits to their users. The idea is that power users will pay more for the compute power they need, while users who use these platforms less will not have to.\nHowever, rate limits may limit the use cases people can perform, especially for those experimenting with\nlong-running agents\nor working on larger coding projects.\nBacklash already\nUnderstandably, many paying Claude users found the decision to throttle their usage limits distasteful, decrying that Anthropic is penalizing power users for the actions of a few who are abusing the system.\nimagine if gas stations didn't tell you how many gallons you were getting because car mileage was a trade secret and the gas station owned the car companies and you could either buy way overpriced gas per-mile or a monthly \"max gas subscription\" that turns off randomly sometimes\nhttps://t.co/eu6eFOV8OM\n— will brown (@willccbb)\nJuly 28, 2025\nAnthropic is adding weekly usage limits to Claude next month, on top of the daily usage limits. Remember, this is the period of *cheap AI*. They’re getting us all hooked as they push the limits on what they can charge.\n—\nNick Nisi @ Laracon (@nicknisi.com)\n2025-07-28T18:36:08.587Z\nAlthough other Claude users gave Anthropic the benefit of the doubt, understanding that there is little the company can do when people use the models and the Claude platform to their limits.\nLet me rephrase:\nWe’re burning more money than expected, and our shareholders want us to cut costs.\nSo, we’re changing our terms for power users… but don’t come after us, because we’ve always had a clause letting us change your usage quotas anytime.\nInstead of penalizing…\n— Guillaume (@glevd)\nJuly 28, 2025\nThe correct sensible reaction:\nThank you for limiting abusers and allocating more server space for normal users like myself ?\nBut that's not gonna get engagement, is it\n— ᛗᚨᚱᚴᚢᛋ (@guitaripod)\nJuly 28, 2025\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nIs your ai infrastructure ready for what's next?\nExplore how four enterprises built AI infrastructure that cuts costs, modernizes systems, and scales performance—fast. In this interactive experience, see what they changed, why it worked, and how you can apply it to your own strategy.\nLearn More",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-29T12:03:45.191114",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-29T12:04:04.568088",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/b7e303fa5cf8bec072118a5764eb4418.mp3",
    "audio_file": "audio/articles/b7e303fa5cf8bec072118a5764eb4418.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-07-29T12:04:56.462778",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  }
]