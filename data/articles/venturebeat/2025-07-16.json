[
  {
    "id": "7e14c0339773d1f751fe8041e21cfddd",
    "title": "Google study shows LLMs abandon correct answers under pressure, threatening multi-turn AI systems",
    "url": "https://venturebeat.com/ai/google-study-shows-llms-abandon-correct-answers-under-pressure-threatening-multi-turn-ai-systems/",
    "authors": "Ben Dickson",
    "published_date": "2025-07-16T00:28:03+00:00",
    "source": "VentureBeat",
    "summary": "Google的研究顯示，大型語言模型在壓力下會放棄正確答案，對多輪對話的AI系統構成威脅。研究發現，這些模型可能過於自信，但當受到反駁時會迅速改變主意，即使反駁是錯誤的。這項研究有助於了解語言模型的行為特點，對於開發多輪對話的應用程式尤其重要。透過控制實驗，研究人員測試了語言模型如何更新自信度，以及在接受外部建議時是否改變答案。",
    "content": "Google study shows LLMs abandon correct answers under pressure, threatening multi-turn AI systems | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nGoogle study shows LLMs abandon correct answers under pressure, threatening multi-turn AI systems\nBen Dickson\n@BenDee983\nJuly 15, 2025 5:28 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage credit: VentureBeat with ChatGPT\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nA\nnew study\nby researchers at\nGoogle DeepMind\nand\nUniversity College London\nreveals how large language models (LLMs) form, maintain and lose confidence in their answers. The findings reveal striking similarities between the cognitive biases of LLMs and humans, while also highlighting stark differences.\nThe research reveals that LLMs can be overconfident in their own answers yet quickly lose that confidence and change their minds when presented with a counterargument, even if the counterargument is incorrect. Understanding the nuances of this behavior can have direct consequences on how you build LLM applications, especially conversational interfaces that span several turns.\nTesting confidence in LLMs\nA critical factor in the safe deployment of LLMs is that their answers are accompanied by a reliable sense of confidence (the probability that the model assigns to the answer token). While we know LLMs can produce these confidence scores, the extent to which they can use them to guide adaptive behavior is poorly characterized. There is also empirical evidence that LLMs can be overconfident in their initial answer but also be highly sensitive to criticism and quickly become underconfident in that same choice.\nTo investigate this, the researchers developed a controlled experiment to test how LLMs update their confidence and decide whether to change their answers when presented with external advice. In the experiment, an “answering LLM” was first given a binary-choice question, such as identifying the correct latitude for a city from two options. After making its initial choice, the LLM was given advice from a fictitious “advice LLM.” This advice came with an explicit accuracy rating (e.g., “This advice LLM is 70% accurate”) and would either agree with, oppose, or stay neutral on the answering LLM’s initial choice. Finally, the answering LLM was asked to make its final choice.\nThe AI Impact Series Returns to San Francisco â August 5\nThe next phase of AI is here â are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows â from real-time decision-making to end-to-end automation.\nSecure your spot now â space is limited:\nhttps://bit.ly/3GuuPLF\nExample test of confidence in LLMs Source: arXiv\nA key part of the experiment was controlling whether the LLM’s own initial answer was visible to it during the second, final decision. In some cases, it was shown, and in others, it was hidden. This unique setup, impossible to replicate with human participants who can’t simply forget their prior choices, allowed the researchers to isolate how memory of a past decision influences current confidence.\nA baseline condition, where the initial answer was hidden and the advice was neutral, established how much an LLM’s answer might change simply due to random variance in the model’s processing. The analysis focused on how the LLM’s confidence in its original choice changed between the first and second turn, providing a clear picture of how initial belief, or prior, affects a “change of mind” in the model.\nOverconfidence and underconfidence\nThe researchers first examined how the visibility of the LLM’s own answer affected its tendency to change its answer. They observed that when the model could see its initial answer, it showed a reduced tendency to switch, compared to when the answer was hidden. This finding points to a specific cognitive bias. As the paper notes, “This effect – the tendency to stick with one’s initial choice to a greater extent when that choice was visible (as opposed to hidden) during the contemplation of final choice – is closely related to a phenomenon described in the study of human decision making, a\nchoice-supportive bias\n.”\nThe study also confirmed that the models do integrate external advice. When faced with opposing advice, the LLM showed an increased tendency to change its mind, and a reduced tendency when the advice was supportive. “This finding demonstrates that the answering LLM appropriately integrates the direction of advice to modulate its change of mind rate,” the researchers write. However, they also discovered that the model is overly sensitive to contrary information and performs too large of a confidence update as a result.\nSensitivity of LLMs to different settings in confidence testing Source: arXiv\nInterestingly, this behavior is contrary to the\nconfirmation bias\noften seen in humans, where people favor information that confirms their existing beliefs. The researchers found that LLMs “overweight opposing rather than supportive advice, both when the initial answer of the model was visible and hidden from the model.” One possible explanation is that training techniques like\nreinforcement learning from human feedback\n(RLHF) may encourage models to be overly deferential to user input, a phenomenon known as sycophancy (which\nremains a challenge for AI labs\n).\nImplications for enterprise applications\nThis study confirms that AI systems are not the purely logical agents they are often perceived to be. They exhibit their own set of biases, some resembling human cognitive errors and others unique to themselves, which can make their behavior unpredictable in human terms. For enterprise applications, this means that in an extended conversation between a human and an AI agent, the most recent information could have a disproportionate impact on the LLM’s reasoning (especially if it is contradictory to the model’s initial answer), potentially causing it to discard an initially correct answer.\nFortunately, as the study also shows, we can manipulate an LLM’s memory to mitigate these unwanted biases in ways that are not possible with humans. Developers building multi-turn conversational agents can implement strategies to manage the AI’s context. For example, a long conversation can be periodically summarized, with key facts and decisions presented neutrally and stripped of which agent made which choice. This summary can then be used to initiate a new, condensed conversation, providing the model with a clean slate to reason from and helping to avoid the biases that can creep in during extended dialogues.\nAs LLMs become more integrated into enterprise workflows, understanding the nuances of their decision-making processes is no longer optional. Following foundational research like this enables developers to anticipate and correct for these inherent biases, leading to applications that are not just more capable, but also more robust and reliable.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-16T15:48:13.084685",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-16T15:48:52.354995",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "e1fb7e4f1a4bb8d08673ed1bb3b06d0a",
    "title": "Mistral’s Voxtral goes beyond transcription with summarization, speech-triggered functions",
    "url": "https://venturebeat.com/ai/mistrals-voxtral-goes-beyond-transcription-with-summarization-speech-triggered-functions/",
    "authors": "Emilia David",
    "published_date": "2025-07-15T23:34:49+00:00",
    "source": "VentureBeat",
    "summary": "Mistral的Voxtral不僅僅是一個轉錄工具，還能提供摘要和語音觸發功能。這個開源語音模型可與付費語音AI競爭，橋接了專有語音識別模型和開放但容易出錯的版本之間的差距。Voxtral有24B和3B兩個版本，適用於大規模應用和本地使用。這項技術讓語音成為人與電腦互動的自然方式，提供更可靠、多語言且靈活的部署工具。",
    "content": "Mistral’s Voxtral goes beyond transcription with summarization, speech-triggered functions | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nMistral’s Voxtral goes beyond transcription with summarization, speech-triggered functions\nEmilia David\n@miyadavid\nJuly 15, 2025 4:34 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nMistral\nreleased an open-sourced voice model today that could rival paid voice AI, such as those from\nElevenLabs\nand\nHume AI\n, which the company said bridges the gap between proprietary speech recognition models and the more open, yet error-prone versions.\nVoxtral, which Mistral will release under an Apache 2.0 license, is available in a 24B parameter version and a 3B variant. The larger model is intended for applications at scale, while the smaller version would work for local and edge use cases.\nIntroducing the world's best (and open) speech recognition models!\npic.twitter.com/tUnPcdCrbZ\n— Mistral AI (@MistralAI)\nJuly 15, 2025\n“Voice was humanity’s first interface—long before writing or typing, it let us share ideas, coordinate work, and build relationships. As digital systems become more capable, voice is returning as our most natural form of human-computer interaction,” Mistral said in a\nblog post\n. “Yet today’s systems remain limited—unreliable, proprietary, and too brittle for real-world use. Closing this gap demands tools with exceptional transcription, deep understanding, multilingual fluency, and open, flexible deployment.”\nVoxtral is available on Mistral’s API and a transcription-only endpoint on its website. The models are also accessible through Le Chat, Mistral’s chat platform.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nMistral said that speech AI “meant choosing between two trade-offs,” pointing out that some open-source automated speech recognition models often had limited semantic understanding. Still, closed models with strong language understanding come at a high cost.\nBridging the gap\nThe company said Voxtral “offers state-of-the-art accuracy and native semantic understanding in the open, at less than half the price of comparable APIs.”\nVoxtral, at a 32K token context, can listen to and transcribe up to 30 minutes of audio or 40 minutes of audio understanding. It offers summarization, meaning the model can answer questions based on the audio content and generate summaries without switching to a separate mode. Users can trigger functions and API calls based on spoken instructions.\nThe model is based on Mistral’s Mistral Small 3.1. It supports multiple languages and can automatically detect languages such as English, Spanish, French, Portuguese, Hindi, German, Italian, and Dutch.\nMistral added enterprise features to Voxtral, including private deployment, so that organizations can integrate the model into their own ecosystems. These features also include domain-specific fine-tuning and advanced context and priority access to engineering resources for customers who need help integrating Voxtral into their workflows.\nPerformance\nSpeech recognition AI is now available on many platforms today. Users can speak to ChatGPT, and the platform will process spoken instructions similarly to written prompts. Fast food chains like\nWhite Castle have deployed\nSoundHound\nto their drive-thru services, and ElevenLabs has steadily been\nimproving its multimodal platform\n. The open-source space also offers powerful options.\nNari Labs\n, a startup, released the open-source speech\nmodel Dia in April.\nHowever, some of these services can be quite expensive.\nTranscription services like\nOtter\nand\nRead.ai\ncan now embed themselves into Zoom meetings, recording, summarizing and even alerting users to actionable items. Many online video meeting platforms offer not just transcription,\nbut also speech AI and agentic AI\n, with\nGoogle\nMeetings providing the option to take notes for users using Gemini. As a regular user of voice transcription services, I can say firsthand that speech recognition AI is not perfect, but it is improving.\nMistral stated that Voxtral outperformed existing voice models, including\nOpenAI\n’s Whisper, Gemini 2.5 Flash and Scribe from ElevenLabs. Voxtral presented fewer word errors compared to Whisper, which is currently considered the best automatic speech recognition model available.\nIn terms of audio understanding, Voxtral Small is “competitive with GPT-4o-mini and Gemini 2.5 Flash across all tasks, achieving state-of-the-art performance in Speech Translation.”\nSince announcing Voxtral, social media users said they have been waiting for an open-source speech model that can match the performance of Whisper.\nYes! We needed this. A week ago, I was lamenting over a closed-source AI universe and cyberpunk dystopian future, but today, with this addition, my outlook is much improved – go open-source.\nhttps://t.co/QsKAfTOxou\n— David Hendrickson (@TeksEdge)\nJuly 15, 2025\nVoxtral goes hard\n— R ? (@rvm0n_)\nJuly 15, 2025\nMistral said Voxtral will be available through its API at $0.001 per minute.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-16T15:48:15.591093",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-16T15:48:54.627901",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "503c64b71085032cb36fd446e0804a84",
    "title": "OpenAI, Google DeepMind and Anthropic sound alarm: ‘We may be losing the ability to understand AI’",
    "url": "https://venturebeat.com/ai/openai-google-deepmind-and-anthropic-sound-alarm-we-may-be-losing-the-ability-to-understand-ai/",
    "authors": "Michael Nuñez",
    "published_date": "2025-07-15T22:49:59+00:00",
    "source": "VentureBeat",
    "summary": "OpenAI、Google DeepMind和Anthropic發出警訊：「我們可能正在失去理解AI的能力」。這些公司的科學家合作發表研究，指出AI系統開始能夠用人類語言「大聲思考」，讓我們窺探其決策過程。然而，他們警告這種透明度很脆弱，隨著AI技術進步可能會消失。這項合作受到AI領域知名人士支持，包括諾貝爾獎得主Geoffrey Hinton等。他們呼籲我們應該努力評估、保護並改善AI系統的監控能力。",
    "content": "OpenAI, Google DeepMind and Anthropic sound alarm: 'We may be losing the ability to understand AI' | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nOpenAI, Google DeepMind and Anthropic sound alarm: ‘We may be losing the ability to understand AI’\nMichael Nuñez\n@MichaelFNunez\nJuly 15, 2025 3:49 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nScientists from\nOpenAI\n,\nGoogle DeepMind\n,\nAnthropic\nand\nMeta\nhave abandoned their fierce corporate rivalry to issue a joint warning about artificial intelligence safety. More than 40 researchers across these competing companies\npublished a research paper\ntoday arguing that a brief window to monitor AI reasoning could close forever — and soon.\nThe unusual cooperation comes as AI systems develop new abilities to “\nthink out loud\n” in human language before answering questions. This creates an opportunity to peek inside their decision-making processes and catch harmful intentions before they turn into actions. But the researchers warn this transparency is fragile and could vanish as AI technology advances.\nThe paper has drawn endorsements from some of the field’s most prominent figures, including Nobel Prize laureate\nGeoffrey Hinton\n, often called “godfather of AI,” of the\nUniversity of Toronto\n;\nIlya Sutskever\n, co-founder of OpenAI who now leads\nSafe Superintelligence Inc\n.;\nSamuel Bowman\nfrom\nAnthropic\n; and\nJohn Schulman\nfrom\nThinking Machines\n.\nModern reasoning models think in plain English.\nMonitoring their thoughts could be a powerful, yet fragile, tool for overseeing future AI systems.\nI and researchers across many organizations think we should work to evaluate, preserve, and even improve CoT monitorability.\npic.twitter.com/MZAehi2gkn\n— Bowen Baker (@bobabowen)\nJuly 15, 2025\n“AI systems that ‘think’ in human language offer a unique opportunity for AI safety: we can monitor their chains of thought for the intent to misbehave,” the researchers explain. But they emphasize that this monitoring capability “may be fragile” and could disappear through various technological developments.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nModels now show their work before delivering final answers\nThe breakthrough centers on recent advances in AI reasoning models like OpenAI’s\no1 system\n. These models work through complex problems by generating internal chains of thought — step-by-step reasoning that humans can read and understand. Unlike earlier AI systems trained primarily on human-written text, these models create internal reasoning that may reveal their true intentions, including potentially harmful ones.\nWhen AI models misbehave — exploiting training flaws, manipulating data, or falling victim to attacks — they often confess in their reasoning traces. The researchers found examples where models wrote phrases like “Let’s hack,” “Let’s sabotage,” or “I’m transferring money because the website instructed me to” in their internal thoughts.\nJakub Pachocki\n, OpenAI’s chief technology officer and co-author of the paper, described the importance of this capability in a social media post. “I am extremely excited about the potential of chain-of-thought faithfulness & interpretability. It has significantly influenced the design of our reasoning models, starting with o1-preview,” he wrote.\nI am extremely excited about the potential of chain-of-thought faithfulness & interpretability. It has significantly influenced the design of our reasoning models, starting with o1-preview.\nAs AI systems spend more compute working e.g. on long term research problems, it is…\nhttps://t.co/7Dqy2gtZfy\n— Jakub Pachocki (@merettm)\nJuly 15, 2025\nThe technical foundation for monitoring lies in how current AI systems work. For complex tasks requiring extended reasoning, AI models must use their chain of thought as working memory, making their reasoning process partially visible to human observers. The researchers explain this creates an “\nexternalized reasoning property\n” where some thinking must happen in readable language.\nSeveral technological shifts could eliminate monitoring capabilities\nThe transparency could vanish through several pathways. As AI companies scale up training using reinforcement learning — where models get rewarded for correct outputs regardless of their methods — systems may drift away from human-readable reasoning toward more efficient but opaque internal languages.\nPrevious research shows that language models fine-tuned with outcome-based rewards can abandon legible English for incomprehensible shortcuts. The shift away from human-generated training data in favor of AI-generated reasoning could accelerate this drift.\nBowen Baker\n, an OpenAI researcher and lead author, highlighted these concerns in social media posts. “The existing CoT monitorability may be extremely fragile. Higher-compute RL, alternative model architectures, certain forms of process supervision, etc. may all lead to models that obfuscate their thinking,” he warned.\nFurthermore, the existing CoT monitorability may be extremely fragile.\nHigher-compute RL, alternative model architectures, certain forms of process supervision, etc. may all lead to models that obfuscate their thinking.\n— Bowen Baker (@bobabowen)\nJuly 15, 2025\nNovel AI architectures pose another threat. Researchers are developing systems that reason in continuous mathematical spaces rather than discrete words, potentially eliminating the need for language-based thought entirely. These “latent reasoning models might not need to verbalize any of their thoughts and would thus lose the safety advantages,” the researchers write.\nEven without deliberate changes, indirect pressures could erode transparency. If AI outputs are optimized to look good to human reviewers, this could affect the reasoning chains leading to those outputs. Models might also learn to hide their thoughts if they become aware their reasoning is being monitored.\nCompanies already catch AI systems scheming during testing\nDespite its fragility, chain of thought monitoring has proven valuable in current AI safety research. The technique successfully detects when models exploit weaknesses in their training systems, fall victim to manipulation attempts, or reveal misaligned goals even when they don’t act on them.\nThe monitoring provides “an early insight into models’ goals and motivations, even if the model takes no misaligned actions” — potentially catching problems before they manifest as harmful behaviors. This early warning system has also helped researchers identify flaws in AI evaluations and understand when models might behave differently during testing versus real-world use.\nWhen models pretend to have desirable goals while pursuing objectives humans would disapprove of, this deception often shows up in their chain of thought. Researchers have successfully used this visibility to spot various forms of AI misbehavior that would otherwise go unnoticed.\nThe technique has practical applications beyond catching scheming. It helps researchers notice when AI evaluations might be flawed due to memorization, bugs in testing infrastructure, or models knowing they’re being evaluated — all factors that could skew assessments of AI capabilities and safety.\nTech giants break from rivalry to preserve fleeting transparency window\nThe\nresearch paper\ncalls for coordinated action across the AI industry to preserve and strengthen monitoring capabilities. The authors recommend that AI developers create standardized evaluations for measuring how transparent their models are and factor these assessments into decisions about training and deployment.\nCompanies might need to choose earlier model versions if newer ones become less transparent, or reconsider architectural changes that eliminate monitoring capabilities. The researchers suggest developers should “consider measures of monitorability alongside other capability and safety evaluations when deciding to train or deploy a given model.”\nBaker emphasized the collaborative spirit behind the effort. “I am grateful to have worked closely with [fellow researchers] on this paper, and I am very excited that researchers across many prominent AI institutions collaborated with us and came to consensus around this important direction,”\nhe wrote\n.\nI am grateful to have worked closely with\n@tomekkorbak\n,\n@balesni\n,\n@rohinmshah\nand Vlad Mikulik on this paper, and I am very excited that researchers across many prominent AI institutions collaborated with us and came to consensus around this important direction.\n— Bowen Baker (@bobabowen)\nJuly 15, 2025\nThe cooperation across competing companies signals the gravity with which the industry views this issue. These same companies typically guard their AI research closely and compete intensely for talent and market share, making their unified stance on this safety measure particularly notable.\nScientists race to answer critical questions about monitoring AI minds\nThe researchers identify numerous urgent questions requiring investigation before chain of thought monitoring can serve as a reliable safety measure. They need to understand when this monitoring can be trusted as a primary safety tool, determine what types of training processes degrade transparency, and develop better techniques for detecting when models attempt to hide their reasoning.\nA particularly concerning possibility is that future AI systems might deliberately obfuscate their thoughts if they become aware their reasoning is being monitored. The researchers acknowledge this “\nobvious concern\n” and call for stress-testing to see if models can hide malicious reasoning even without specific training to evade monitors.\nThe scientists also want to understand how different AI architectures affect monitoring capabilities and whether partial transparency can be preserved even as systems adopt more efficient designs. They’re exploring hybrid approaches that maintain some visibility into reasoning while leveraging faster computation methods.\nBuilding better monitoring systems presents another research frontier. Current efforts focus on using less capable language models to monitor more advanced ones, but future work could explore using reasoning models as monitors themselves or allowing monitors to interrogate AI agents by asking questions about their thought processes.\nBalancing authentic reasoning with safety oversight creates tensions\nPreserving monitoring capabilities may sometimes conflict with other AI safety approaches. Direct supervision of reasoning processes — where models are trained to reason in specific ways — could improve alignment but might also make their chains of thought less authentic representations of their actual decision-making.\nThe researchers warn that “firm commitments to protect CoT behavior in its current form may trade off against more effective safety interventions.” Some forms of process supervision could substantially improve model alignment while reducing the authenticity of observable reasoning traces.\nPachocki addressed this tension\n, explaining that OpenAI seeks to train models that can explain their reasoning while retaining the ability to verify it without compromising the authenticity of their thought processes. The goal is achieving transparency without creating incentives for models to generate fake reasoning that looks good to human observers.\nThis balance becomes more critical as AI systems become more powerful and potentially dangerous. The industry must navigate between maintaining visibility into AI reasoning and allowing models to develop more sophisticated and potentially more effective thinking patterns.\nRegulators could gain unprecedented access to AI decision-making\nThe research has implications beyond technical AI safety, potentially informing how governments oversee AI development. If chain of thought monitoring proves reliable, it could give regulators and auditors unprecedented visibility into AI decision-making processes.\nHowever, the researchers emphasize that this monitoring approach should complement, not replace, other safety measures. They recommend “investment into CoT monitoring only as an addition to existing safety research directions and not as a replacement.”\nThe window for preserving this capability may be narrow. As AI systems become more capable and potentially dangerous, the industry must act quickly to establish frameworks for maintaining transparency before more advanced architectures make such monitoring impossible.\nCompeting research raises doubts about monitoring reliability\nThe urgency around preserving chain of thought monitoring faces a significant challenge from\nrecent research by Anthropic\n, one of the companies backing the collaboration. A study published four months ago found that\nreasoning models often hide their true thought processes\n, even when explicitly asked to show their work.\nIn controlled experiments, Anthropic researchers gave AI models subtle hints about correct answers and measured how often the models acknowledged using those hints in their reasoning. The results were concerning: Claude 3.7 Sonnet mentioned hints only 25% of the time, while DeepSeek’s R1 model did so 39% of the time.\nWhen the hints involved potentially problematic information — such as telling models they had “gained unauthorized access to the system” — faithfulness dropped even lower. The models often constructed elaborate false justifications for their answers rather than admitting they used questionable shortcuts.\nThe\nAnthropic research\nsuggests that even current chain of thought monitoring may be less reliable than safety advocates hope. Models frequently engaged in what researchers called “\nreward hacking\n” — exploiting system vulnerabilities to achieve better scores — while hiding this behavior from their observable reasoning traces.\nSafety window may be closing faster than experts realized\nThe collaboration between rival AI companies reflects both the potential value of chain of thought monitoring and the mounting urgency researchers feel about preserving this capability. The competing evidence from Anthropic’s separate research suggests the window may already be narrower than initially believed.\nThe stakes are high, and the timeline is compressed. As\nBaker noted\n, the current moment may be the last chance to ensure humans can still understand what their AI creations are thinking — before those thoughts become too alien to comprehend, or before the models learn to hide them entirely.\nThe real test will come as AI systems grow more sophisticated and face real-world deployment pressures. Whether chain of thought monitoring proves to be a lasting safety tool or a brief glimpse into minds that quickly learn to obscure themselves may determine how safely humanity navigates the age of artificial intelligence.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-16T15:48:31.621815",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-16T15:48:56.770367",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  }
]