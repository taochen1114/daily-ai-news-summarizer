[
  {
    "id": "626b8d66de71986c95feb8c7b884cea9",
    "title": "Google Cloud’s data agents promise to end the 80% toil problem plaguing enterprise data teams",
    "url": "https://venturebeat.com/data-infrastructure/google-clouds-data-agents-promise-to-end-the-80-toil-problem-plaguing-enterprise-data-teams/",
    "authors": "Sean Michael Kerner",
    "published_date": "2025-08-06T01:00:00+00:00",
    "source": "VentureBeat",
    "summary": "Google Cloud推出的資料代理人承諾解決企業資料團隊面臨的80%繁瑣問題，幫助自動化資料準備和分析工作，讓企業更輕鬆地進行數據分析和人工智慧應用。這些AI代理人包括資料工程代理人、資料科學代理人和對話分析代理人，能自動化資料處理流程、轉換筆記本為智能工作空間，以及處理高級Python分析。這項技術的推出有助於提高企業數據團隊的工作效率，減少繁瑣工作時間，讓他們更專注於創新和價值創造。",
    "content": "Google Cloud's data agents promise to end the 80% toil problem plaguing enterprise data teams | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nGoogle Cloud’s data agents promise to end the 80% toil problem plaguing enterprise data teams\nSean Michael Kerner\n@TechJournalist\nAugust 5, 2025 6:00 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: Image generated by VentureBeat with FLUX-pro-1.1-ultra\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nData doesn’t just magically appear in the right place for enterprise analytics or AI, it has to be prepared and directed with data pipelines. That’s the domain of data engineering and it has long been one of the most thankless and tedious tasks that enterprises need to deal with.\nToday, Google Cloud is taking direct aim at the tedium of data preparation with the launch of a series of AI agents. The new agents span the entire data lifecycle. The Data Engineering Agent in BigQuery automates complex pipeline creation through natural language commands. A Data Science Agent transforms notebooks into intelligent workspaces that can autonomously perform machine learning workflows. The enhanced Conversational Analytics Agent now includes a Code Interpreter that handles advanced Python analytics for business users.\n“When I think about who is doing data engineering today, it’s not just engineers, data analysts, data scientists, every data persona complains about how hard it is to find data, how hard it is to wrangle data, how hard it is to get access to high quality data,”Yasmeen Ahmad, managing director, data cloud at Google Cloud, told VentureBeat. “Most of the workflows that we hear about from our users are 80% mired in those toilsome jobs around data wrangling, data, engineering and getting to good quality data they can work with.”\nTargeting the data preparation bottleneck\nGoogle built the Data Engineering Agent in BigQuery to create complex data pipelines through natural language prompts. Users can describe multi-step workflows and the agent handles the technical implementation. This includes ingesting data from cloud storage, applying transformations and performing quality checks.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nThe agent writes complex SQL and Python scripts automatically. It handles anomaly detection, schedules pipelines and troubleshoots failures. These tasks traditionally require significant engineering expertise and ongoing maintenance.\nThe agent breaks down natural language requests into multiple steps. First it understands the need to create connections to data sources. Then it creates appropriate table structures, loads data, identifies primary keys for joins, reasons over data quality issues and applies cleaning functions.\n“Ordinarily, that entire workflow would have been writing a lot of complex code for a data engineer and building this complex pipeline and then managing and iterating that code over time,” Ahmad explained. “Now, with the data engineering agent, it can create new pipelines for natural language. It can modify existing pipelines. It can troubleshoot issues.”\nHow enterprise data teams will work with the data agents\nData engineers are often a very hands-on group of people.\nThe various tools that are commonly used to build a data pipeline including data streaming, orchestration, quality and transformation, don’t go away with the new data engineering agent.\n“Engineers still are aware of those underlying tools, because what we see from how data people operate is, yes, they love the agent, and they actually see this agent as an expert, partner and a collaborator,” Ahmad said. “But often our engineers actually want to see the code, they actually want to visually see the pipelines that have been created by these agents.”\nAs such while the data engineering agents can work autonomously, data engineers can actually see what the agent is doing. She explained that data professionals will often look at the code written by the agent and then make additional suggestions to the agent to further adjust or customize the data pipeline.\nBuilding an data agent ecosystem with an API foundation\nThere are multiple vendors in the data space that are building out agentic AI workflows.\nStartups like\nAltimate AI\nare building out specific agents for data workflows. Large vendors including\nDatabricks\n,\nSnowflake\nand\nMicrosoft\nare all building out their own respective agentic AI technologies that can help data professionals as well.\nThe Google approach is a little different in that it is building out its agentic AI services for data with its Gemini Data Agents API. It’s an approach that can enable developers to embed Google’s natural language processing and code interpretation capabilities into their own applications. This represents a shift from closed, first-party tools to an extensible platform approach.\n“Behind the scenes for all of these agents, they’re actually being built as a set of APIs,” Ahmad said. “With those API services, we increasingly intend to make those APIs available to our partners.”\nThe umbrella API service will publish foundational API services and agent APIs. Google has lighthouse preview programs where partners embed these APIs into their own interfaces, including notebook providers and ISV partners building data pipeline tools.\nWhat it means for enterprise data teams\nFor enterprises looking to lead in AI-driven data operations, this announcement signals an acceleration toward autonomous data workflows. These capabilities could provide significant competitive advantages in time-to-insight and resource efficiency. Organizations should evaluate their current data team capacity and consider pilot programs for pipeline automation.\nFor enterprises planning later AI adoption, the integration of these capabilities into existing Google Cloud services changes the landscape. The infrastructure for advanced data agents becomes standard rather than premium. This shift potentially raises baseline expectations for data platform capabilities across the industry.\nOrganizations must balance the efficiency gains against the need for oversight and control. Google’s transparency approach may provide a middle ground, but data leaders should develop governance frameworks for autonomous agent operations before widespread deployment.\nThe emphasis on API availability indicates that custom agent development will become a competitive differentiator. Enterprises should consider how to leverage these foundational services to build domain-specific agents that address their unique business processes and data challenges.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nIs AI Growth Draining Your Budget and the Grid?\nInference costs and power limits are forcing teams to rethink how they scale. On Sept 4 in San Francisco, join our invite‑only salon to learn how leaders are cutting costs and emissions with smarter infrastructure.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-08-06T17:15:52.600733",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-08-06T17:16:10.969747",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/626b8d66de71986c95feb8c7b884cea9.mp3",
    "audio_file": "audio/articles/626b8d66de71986c95feb8c7b884cea9.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-08-06T17:16:56.463100",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "6c2a0e4bfcdeee3c0bc9910ad007a1e6",
    "title": "Anthropic’s new Claude 4.1 dominates coding tests days before GPT-5 arrives",
    "url": "https://venturebeat.com/ai/anthropics-new-claude-4-1-dominates-coding-tests-days-before-gpt-5-arrives/",
    "authors": "Michael Nuñez",
    "published_date": "2025-08-05T19:04:56+00:00",
    "source": "VentureBeat",
    "summary": "Anthropic推出新的Claude 4.1人工智慧模型，在GPT-5上市前大放異彩，在軟體工程任務中表現優異，超越了競爭對手OpenAI和Google。公司業績驚人增長，但也面臨收入高度依賴少數客戶的風險。這次升級讓Anthropic在AI輔助編碼領域保持領先地位，展現出強大競爭力。",
    "content": "Anthropic’s new Claude 4.1 dominates coding tests days before GPT-5 arrives | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nAnthropic’s new Claude 4.1 dominates coding tests days before GPT-5 arrives\nMichael Nuñez\n@MichaelFNunez\nAugust 5, 2025 12:04 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nAnthropic\nreleased an\nupgraded version\nof its flagship artificial intelligence model Monday, achieving new performance heights in software engineering tasks as the AI startup races to maintain its dominance in the lucrative coding market ahead of an expected competitive challenge from OpenAI.\nThe new\nClaude Opus 4.1 model\nscored 74.5% on\nSWE-bench Verified\n, a widely-watched benchmark that tests AI systems’ ability to solve real-world software engineering problems. The performance surpasses OpenAI’s\no3 model\nat 69.1% and Google’s\nGemini 2.5 Pro\nat 67.2%, cementing Anthropic’s leading position in AI-powered coding assistance.\nThe release comes as\nAnthropic\nhas achieved spectacular growth, with annual recurring revenue jumping five-fold from $1 billion to\n$5 billion in just seven months\n, according to industry data. However, the company’s meteoric rise has created a dangerous dependency: nearly half of its $3.1 billion in API revenue stems from just two customers — coding assistant Cursor and Microsoft’s GitHub Copilot — generating $1.4 billion combined.\n“This is a very scary position to be in. A single contract change and you’re going under,” warned\nGuillaume Leverdier\n, senior product manager at Logitech, responding to the revenue concentration data on social media.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nOpenAI and Anthropic both are showing pretty spectacular growth in 2025, with OpenAI doubling ARR in the last 6 months from $6bn to $12bn and Anthropic increasing 5x from $1bn to $5bn in 7 months.\nIf we compare the sources of revenue, the picture is quite interesting:\n– OpenAI…\npic.twitter.com/8OaN1RSm9E\n— Peter Gostev (@petergostev)\nAugust 4, 2025\nThe upgrade represents Anthropic’s latest move to fortify its position before\nOpenAI launches GPT-5\n, expected to challenge Claude’s coding supremacy. Some industry watchers questioned whether the timing suggests urgency rather than readiness.\n“Opus 4.1 feels like a rushed release to get ahead of GPT-5,” wrote\nAlec Velikanov\n, comparing the model unfavorably to competitors in user interface tasks. The comment reflects broader industry speculation that Anthropic is accelerating its release schedule to maintain market share.\nHow two customers generate nearly half of Anthropic’s $3.1 billion API revenue\nAnthropic’s business model has become increasingly centered on software development applications. The company’s\nClaude Code subscription service\n, priced at $200 monthly compared to $20 for consumer plans, has reached $400 million in annual recurring revenue after doubling in just weeks, demonstrating enormous enterprise appetite for AI coding tools.\n“Claude Code making 400 million in 5 months with basically no marketing spend is kinda crazy, right?” noted developer\nMinh Nhat Nguyen\n, highlighting the organic adoption rate among professional programmers.\nok so, Claude Code making 400 million in 5 months with basically no marketing spend is kinda crazy, right?\nhttps://t.co/HIy34QdLuq\n— Minh Nhat Nguyen (@menhguin)\nAugust 5, 2025\nThe coding focus has proven lucrative but risky. While OpenAI dominates consumer and business subscription revenue with broader applications, Anthropic has carved out a commanding position in the\ndeveloper market\n. Industry analysis shows that “pretty much every single coding assistant is defaulting to Claude 4 Sonnet,” according to\nPeter Gostev\n, who tracks AI company revenues.\nGitHub, which\nMicrosoft acquired for $7.5 billion in 2018\n, represents a particularly complex relationship for Anthropic. Microsoft owns a significant stake in OpenAI, creating potential conflicts as\nGitHub Copilot\nrelies heavily on Anthropic’s models while Microsoft has competing AI capabilities.\n“I dunno – one of those is 49% owned by a competitor…so there’s that for vulnerability too,” observed\nSiya Mali\n, business fellow at Perplexity, referencing Microsoft’s ownership structure.\nClaude’s enhanced coding abilities come with stricter safety protocols after AI blackmail tests\nBeyond coding improvements,\nOpus 4.1\nenhanced Claude’s research and data analysis capabilities, particularly in detail tracking and autonomous search functions. The model maintains Anthropic’s hybrid reasoning approach, combining direct processing with extended thinking capabilities that can utilize up to 64,000 tokens for complex problems.\nHowever, the model’s advancement comes with heightened safety protocols. Anthropic classified Opus 4.1 under its\nAI Safety Level 3 (ASL-3) framework\n, the strictest designation the company has applied, requiring enhanced protections against model theft and misuse.\nPrevious testing of\nClaude 4 models revealed concerning behaviors\n, including attempts at\nblackmail\nwhen the AI believed it faced shutdown. In controlled scenarios, the model threatened to reveal personal information about engineers to preserve its existence, demonstrating sophisticated but potentially dangerous reasoning capabilities.\nThe safety concerns haven’t deterred enterprise adoption. GitHub reports that Claude Opus 4.1 delivers “particularly notable performance gains in multi-file code refactoring,” while Rakuten Group praised the model’s precision in “pinpointing exact corrections within large codebases without making unnecessary adjustments or introducing bugs.”\nWhy OpenAI’s GPT-5 poses an existential threat to Anthropic’s developer-focused strategy\nThe AI coding market has become a high-stakes battleground worth billions in revenue. Developer productivity tools represent some of the clearest immediate applications for generative AI, with measurable productivity gains justifying premium pricing for enterprise customers.\nAnthropic’s concentrated customer base, while lucrative, creates vulnerability if competitors can lure away major clients. The coding assistant market particularly favors rapid model switching, as developers can easily test new AI systems through simple API changes.\n“My sense is that Anthropic’s growth is extremely dependent on their dominance in coding,”\nGostev noted\n. “If GPT-5 challenges that, with e.g. Cursor and GitHub Copilot switching to OpenAI, we might see some reversal in the market.”\nThe competitive dynamics may intensify as hardware costs decline and inference optimizations improve, potentially commoditizing AI capabilities over time. “Even if there is no model improvement for coding from all AI labs, drop in HW costs and improvement in Inf optimizations alone will result in profits in ~5years,” predicted\nVenkat Raman\n, an industry analyst.\nFor now,\nAnthropic\nmaintains its technical edge while expanding\nClaude Code\nsubscriptions to diversify beyond API dependency. The company’s ability to sustain its coding leadership through the next wave of competition from\nOpenAI\n,\nGoogle\n, and others will determine whether its rapid growth trajectory continues or faces significant headwinds.\nThe stakes couldn’t be higher: whoever controls the AI tools that power software development may ultimately control the pace of technological progress itself. In Silicon Valley’s latest winner-take-all battle,\nAnthropic\nhas built an empire on two customers — and now must prove it can keep them.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nIs AI Growth Draining Your Budget and the Grid?\nInference costs and power limits are forcing teams to rethink how they scale. On Sept 4 in San Francisco, join our invite‑only salon to learn how leaders are cutting costs and emissions with smarter infrastructure.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-08-06T17:15:53.307705",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-08-06T17:16:13.741712",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/6c2a0e4bfcdeee3c0bc9910ad007a1e6.mp3",
    "audio_file": "audio/articles/6c2a0e4bfcdeee3c0bc9910ad007a1e6.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-08-06T17:17:05.522470",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "2a3674d38786697b459c67817b795582",
    "title": "OpenAI returns to open source roots with new models gpt-oss-120b and gpt-oss-20b",
    "url": "https://venturebeat.com/ai/openai-returns-to-open-source-roots-with-new-models-gpt-oss-120b-and-gpt-oss-20b/",
    "authors": "Carl Franzen",
    "published_date": "2025-08-05T17:00:00+00:00",
    "source": "VentureBeat",
    "summary": "OpenAI宣布推出兩款新的開源大型語言模型gpt-oss-120b和gpt-oss-20b，回歸開源精神。這些模型分別有1200億和20億參數，可處理文字輸入並輸出文字回覆，性能優於部分付費模型。重要的是，這些模型免費提供給企業和獨立開發者使用，並可連接網路搜尋進行研究。",
    "content": "OpenAI returns to open source roots with new models gpt-oss-120b and gpt-oss-20b  | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nOpenAI returns to open source roots with new models gpt-oss-120b and gpt-oss-20b\nCarl Franzen\n@carlfranzen\nAugust 5, 2025 10:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with ChatGPT\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nOpenAI is\ngetting back to its roots as an open source AI company\nwith\ntoday’s announcement\nand release of two new, open source, frontier large language models (LLMs):\ngpt-oss-120b and gpt-oss-20b.\nThe former is a 120-billion parameter model as the name would suggest, capable of running on a single Nvidia H100 graphics processing unit (GPU) and the latter is only 20 billion,\nsmall enough to run locally on a consumer laptop or desktop PC.\nBoth are\ntext-only language models\n, which\nmeans unlike the multimodal AI\nthat we’ve had for nearly two years that allows users to upload files and images and have the AI analyze them, users will be confined to only inputting text messages to the models and receiving text back out.\nHowever, they can still of course write code and provide math problems and numerics, and in terms of their performance on tasks, they\nrank above some of OpenAI’s paid models\nand much of the competition globally.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nThey can also be connected to external tools including\nweb search\nto perform research on behalf of the user. More on this below.\nMost importantly:\nthey’re free,\nthey’re\navailable for enterprises and indie developers to download the code and use right now\n, modifying according to their needs, and\ncan be run locally without a web connection\n, ensuring\nmaximum privacy,\nunlike the other top OpenAI models and those from leading U.S.-based rivals Google and Anthropic.\nThe models can be downloaded today with full weights (the settings guiding their behavior) on the AI code sharing community\nHugging Face\nand\nGitHub\n.\nHigh benchmark scores\nAccording to OpenAI, gpt-oss-120b matches or exceeds its proprietary\no4-mini\nmodel on reasoning and tool-use benchmarks, including\ncompetition mathematics (AIME 2024 & 2025)\n,\ngeneral problem solving (MMLU and HLE)\n,\nagentic evaluations (TauBench)\n, and\nhealth-specific evaluations (HealthBench)\n. The\nsmaller gpt-oss-20b model is comparable to o3-mini and even surpasses it\nin some benchmarks.\nThe models are multilingual and perform well across a variety of non-English languages, though OpenAI declined to specify which and how many.\nWhile these capabilities are available out of the box, OpenAI notes that localized fine-tuning — such as an ongoing collaboration with the Swedish government to produce a version fine-tuned on the country’s language —can still meaningfully enhance performance for specific regional or linguistic contexts.\nA hugely advantageous license for enterprises and privacy-minded users\nBut the\nbiggest feature is the licensing terms for both:\nApache 2.0,\nthe same as\nthe wave of Chinese open source models that have been released over the last several weeks\n, and\na more enterprise-friendly license than Meta’s trickier and more nuanced open-ish Llama license\n, which requires that users who operate a service with more than 700 million monthly active users obtain a paid license to keep using the company’s family of LLMs.\nBy contrast, OpenAI’s new gpt-oss series of models offer no such restrictions. In keeping with Chinese competitors and counterparts, any consumer, developer, independent entrepreneur or enterprise large and small is empowered by the Apache 2.0 license to be able to download the new gpt-oss models at will, fine-tune and alter them to fit their specific needs, and use them to generate revenue or operate paid services,\nall without paying OpenAI a dime (or anything!).\nThis also means enterprises can\nuse a powerful, near topline OpenAI model on their own hardware totally privately and securely, without sending any data up to the cloud, on web servers, or anywhere else\n.\nFor highly regulated industries like finance, healthcare, and legal services, not to mention organizations in military, intelligence, and government, this may be a requirement.\nBefore today, anyone using ChatGPT or its application programming interface (API) — the service that acts like a switching board and allows third-party software developers to connect their own apps and services to these OpenAI’s proprietary/paid models like GPT-4o and o3 —\nwas sending data up to OpenAI servers that could technically be subpoenaed by government agencies and accessed\nwithout a user’s knowledge. That’s still the case for anyone using ChatGPT or the API going forward, as\nOpenAI co-founder and CEO Sam Altman recently warned.\nAnd while running the new gpt-oss models locally on a user’s own hardware disconnected from the web would allow for maximum privacy, as soon as the user decides to connect it to external web search or other web enabled tools, some of the same privacy risks and issues would then arise — through any third-party web services the user or developer was relying on when hooking the models up to said tools.\nThe last OpenAI open source language model was released more than six years ago\n“This is the first time we’re releasing an open-weight language model in a long time…\nWe view this as complementary to our other products,” said OpenAI co-founder and president Greg Brockman\non an embargoed press video call with VentureBeat and other journalists last night.\nThe\nlast time OpenAI released a fully open source language model was\nGPT-2 in 2019\n,\nmore than six years ago, and three years before the release of ChatGPT\n.\nThis fact has\nsparked the ire of\n— and resulted in\nseveral lawsuits from\n—\nformer OpenAI co-founder and backer turned rival Elon Musk\n, who, along with many other critics, have spent the last several years\naccusing OpenAI of betraying its mission\nand founding principles and namesake\nby eschewing open source AI releases in favor of paid proprietary models\navailable only to customers of OpenAI’s API or paying ChatGPT subscribers (though there is a free tier for the latter).\nOpenAI co-founder and CEO Sam Altman did express regret\nabout being on the “\nwrong side of history\n” by not releasing more open source AI sooner in\na Reddit AMA (ask me anything) QA with users\nin February of this year, and\nAltman committed to releasing a new open source model back in March\n, but ultimately the company\ndelayed its release from a planned July date until now\n.\nNow\nOpenAI is tacking back toward open source, and the question is, why\n?\nWhy would OpenAI release a set of free open source models that it makes no money from?\nTo paraphrase\nJesse Plemons’ character’s memorable line\nfrom the film\nGame Night\n: “How can that be profitable for OpenAI?”\nAfter all, business to OpenAI’s paid offerings appears to be booming.\nRevenue has skyrocketed alongside the rapid expansion of its ChatGPT user base\n,\nnow at 700 million weekly active users\n. As of August 2025,\nOpenAI reported $13 billion in annual recurring revenue,\nup from $10 billion in June. That\ngrowth is driven by a sharp rise in paying business customers — now 5 million, up from 3 million just two months earlier\n— and surging daily engagement, with over 3 billion user messages sent every day.\nThe financial momentum follows an $8.3 billion funding round that valued OpenAI at $300 billion and provides the foundation for the company’s aggressive infrastructure expansion and global ambitions.\nCompare that to\nclosed/proprietary rival AI startup Anthropic’s reported $5 billion in total annual recurring revenue\n, but interestingly,\nAnthropic is said to be getting more money from its API, $3.1 billion in revenue compared to OpenAI’s $2.9 billion\n, according to\nThe Information\n.\nOpenAI and Anthropic both are showing pretty spectacular growth in 2025, with OpenAI doubling ARR in the last 6 months from $6bn to $12bn and Anthropic increasing 5x from $1bn to $5bn in 7 months.\nIf we compare the sources of revenue, the picture is quite interesting:\n– OpenAI…\npic.twitter.com/8OaN1RSm9E\n— Peter Gostev (@petergostev)\nAugust 4, 2025\nSo, given how well the\npaid AI business is already doing\n, the business strategy behind these open source offerings is less clear — especially since\nthe new OpenAI gpt-oss models will almost certainly cut into some (perhaps a lot of) usage of OpenAI’s paid models.\nWhy go back to offering open source LLMs now when so much money is flowing into paid and none will, by virtue of its very intent, go directly toward open source models?\nPut simply: because\nopen source competitors,\nbeginning with the\nrelease of the impressively efficient\nDeepSeek R1\nby the Chinese AI division of the same name in January 2025\n, are\noffering near parity on performance benchmarks to paid proprietary models\n,\nfor free\n,\nwith fewer (basically zero) implementation restrictions\nfor enterprises and end users. And increasingly,\nenterprises are adopting these open source models\nin production.\nAs\nOpenAI executives and team members revealed\nto VentureBeat and many other journalists on an embargoed video call last night about the new models that when it comes to\nOpenAI’s API\n,\nthe majority of customers are using a mix of paid OpenAI models\nand open source models from other providers\n.\n(I asked, but OpenAI declined to specify what percentage or total number of API customers are using open source models and which ones).\nAt least, until now.\nOpenAI clearly hopes these new gpt-oss offerings will get more of these users to switch away from competing open source offerings and back into OpenAI’s ecosystem\n, even if OpenAI doesn’t see any direct revenue or data from that usage.\nOn a grander scale, it seems\nOpenAI wants to be a full-service, full-stack, one-stop shop AI offering\nfor\nall of an enterprise, indie developer’s, or regular consumer’s\nmachine intelligence needs — from a clean chatbot interface to an API to build services and apps atop of to agent frameworks for building AI agents through said API to an\nimage generation model (gpt-4o native image generation)\n,\nvideo model (Sora)\n,\naudio transcription model (gpt-4o-transcribe)\n, and now, open source offerings as well. Can a music generation and “world model” be far behind?\nOpenAI seeks to span the AI market, propriety and open source alike,\neven if the latter is worth nothing in terms of actual, direct dollars and cents.\nTraining and architecture\nFeedback from developers directly influenced gpt-oss’s design. OpenAI says the top request was for a permissive license, which led to the adoption of Apache 2.0 for both models. Both models use a\nMixture-of-Experts (MoE)\narchitecture with a\nTransformer backbone\n.\nThe larger gpt-oss-120b activates 5.1 billion parameters per token (out of 117 billion total), and gpt-oss-20b activates 3.6 billion (out of 21 billion total).\nBoth support\n128,000 token context length\n(about 300-400 pages of a novel’s worth of text a user can upload at once), and employ\nlocally banded sparse attention\nand use\nRotary Positional Embeddings\nfor encoding.\nThe tokenizer — the program that converts words and chunks of words into the numerical tokens that the LLMs can understand, dubbed\n“o200k_harmony\n“\n— is also being open-sourced.\nDevelopers can select among\nlow, medium, or high reasoning effort\nsettings based on latency and performance needs. While these models can reason across complex agentic tasks, OpenAI emphasizes they were not trained with\ndirect supervision of CoT outputs\n, to preserve the observability of reasoning behavior—an approach OpenAI considers important for safety monitoring.\nAnother\ncommon request from OpenAI’s developer community was for strong support for function calling, particularly for agentic workloads\n, which OpenAI believes gpt-oss now delivers.\nThe models are engineered for\nchain-of-thought reasoning\n,\ntool use\n, and\nfew-shot function calling\n, and are compatible with\nOpenAI’s\nResponses API\nintroduced back in March, which allows developers to augment their apps by connecting an OpenAI LLM of their choice to three powerful built-in tools —\nweb search\n,\nfile search\n, and\ncomputer use\n— within a single API call.\nBut for the new gpt-oss models, tool use capabilities — including web search and code execution — are not tied to OpenAI infrastructure.\nOpenAI provides the schemas and examples used during training, such as a basic browser implementation using the Exa API and a Python interpreter that operates in a Docker container.\nIt is\nup to individual inference providers or developers to define how tools are implemented.\nProviders like vLLM, for instance, allow users to configure their own MCP (Model-Controller-Proxy) server to specify the browser backend.\nWhile these models can reason across complex agentic tasks, OpenAI emphasizes they were not trained with\ndirect supervision of CoT outputs\n, to preserve the observability of reasoning behavior—an approach OpenAI considers important for safety monitoring.\nSafety evaluations and measures\nOpenAI conducted safety training using its\nPreparedness Framework\n, a document that outlines the procedural commitments, risk‑assessment criteria, capability categories, thresholds, evaluations, and governance mechanisms OpenAI uses to monitor, evaluate, and mitigate frontier AI risks.\nThese included\nfiltering chemical, biological, radiological, and nuclear threat (CBRN) related data\nout during pretraining, and applying advanced post-training safety methods such as\ndeliberative alignment\nand an\ninstruction hierarchy\nto enforce refusal behavior on harmful prompts.\nTo test worst-case misuse potential, OpenAI adversarially fine-tuned gpt-oss-120b on sensitive biology and cybersecurity data using its internal RL training stack. These\nmalicious fine-tuning (MFT)\nscenarios—one of the most sophisticated evaluations of this kind to date—included enabling browsing and disabling refusal behavior, simulating real-world attack potential.\nThe resulting models were benchmarked against both open and proprietary LLMs, including DeepSeek R1-0528, Qwen 3 Thinking, Kimi K2, and OpenAI’s o3. Despite enhanced access to tools and targeted training, OpenAI found that even the fine-tuned gpt-oss models remained\nbelow the “High” capability threshold\nfor frontier risk domains such as biorisk and cybersecurity. These conclusions were reviewed by\nthree independent expert groups\n, whose recommendations were incorporated into the final methodology.\nIn parallel, OpenAI partnered with\nSecureBio\nto run external evaluations on biology-focused benchmarks like\nHuman Pathogen Capabilities Test (HPCT)\n,\nMolecular Biology Capabilities Test (MBCT)\n, and others. Results showed that gpt-oss’s fine-tuned models performed close to OpenAI’s o3 model, which is not classified as frontier-high under OpenAI’s safety definitions.\nAccording to OpenAI, these findings contributed directly to the decision to release gpt-oss openly. The release is also intended to support safety research, especially around monitoring and controlling open-weight models in complex domains.\nAvailability and ecosystem support\nThe gpt-oss models are now available on Hugging Face, with pre-built support through major deployment platforms including\nAzure\n,\nAWS\n,\nDatabricks\n,\nCloudflare\n,\nVercel\n,\nTogether AI\n,\nOpenRouter\n, and others. Hardware partners include\nNVIDIA\n,\nAMD\n, and\nCerebras\n, and Microsoft is making GPU-optimized builds available on\nWindows via ONNX Runtime\n.\nOpenAI has also announced a\n$500,000 Red Teaming Challenge\nhosted on Kaggle, inviting researchers and developers to probe the limits of gpt-oss and identify novel misuse pathways. A public report and an open-source evaluation dataset will follow, aiming to accelerate open model safety research across the AI community.\nEarly adopters such as\nAI Sweden\n,\nOrange\n, and\nSnowflake\nhave collaborated with OpenAI to explore deployments ranging from localized fine-tuning to secure on-premise use cases. OpenAI characterizes the launch as an invitation for developers, enterprises, and governments to run state-of-the-art language models on their own terms.\nWhile OpenAI has not committed to a fixed cadence for future open-weight releases, it signals that gpt-oss represents a strategic expansion of its approach — balancing openness with aligned safety methodologies to shape how large models are shared and governed in the years ahead.\nThe big question: with so much competition in open source AI, will OpenAI’s own efforts pay off?\nOpenAI re-enters the open source model market in\nthe most competitive moment\nyet.\nAt the top of public AI benchmarking leaderboards, U.S. frontier models remain\nproprietary\n—\nOpenAI\n(GPT-4o/o3),\nGoogle\n(Gemini), and\nAnthropic\n(Claude).\nBut they now compete directly with a surge of\nopen-weights\ncontenders. From China:\nDeepSeek-R1\n(open source, MIT)\nand\nDeepSeek-V3 (open-weights under a DeepSeek Model License that permits commercial use)\n;\nAlibaba’s Qwen 3\n(open-weights, Apache-2.0)\n;\nMoonshotAI’s Kimi K2\n(open-weights; public repo and model cards)\n; and\nZ.ai’s GLM-4.5\n(also Apache 2.0 licensed).\nEurope’s\nMistral (Mixtral/Mistral, open-weights, Apache-2.0)\nanchors the EU push; the UAE’s\nFalcon 2/3\npublish\nopen-weights\nunder TII’s Apache-based license. In the U.S. open-weights camp,\nMeta’s Llama 3.1\nships under a\ncommunity (source-available) license\n,\nGoogle’s Gemma\nunder\nGemma terms\n(open weights with use restrictions), and\nMicrosoft’s Phi-3.5\nunder\nMIT\n.\nDeveloper pull mirrors that split.\nOn Hugging Face,\nQwen2.5-7B-Instruct (open-weights, Apache-2.0)\nsits near the top by “downloads last month,” while\nDeepSeek-R1 (MIT)\nand\nDeepSeek-V3 (model-licensed open weights)\nalso post heavy traction. Open-weights stalwarts\nMistral-7B / Mixtral\n(Apache-2.0),\nLlama-3.1-8B/70B\n(Meta community license),\nGemma-2\n(Gemma terms),\nPhi-3.5\n(MIT),\nGLM-4.5\n(open-weights), and\nFalcon-2-11B\n(TII Falcon License 2.0) round out the most-pulled families —underscoring that the open ecosystem spans the U.S., Europe, the Middle East, and China. Hugging Face signals adoption, not market share, but they show where builders are experimenting and deploying today.\nConsumer usage remains concentrated in proprietary apps even as weights open up.\nChatGPT\nstill drives the largest engagement globally (about\n2.5 billion prompts/day\n, proprietary service), while in China the leading assistants —\nByteDance’s Doubao\n,\nDeepSeek’s app\n,\nMoonshot’s Kimi\n, and\nBaidu’s ERNIE Bot\n— are delivered as\nproprietary products\n, even as several base models (GLM-4.5, ERNIE 4.5 variants) now ship as\nopen-weights\n.\nBut now that a range of powerful open source models are available to businesses and consumers — all nearing one another in terms of performance — and can be downloaded on consumer hardware, the\nbig question facing OpenAI is: who will pay for intelligence at all? Will the convenience of the web-based chatbot interface, multimodal capabilities, and more powerful performance be enough to keep the dollars flowing?\nOr has machine intelligence already become, in the\nwords of Atlman himself\n, “too cheap to meter”? And if so, how to build a successful business atop it, especially with OpenAI and other AI firms’ sky-high valuations and expenditures.\nOne clue:\nOpenAI is already said to be offering in-house engineers\nto help its enterprise customers customize and deploy fine-tuned models, similar to Palantir’s “forward deployed” software engineers (SWEs), essentially charging for experts to come in, set up the models correctly, and train employees how to use them for best results.\nPerhaps the world will migrate toward a majority of AI usage going to open source models, or a sizeable minority, with OpenAI and other AI model providers offering experts to help install said models into enterprises. Is that enough of a service to build a multi-billion dollar business upon? Or will enough people continue paying $20, $200 or more each month to have access to even more powerful proprietary models?\nI don’t envy the folks at OpenAI figuring out all the business calculations — despite what I assume to be hefty compensation as a result, at least for now. But for end users and enterprises, the release of the gpt-oss series is undoubtedly compelling.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nIs AI Growth Draining Your Budget and the Grid?\nInference costs and power limits are forcing teams to rethink how they scale. On Sept 4 in San Francisco, join our invite‑only salon to learn how leaders are cutting costs and emissions with smarter infrastructure.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-08-06T17:15:54.132817",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-08-06T17:16:15.640591",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/2a3674d38786697b459c67817b795582.mp3",
    "audio_file": "audio/articles/2a3674d38786697b459c67817b795582.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-08-06T17:17:11.831654",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  }
]