[
  {
    "id": "41ea6141f6a835a6d63ee837d3a16fe3",
    "title": "SAG-AFTRA board approves agreement with game companies on AI and new contract",
    "url": "https://venturebeat.com/games/sag-aftra-board-approves-agreement-with-game-companies-on-ai-and-new-contract/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-13T00:04:00+00:00",
    "source": "VentureBeat",
    "summary": "SAG-AFTRA的董事會批准了與遊戲公司達成的協議，涉及AI和新合同。這份新合同將提高表演者的報酬，設立了保護AI使用的規則，並確保了更好的安全條款，例如在危險情況下需要有合格醫務人員在場。這項協議還包括了對數位人物使用的最低標準和更高的報酬。這將對遊戲行業的表演者帶來更好的工作條件和報酬。",
    "content": "SAG-AFTRA board approves agreement with game companies on AI and new contract | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSAG-AFTRA board approves agreement with game companies on AI and new contract\nDean Takahashi\n@deantak\nJune 12, 2025 5:04 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nSAG-AFTRA's board has approved a deal with game companies, setting up membership vote.\nImage Credit: SAG-AFTRA\nThe\nScreen Actors Guild-American Federation of Television and Radio Artists\n(SAG-AFTRA) National Board approved the tentative agreement with the video game bargaining group.\nThe contract on terms for the Interactive Media Agreement will now be submitted to the membership for ratification.\nThe new contract accomplishes important guardrails and gains around AI, including the requirement of informed consent across various AI uses and the ability for performers to suspend informed consent for Digital Replica use during a strike.\nIf ratified, the agreement would provide compounded increases in performer compensation at a rate of 15.17% upon ratification plus additional 3% increases in November 2025, November 2026 and November 2027. Additionally, the overtime rate maximum for overscale performers will now be based on double scale. The health & retirement contribution rates to the SAG-AFTRA Health Plan will be raised from 16.5% to 17% upon ratification and to 17.5% in Oct. 2026.\nCompensation gains include the establishment of collectively-bargained minimums for the use of Digital Replicas created with IMA-covered performances and higher minimums (7.5x scale) for “Real Time Generation,” i.e., embedding a Digital Replica-voiced chatbot in a video game. “Secondary Performance Payments” will also ensure compensation when visual performances are re-used in another videogame.\nEssential new safety provisions were also secured, including a requirement for a qualified medical professional to be present or readily available at rehearsals and performances during which hazardous actions or working conditions are planned. Rest periods are now provided for on-camera principal performers and employers can no longer request that performers complete stunts or other dangerous activity in virtual auditions.\nThe spokesperson for the video game producers party to the Interactive Media Agreement, Audrey Cooling, said earlier this week in a statement, “We are pleased to have reached a tentative contract agreement that reflects the important contributions of SAG-AFTRA-represented performers in video games. This agreement builds on three decades of successful partnership between the interactive entertainment industry and the union.”\nCooling added, “It delivers historic wage increases of over 24% for performers, enhanced health and safety protections, and industry-leading AI provisions requiring transparency, consent and compensation for the use of digital replicas in games. We look forward to continuing to work with performers to create new and engaging entertainment experiences for billions of players throughout the world.”\nThe full terms of the three-year deal will be released with the ratification materials on Wednesday, June 18.\nA tentative agreement was reached with the video game employers on June 9 and the strike was officially suspended on June 11.\nMember informational meetings are being scheduled and additional details will be available at\nsagaftra.org/videogames2025\nin the coming days.\nEligible SAG-AFTRA members will have until 5 p.m. PDT on Wednesday, July 9, 2025 to cast their vote on ratification.\nSAG-AFTRA represents approximately 160,000 actors, announcers, broadcast journalists, dancers, DJs, news writers, news editors, program hosts, puppeteers, recording artists, singers, stunt performers, voiceover artists and other entertainment and media professionals.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:02.327130",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:35.775026",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/41ea6141f6a835a6d63ee837d3a16fe3.mp3"
  },
  {
    "id": "1d10958faf07322d96af124a9be2ca70",
    "title": "Meta’s new world model lets robots manipulate objects in environments they’ve never encountered before",
    "url": "https://venturebeat.com/ai/metas-new-world-model-lets-robots-manipulate-objects-in-environments-theyve-never-encountered-before/",
    "authors": "Ben Dickson",
    "published_date": "2025-06-12T22:22:07+00:00",
    "source": "VentureBeat",
    "summary": "Meta最新的世界模型讓機器人能夠在從未遇過的環境中操作物件。這個模型可以幫助AI應用在不可預測的環境中預測結果並計劃行動，為製造和物流等領域帶來更多可能性。這個技術讓機器人能夠像人類一樣觀察世界並做出反應，是AI在實際環境中更進一步發展的重要一步。",
    "content": "Meta's new world model lets robots manipulate objects in environments they've never encountered before | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nMeta’s new world model lets robots manipulate objects in environments they’ve never encountered before\nBen Dickson\n@BenDee983\nJune 12, 2025 3:22 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage credit: VentureBeat with Gemini\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nWhile large language models (LLMs) have mastered text (and other modalities to some extent), they lack the physical “common sense” to operate in dynamic, real-world environments. This has limited the deployment of AI in areas like manufacturing and logistics, where understanding cause and effect is critical.\nMeta’s latest model,\nV-JEPA 2\n, takes a step toward bridging this gap by learning a world model from video and physical interactions.\nV-JEPA 2 can help create AI applications that require predicting outcomes and planning actions in unpredictable environments with many edge cases. This approach can provide a clear path toward more capable robots and advanced automation in physical environments.\nHow a ‘world model’ learns to plan\nHumans develop physical intuition early in life by observing their surroundings. If you see a ball thrown, you instinctively know its trajectory and can predict where it will land. V-JEPA 2 learns a similar “world model,” which is an AI system’s internal simulation of how the physical world operates.\nmodel is built on three core capabilities that are essential for enterprise applications: understanding what is happening in a scene, predicting how the scene will change based on an action, and planning a sequence of actions to achieve a specific goal. As Meta states in its\nblog\n, its “long-term vision is that world models will enable AI agents to plan and reason in the physical world.”\nThe model’s architecture, called the Video Joint Embedding Predictive Architecture (V-JEPA), consists of two key parts. An “encoder” watches a video clip and condenses it into a compact numerical summary, known as an\nembedding\n. This embedding captures the essential information about the objects and their relationships in the scene. A second component, the “predictor,” then takes this summary and imagines how the scene will evolve, generating a prediction of what the next summary will look like.\nV-JEPA is composed of an encoder and a predictor (source: Meta blog)\nThis architecture is the latest evolution of the JEPA framework, which was first applied to images with\nI-JEPA\nand now advances to video, demonstrating a consistent approach to building world models.\nUnlike generative AI models that try to predict the exact color of every pixel in a future frame — a computationally intensive task — V-JEPA 2 operates in an abstract space. It focuses on predicting the high-level features of a scene, such as an object’s position and trajectory, rather than its texture or background details, making it far more efficient than other larger models at just 1.2 billion parameters\nThat translates to lower compute costs and makes it more suitable for deployment in real-world settings.\nLearning from observation and action\nV-JEPA 2 is trained in two stages. First, it builds its foundational understanding of physics through\nself-supervised learning\n, watching over one million hours of unlabeled internet videos. By simply observing how objects move and interact, it develops a general-purpose world model without any human guidance.\nIn the second stage, this pre-trained model is fine-tuned on a small, specialized dataset. By processing just 62 hours of video showing a robot performing tasks, along with the corresponding control commands, V-JEPA 2 learns to connect specific actions to their physical outcomes. This results in a model that can plan and control actions in the real world.\nV-JEPA two-stage training pipeline (source: Meta)\nThis two-stage training enables a critical capability for real-world automation: zero-shot robot planning. A robot powered by V-JEPA 2 can be deployed in a new environment and successfully manipulate objects it has never encountered before, without needing to be retrained for that specific setting.\nThis is a significant advance over previous models that required training data from the\nexact\nrobot and environment where they would operate. The model was trained on an open-source dataset and then successfully deployed on different robots in Meta’s labs.\nFor example, to complete a task like picking up an object, the robot is given a goal image of the desired outcome. It then uses the V-JEPA 2 predictor to internally simulate a range of possible next moves. It scores each imagined action based on how close it gets to the goal, executes the top-rated action, and repeats the process until the task is complete.\nUsing this method, the model achieved success rates between 65% and 80% on pick-and-place tasks with unfamiliar objects in new settings.\nReal-world impact of physical reasoning\nThis ability to plan and act in novel situations has direct implications for business operations. In logistics and manufacturing, it allows for more adaptable robots that can handle variations in products and warehouse layouts without extensive reprogramming. This can be especially useful as companies are exploring the deployment of\nhumanoid robots\nin factories and assembly lines.\nThe same world model can power highly realistic digital twins, allowing companies to simulate new processes or train other AIs in a physically accurate virtual environment. In industrial settings, a model could monitor video feeds of machinery and, based on its learned understanding of physics, predict safety issues and failures before they happen.\nThis research is a key step toward what Meta calls “advanced machine intelligence (AMI),” where AI systems can “learn about the world as humans do, plan how to execute unfamiliar tasks, and efficiently adapt to the ever-changing world around us.”\nMeta has released the model and its training code and hopes to “build a broad community around this research, driving progress toward our ultimate goal of developing world models that can transform the way AI interacts with the physical world.”\nWhat it means for enterprise technical decision-makers\nV-JEPA 2 moves robotics closer to the software-defined model that cloud teams already recognize: pre-train once, deploy anywhere. Because the model learns general physics from public video and only needs a few dozen hours of task-specific footage, enterprises can slash the data-collection cycle that typically drags down pilot projects. In practical terms, you can prototype a pick-and-place robot on an affordable desktop arm, then roll the same policy onto an industrial rig on the factory floor without gathering thousands of fresh samples or writing custom motion scripts.\nLower training overhead also reshapes the cost equation. At 1.2 billion parameters, V-JEPA 2 fits comfortably on a single high-end GPU, and its abstract prediction targets reduce inference load further. That lets teams run closed-loop control on-prem or at the edge, avoiding cloud latency and the compliance headaches that come with streaming video outside the plant. Budget that once went to massive compute clusters can fund extra sensors, redundancy, or faster iteration cycles instead.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:02.483371",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:37.907059",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/1d10958faf07322d96af124a9be2ca70.mp3"
  },
  {
    "id": "f1da1ca3ea421eab33c45699a0ed79fd",
    "title": "Cloud collapse: Replit and LlamaIndex knocked offline by Google Cloud identity outage",
    "url": "https://venturebeat.com/ai/cloud-collapse-replit-llamaindex-knocked-offline-by-google-cloud-identity-outage/",
    "authors": "Emilia David",
    "published_date": "2025-06-12T22:05:33+00:00",
    "source": "VentureBeat",
    "summary": "Google Cloud的身份驗證故障導致Replit和LlamaIndex無法運作，影響了許多AI開發工具和資料存儲服務。這次故障也讓一些AI平台像是ChatGPT和Claude無法運作。雖然影響範圍廣泛，但Google的一些服務仍持續運作。故障影響了API Gateway、Cloud Data Fusion、Google Cloud Storage等服務，也讓Google的移動開發平台Firebase癱瘓。Cloudflare表示受影響的服務數量有限，預計很快就會恢復正常。",
    "content": "Cloud collapse: Replit and LlamaIndex knocked offline by Google Cloud identity outage | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nCloud collapse: Replit and LlamaIndex knocked offline by Google Cloud identity outage\nEmilia David\n@miyadavid\nJune 12, 2025 3:05 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat, generated with ChatGPT\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nDays after\nOpenAI\nand\nGoogle Cloud\nannounced a partnership to support the growing use of generative AI platforms, much of the AI-powered web and tools went down due to an outage of the leading cloud providers.\nGoogle Cloud Service Platform (GCP) and some\nCloudflare\nservices\nbegan experiencing\nissues\naround 10:00 a.m. PT\ntoday, affecting several AI development tools and data storage services, including ChatGPT and Claude, as well as a variety of other AI platforms.\nWe are aware of a service disruption to some Google Cloud services and we are working hard to get you back up and running ASAP.\nPlease view our status dashboard for the latest updates:\nhttps://t.co/sT6UxoRK4R\n— Google Cloud (@googlecloud)\nJune 12, 2025\nA GCP spokesperson confirmed the outage to VentureBeat, urging users to check its public status dashboard.\nGCP said affected services include API Gateway, Agent Assist, Cloud Data Fusion, Contact Center AI Platform, Google App Engine, Google BigQuery, Google Cloud Storage, Identity Platform, Speech-to-Text, Text-to-Speech and Vertex AI Search, among other tools. Google’s mobile development platform, Firebase, also\nwent down\n.\nVentureBeat staffers had trouble accessing Google Meet, but other Google services on Workspace remained online.\nA Cloudflare spokesperson told VentureBeat only “a limited number of services at Cloudflare use Google Cloud and were impacted. We expect them to come back shortly. The core Cloudflare services were not impacted.”\nDespite media reports and user-provided feedback on Down Detector,\nAWS\nstated that\nits service remains up\n, including AI platforms such as Bedrock and Sagemaker.\nOpenAI acknowledged some users had issues logging into their platforms but have\nsince resolved the problem\n.\nAnthropic\nnoted on its\nstatus page\nthat Claude experienced “elevated error rates on the API, console and Claude AI.”\nWe are aware of issues affecting multiple external internet providers, impacting the availability of our services such as single sign-on (SSO) and other log-in methods. Our engineering teams are working to mitigate these issues.\nThank you for your continued patience.\nFor the…\n— OpenAI (@OpenAI)\nJune 12, 2025\nDeveloper tools like\nLlamaIndex\n’s LlamaCloud,\nWeights & Biases\n,\nWindsurf\n,\nSupabase\nand\nReplit\nreported issues. Other platforms like\nCharacter AI\nalso\nannounced\nthey were affected.\nHi folks – LlamaCloud (\nhttps://t.co/DHMd6BFO0l\n) is currently down due to the ongoing global AWS/GCP/Firebase outage.\nWe are closely monitoring the solution and will keep you posted when it's resolved!\n— Jerry Liu (@jerryjliu0)\nJune 12, 2025\n? We're aware of the Google Cloud outage affecting various web services, including Weights & Biases products like W&B Models and\n@weave_wb\n.\nOur team is monitoring the situation and will provide updates.\nThank you for your patience.\n— Weights & Biases (@weights_biases)\nJune 12, 2025\nOur upstream cloud providers are currently experiencing a major outage. We are working as best we can to restore Replit services.\n— Replit ⠕ (@Replit)\nJune 12, 2025\nIn addition to AI tools, other websites and internet services, such as Spotify and Discord, also reportedly went down.\nNeeding more cloud\nIn many ways, the outage highlights the challenges of relying on a single cloud service or database provider and the risks associated with an interconnected Internet. If one of your cloud services goes down, it could impact some users whose log-in or data stream is hosted there.\nGoogle Cloud has been gradually wresting\nmarket leadership in enterprise AI\nfrom its competitors, thanks to the large number of developer and database tools it has begun offering organizations. On the other hand, Cloudflare has been\npartnering with companies\nlike Hugging Face to deploy AI apps faster.\nFirst reported by\nReuters\n, Google and OpenAI have struck a deal that will allow OpenAI to utilize Google Cloud to meet the growing demand on its platform.\nBut that’s not to say Google or Cloudflare may lose an edge among enterprise AI users who depend on consistent uptime. While the company continues to investigate the cause of the outage, enterprises often have, and should have, redundancies in case their provider goes down. Outages happen, and they happen far too frequently.\nThe last massive outage happened around the same time last year, in July, when CrowdStrike\naccidentally triggered outages\nthat impacted Microsoft Windows users.\nIn typical fashion, many people saw the outages as an opportunity for comedy, or at least to catch up on tasks they’d been putting off.\nmuch of the AI internet is down now\nfirebase is down, cursor is down, lovable is down, supabase is down, google ai is down, cursor is down, aws is down… almost everything is down.\nfinally time to catch up on the 87 tools, 14 models, and 12 AI startup ideas we want to build.\n— GREG ISENBERG (@gregisenberg)\nJune 12, 2025\nThank GCP.I couldn’t find a reason to dip out of a couple meetings this afternoon and now I do!\n—\nAdam (@adambahm.bsky.social)\n2025-06-12T19:38:07.612Z\nSo yes, it seems like the digital universe is giving everyone a forced break today!\n— Murugan (MGA) (@murugan_mga)\nJune 12, 2025\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:02.627634",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:41.643893",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/f1da1ca3ea421eab33c45699a0ed79fd.mp3"
  },
  {
    "id": "43f04770b0bf01f5765a3ce040a196f7",
    "title": "TensorWave deploys AMD Instinct MI355X GPUs in its cloud platform",
    "url": "https://venturebeat.com/games/tensorwave-deploys-amd-instinct-mi355x-gpus-in-its-cloud-platform/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-12T18:30:00+00:00",
    "source": "VentureBeat",
    "summary": "TensorWave在其雲平台中部署了AMD最新的Instinct MI355X GPU，這提供了高效能的AI運算能力，讓客戶在處理複雜AI任務時能有更好的表現。TensorWave的專注於AMD技術，讓客戶可以享有高效能的AI軟體支援，同時避免被特定廠商所限制，降低擁有成本。這項部署讓企業和新創公司都能享有高達25%的效率提升和40%的成本降低。",
    "content": "TensorWave deploys AMD Instinct MI355X GPUs in its cloud platform | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nTensorWave deploys AMD Instinct MI355X GPUs in its cloud platform\nDean Takahashi\n@deantak\nJune 12, 2025 11:30 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nTensorWave is deploying AMD's latest Instinct AI GPUs in its AI cloud platform.\nImage Credit: TensorWave\nTensorWave\n, a leader in AMD-powered AI infrastructure solutions, today announced the deployment of AMD Instinct MI355X GPUs in its high-performance cloud platform.\nAs one of the first cloud providers to bring the AMD Instinct MI355X to market, TensorWave enables\ncustomers to unlock next-level performance for the most demanding AI workloads—all with unmatched white-glove onboarding and support.\nThe new AMD Instinct MI355X GPU is built on the 4th Gen AMD CDNA architecture and features 288GB of HBM3E memory and 8TB/s memory bandwidth, optimized for generative AI training, inference, and high-performance computing (HPC).\nTensorWave’s early adoption allows its customers to benefit from the MI355X’s compact, scalable design and advanced architecture, delivering high-density compute with advanced cooling infrastructure at scale.\n“TensorWave’s deep specialization in AMD technology makes us a highly optimized environment for next-gen AI workloads,” said Piotr Tomasik, president at TensorWave, in a statement. “With the Instinct MI325X now deployed on our cloud and Instinct MI355X coming soon, we’re enabling startups and enterprises alike to achieve up to 25% efficiency gains and 40% cost reductions, results we’ve already seen with customers using our AMD-powered infrastructure.”\nTensorWave’s exclusive use of AMD GPUs provides customers with an open, optimized AI software stack powered by AMD ROCm, avoiding vendor lock-in and reducing total cost of ownership. Its focus on scalability, developer-first onboarding, and enterprise-grade SLAs makes it the go-to partner for organizations prioritizing performance and choice.\n“AMD Instinct MI350 series GPUs deliver breakthrough performance for the most demanding AI and HPC workloads,” said Travis Karr, corporate vice president of business development, Data Center GPU Business,\nAMD, in a statement. “The AMD Instinct portfolio, together with our ROCm open software ecosystem, enables customers to develop cutting-edge platforms that power generative AI, AI-driven scientific discovery, and high-performance computing applications.”\nTensorWave is also currently building the largest AMD-specific AI training cluster in North America, advancing its mission to democratize access to high-performance compute. By delivering end-to-end support for AMD-based AI workloads, TensorWave empowers customers to seamlessly transition, optimize, and scale within an open and rapidly evolving ecosystem.\nFor more information please visit:\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:02.771013",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:43.906771",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/43f04770b0bf01f5765a3ce040a196f7.mp3"
  },
  {
    "id": "eb19770d3c3446d9e190f11e1391d488",
    "title": "AMD debuts AMD Instinct MI350 Series accelerator chips with 35X better inferencing",
    "url": "https://venturebeat.com/games/amd-debuts-amd-instinct-mi350-series-accelerator-chips-with-35x-better-inferencing/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-12T18:30:00+00:00",
    "source": "VentureBeat",
    "summary": "AMD 推出 AMD Instinct MI350 系列加速器晶片，推進推理速度提升了35倍，並在 AI 運算方面有四倍的提升。這代表 AMD 在 AI 領域的技術突破，將帶來更強大的人工智慧解決方案，並展示了對未來 AI 技術發展的信心。AMD CEO 蘇姿丰更表示，未來的 AI 發展需要產業間的開放合作，而非封閉系統。",
    "content": "AMD debuts AMD Instinct MI350 Series accelerator chips with 35X better inferencing | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nAMD debuts AMD Instinct MI350 Series accelerator chips with 35X better inferencing\nDean Takahashi\n@deantak\nJune 12, 2025 11:30 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nAMD Instinct MI350 Series accelerators are debuting now.\nImage Credit: AMD\nAMD unveiled its comprehensive end-to-end integrated AI platform vision and introduced its open, scalable rack-scale AI infrastructure built on industry standards at its annual Advancing AI event.\nThe Santa Clara, California-based chip maker announced its new AMD Instinct MI350 Series accelerators, which are four times faster on AI compute and 35 times faster on inferencing than prior chips.\nAMD and its partners showcased AMD Instinct-based products and the continued growth of the AMD ROCm ecosystem. It also showed its powerful, new, open rack-scale designs and roadmap that bring leadership Rack Scale AI performance beyond 2027.\n“We can now say we are at the inference inflection point, and it will be the driver,” said Lisa Su, CEO of AMD, in a keynote at the Advancing AI event.\nIn closing, in a jab at Nvidia, she said, “The future of AI will not be built by any one company or within a closed system. It will be shaped by open collaboration across the industry with everyone bringing their best ideas.”\nLisa Su, CEO of AMD, at Advancing AI event.\nAMD unveiled the Instinct MI350 Series GPUs, setting a new benchmark for performance, efficiency and scalability in generative AI and high-performance computing. The MI350 Series, consisting of both Instinct MI350X and MI355X GPUs and platforms, delivers a four times generation-on-generation AI compute increase and a 35 times generational leap in inferencing, paving the way for transformative AI solutions across industries.\n“We are tremendously excited about the work you are doing at AMD,” said Sam Altman, CEO of Open AI, on stage with Lisa Su.\nHe said he couldn’t believe it when he heard about the specs for MI350 from AMD, and he was grateful that AMD took his company’s feedback.\nAMD said its latest Instinct GPUs can beat Nvidia chips.\nAMD demonstrated end-to-end, open-standards rack-scale AI infrastructure—already rolling out with AMD Instinct MI350 Series accelerators, 5th Gen AMD Epyc processors and AMD Pensando Pollara network interface cards (NICs) in hyperscaler deployments such as Oracle Cloud Infrastructure (OCI) and set for broad availability in 2H 2025. AMD also previewed its next generation AI rack called Helios.\nIt will be built on the next-generation AMD Instinct MI400 Series GPUs, the Zen 6-based AMD Epyc Venice CPUs and AMD Pensando Vulcano NICs.\n“I think they are targeting a different type of customer than Nvidia,” said Ben Bajarin, analyst at Creative Strategies, in a message to GamesBeat. “Specifically I think they see the neocloud opportunity and a whole host of tier two and tier three clouds and the on-premise enterprise deployments.”\nBajarin added, “We are bullish on the shift to full rack deployment systems and that is where Helios fits in which will align with Rubin timing. But as the market shifts to inference, which we are just at the start with, AMD is well positioned to compete to capture share. I also think, there are lots of customers out there who will value AMD’s TCO where right now Nvidia may be overkill for their workloads. So that is area to watch, which again gets back to who the right customer is for AMD and it might be a very different customer profile than the customer for Nvidia.”\nThe latest version of the AMD open-source AI software stack, ROCm 7, is engineered to meet the growing demands of generative AI and high-performance computing workloads— while dramatically improving developer experience across the board. (Radeon Open Compute is an open-source software platform that allows for GPU-accelerated computing on AMD GPUs, particularly for high-performance computing and AI workloads). ROCm 7 features improved support for industry-standard frameworks, expanded hardware compatibility, and new development tools, drivers, APIs and libraries to accelerate AI development and deployment.\nIn her keynote, Su said, “Opennesss should be more than just a buzz word.”\nThe Instinct MI350 Series exceeded AMD’s five-year goal to improve the energy efficiency of AI training and high-performance computing nodes by 30 times, ultimately delivering a 38 times improvement. AMD also unveiled a new 2030 goal to deliver a 20 times increase in rack-scale energy efficiency from a 2024 base year, enabling a typical AI model that today requires more than 275 racks to be trained in fewer than one fully utilized rack by 2030, using 95% less electricity.\nAMD also announced the broad availability of the AMD Developer Cloud for the global developer and open-source communities. Purpose-built for rapid, high-performance AI development, users will have access to a fully managed cloud environment with the tools and flexibility to get started with AI projects – and grow without limits. With ROCm 7 and the AMD Developer Cloud, AMD is lowering barriers and expanding access to next-gen compute. Strategic collaborations with leaders like Hugging Face, OpenAI and Grok are proving the power of co-developed, open solutions. The announcement got some cheers from folks in the audience, as the company said it would give attendees developer credits.\nBroad Partner Ecosystem Showcases AI Progress Powered by AMD\nAMD’s ROCm 7\nAMD customers discussed how they are using AMD AI solutions to train today’s leading AI models, power inference at scale and accelerate AI exploration and development.\nMeta detailed how it has leveraged multiple generations of AMD Instinct and Epyc solutions across its data center infrastructure, with Instinct MI300X broadly deployed for Llama 3 and Llama 4 inference. Meta continues to collaborate closely with AMD on AI roadmaps, including plans to leverage MI350 and MI400 Series GPUs and platforms.\nOracle Cloud Infrastructure is among the first industry leaders to adopt the AMD open rack-scale AI infrastructure with AMD Instinct MI355X GPUs. OCI leverages AMD CPUs and GPUs to deliver balanced, scalable performance for AI clusters, and announced it will offer zettascale AI clusters accelerated by the latest AMD Instinct processors with up to 131,072 MI355X GPUs to enable customers to build, train, and inference AI at scale.\nAMD says its Instinct GPUs are more efficient than Nvidia’s.\nMicrosoft announced Instinct MI300X is now powering both proprietary and open-source models in production on Azure.\nHUMAIN discussed its landmark agreement with AMD to build open, scalable, resilient and cost-efficient AI infrastructure leveraging the full spectrum of computing platforms only AMD can provide.Cohere shared that its high-performance, scalable Command models are deployed on Instinct MI300X, powering enterprise-grade LLM inference with high throughput, efficiency and data privacy.\nIn the keynote, Red Hat described how its expanded collaboration with AMD enables production-ready AI environments, with AMD Instinct GPUs on Red Hat OpenShift AI delivering powerful, efficient AI processing across hybrid cloud environments.\n“They can get the most out of the hardware they’re using,” said the Red Hat exec on stage.\nAstera Labs highlighted how the open UALink ecosystem accelerates innovation and delivers greater value to customers and shared plans to offer a comprehensive portfolio of UALink products to support next-generation AI infrastructure.Marvell joined AMD to share the UALink switch roadmap, the first truly open interconnect, bringing the ultimate flexibility for AI infrastructure.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:02.860010",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:45.416869",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/eb19770d3c3446d9e190f11e1391d488.mp3"
  },
  {
    "id": "86b621613fb8229af946a497157959a6",
    "title": "Disney Pinnacle unveils Genesis Keys quest with Web3 support from Dapper Labs",
    "url": "https://venturebeat.com/games/disney-pinnacle-unveils-genesis-keys-quest-with-web3-support-from-dapper-labs/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-12T17:50:00+00:00",
    "source": "VentureBeat",
    "summary": "Disney Pinnacle和Dapper Labs合作推出Genesis Keys任務，讓迪士尼粉絲們可以每四小時領取免費的Genesis Keys，參與全球尋找獨一無二的Genesis Editions數位徽章。這個活動標誌著Disney Pinnacle在夏季展開更多行動的第一階段。Dapper Labs是一家Web3公司，他們與主要IP持有者合作，致力於將數位所有權帶給全球粉絲。",
    "content": "Disney Pinnacle unveils Genesis Keys quest with Web3 support from Dapper Labs | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nDisney Pinnacle unveils Genesis Keys quest with Web3 support from Dapper Labs\nDean Takahashi\n@deantak\nJune 12, 2025 10:50 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nDisney Pinnacle is coming from Dapper Labs.\nImage Credit: Disney\nDisney Pinnacle\nby\nDapper Labs\n, the digital pin collecting and trading platform, has now announced its Genesis Keys quest.\nThat means Disney fans worldwide can claim free Genesis Keys every four hours as part of the global quest for one-of-one Genesis Editions — the first digital pins ever minted on the platform.\nThe Genesis Keys campaign marks the first phase of Disney Pinnacle by Dapper Labs’ rollout gearing up for more action throughout the summer. As the community collectively claims Keys at 100K, 200K, and 300K milestones, Genesis Capsules containing ultra-rare digital pins will be released. Each Genesis Pin is uniquely serialized and features iconic Disney moments.\nDapper Labs is the Web3 company behind NBA Top Shot, NFL All Day, Disney Pinnacle, and the Flow blockchain. Since 2018, Dapper Labs has pioneered the industry in creating blockchain-based consumer experiences, working with major IP holders to bring digital ownership to fans worldwide. The Vancouver-based company has raised over $600 million from leading investors including Andreessen Horowitz, Coatue, and GV.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:02.941954",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:48.598896",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/86b621613fb8229af946a497157959a6.mp3"
  },
  {
    "id": "de9a8422980bc0b8a5f4d567551da5cb",
    "title": "Lil Snack makes a snackable Scattergories game in partnership with Hasbro",
    "url": "https://venturebeat.com/gaming-business/lil-snack-makes-a-snackable-scattergories-game-in-partnership-with-hasbro/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-12T15:30:00+00:00",
    "source": "VentureBeat",
    "summary": "Lil Snack和Hasbro合作推出了一款基於Scattergories桌遊的小型遊戲，每日更新挑戰玩家。他們的遊戲廣受歡迎，已有超過10萬次遊玩次數，並在第一年就創造了七位數的收入。這個合作將帶來更多基於知名品牌的遊戲，並將AI技術應用在遊戲開發中，致力於提供新穎的遊戲體驗。",
    "content": "Lil Snack makes a snackable Scattergories game in partnership with Hasbro | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nLil Snack makes a snackable Scattergories game in partnership with Hasbro\nDean Takahashi\n@deantak\nJune 12, 2025 8:30 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nLil Snack has teamed up with Hasbro.\nImage Credit: Lil Snack\nWhen we last talked with Lil Snack, they were a two-person company making little games that were like snacks. For fun, we posted their games on the GamesBeat.com web site, and it was cool to tell people that\nwe had games on our site\nat the 2024 GamesBeat Summit.\nIt was just a couple of guys using AI to make games.\nFlash forward to today, and now they’ve got a partnership with Hasbro to make a game based on the Scattergories board game. Lil Snack has ten new games operated daily for as many as 650 days in a row. It has 600,000 monthly active users and more than 10 million plays to date.\nThe company’s audience for its snackable games is broad, with roughly 50% male and 50% female across all demographics. It generated seven figures in revenue in its first year, and the deal with Hasbro means that Scattergories Daily Live will debut this month on LilSnack.com. About 35% of the audience is playing five days a week or more.\nLil Snack’s games have been played more than 10 million times.\n“Our partnership with Hasbro highlights what’s possible when iconic brands like Scattergories meet fresh distribution — and affirms the momentum behind daily games as a new category,” said Eric Berman, cofounder of Lil Snack.\nNot bad. The games are refreshed daily to challenge players. And more intellectual properties are coming soon based on the partnership. And more games are coming soon.\n“The exciting relationship for us with Hasbro is incredibly topical, especially coming off the success of Monopoly Go,” said Berman. “We are building a game that will be utilizing AI with Hasbro. I think our approach to AI using consumer products is being human first, in terms of how we create games. What Scopely (maker of Monopoly Go) was for mobile, we would like to be for daily games,” said Berman”\nHe added, “Humans are the creative pieces behind everything we do, and we have no intent on that going away. If anything, we intend to try to make it more human, but we’re trying to use AI in a very novel way to deliver new game experiences that would not have been able to exist before.”\nNot so much AI after all\nLil Snack has hired a sizable team in the past year.\nWhile Lil Snack started heavily with AI generating games and game images at the beginning, it’s not doing as much now. The team of humans has grown instead.\nBut a number of them have never made games before. The AI tools have become a way for those people to make games that they could not have done before.\n“We try to hire people across the country. We try to hire folks of different backgrounds, different demos, to just really keep the broadness of the games applicable to as large of an audience as we possibly can,” Berman said.\nLil Snack has raised money from\nA16z and Lerer Hippeau\n.\n“We’ve been watching the rise of daily games closely—they’re a fun, bite-sized way people are building habits around play. Partnering with Lil Snack gives us a great chance to bring fan-favorites like Scattergories into that world in a way that feels fresh and relevant,” said Barry Dorf, vice president of interactive global licensing at Hasbro, in a statement.\nLil Snack Vision\nLil Snack’s growth is accelerating in an organic way.\n“The goal with Lil Snack is to create a new form of entertainment – a daily ritual of play, with games that are quick, fun, and culturally in tune,” said Travis Chen, cofounder of Lil Snack.\nBesides Hasbro, Lil Snack also made a game based on Sony’s Wheel of Fortune property. Berman said the game has done really well. Just about all the growth so far has been organic.\nRegarding the business model, Berman said, “We partner with the biggest consumer platforms on the planet to deliver daily engagement and retention through daily games. Now, our partners can benefit from beloved IP from Hasbro, with many more games coming soon.”\nScattergories\nScattergories is going daily with Lil Snack.\nDaily Scattergories is a daily single player interpretation of the iconic board game. Players are given a series of categories each day that they must find words to match – all in a race against the clock!\nAfter players finish submitting their category answers, each answer is judged by the Scattergories head! This category judging introduces an innovative single player mechanic to the classically multiplayer game.\nPlayers can then share their results with friends and family encouraging friendly competition. Berman expects to raise more money in the future to help fuel the growing team and build more product.\n“We’ve run over 650 days of games in a row now, and we are pretty good at putting new games live every single day,” Berman said.\nBerman sees a lot of big companies moving into daily games and light snackable games played on the TV. There are companies like Samsung and Netflix going at it from different directions, and more are kicking the tires, he said.\nAs for the games that ran on GamesBeat, Berman noted that some of them were just too hard. The company has since learned that people want light games where they can be successful. They don’t want to be stumped by tough trivia questions. They’re just looking for a daily escape.\n“We’ve definitely been trying to be less narrow and are broadening it is as much as we possibly can,” Berman said.\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:03.053352",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:51.033411",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/de9a8422980bc0b8a5f4d567551da5cb.mp3"
  },
  {
    "id": "51ce6e7b5d08e094b4c085ee2e9b13ad",
    "title": "Google DeepMind just changed hurricane forecasting forever with new AI model",
    "url": "https://venturebeat.com/ai/google-deepmind-just-changed-hurricane-forecasting-forever-with-new-ai-model/",
    "authors": "Michael Nuñez",
    "published_date": "2025-06-12T15:00:00+00:00",
    "source": "VentureBeat",
    "summary": "Google DeepMind利用新的AI模型改變了颶風預測，能夠準確預測熱帶氣旋的路徑和強度，並與美國國家颶風中心合作。這是人工智慧在氣象預測領域的重大突破，將幫助減少颶風帶來的經濟損失。這個新的實驗性預測模型能提供高達15天、50種可能的風暴情境，並讓專家預測員即時查看AI預測結果。",
    "content": "Google DeepMind just changed hurricane forecasting forever with new AI model | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nGoogle DeepMind just changed hurricane forecasting forever with new AI model\nMichael Nuñez\n@MichaelFNunez\nJune 12, 2025 8:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nGoogle DeepMind\nannounced Thursday what it claims is a major breakthrough in hurricane forecasting, introducing an artificial intelligence system that can predict both the path and intensity of tropical cyclones with unprecedented accuracy — a longstanding challenge that has eluded traditional weather models for decades.\nThe company launched\nWeather Lab\n, an interactive platform showcasing its experimental cyclone prediction model, which generates 50 possible storm scenarios up to 15 days in advance. More significantly, DeepMind announced a partnership with the\nU.S. National Hurricane Center\n, marking the first time the federal agency will incorporate experimental AI predictions into its operational forecasting workflow.\n“We are presenting three different things,” said Ferran Alet, a DeepMind research scientist leading the project, during a press briefing Wednesday. “The first one is a new experimental model tailored specifically for cyclones. The second one is, we’re excited to announce a partnership with the National Hurricane Center that’s allowing expert human forecasters to see our predictions in real time.”\nThe announcement marks a critical juncture in the application of artificial intelligence to weather forecasting, an area where machine learning models have rapidly gained ground against traditional physics-based systems. Tropical cyclones — which include hurricanes, typhoons, and cyclones — have caused\n$1.4 trillion in economic losses over the past 50 years\n, making accurate prediction a matter of life and death for millions in vulnerable coastal regions.\nWhy traditional weather models struggle with both storm path and intensity\nThe breakthrough addresses a fundamental limitation in current forecasting methods. Traditional weather models face a stark trade-off: global, low-resolution models excel at predicting where storms will go by capturing vast atmospheric patterns, while regional, high-resolution models better forecast storm intensity by focusing on turbulent processes within the storm’s core.\n“Making tropical cyclone predictions is hard because we’re trying to predict two different things,” Alet explained. “The first one is track prediction, so where is the cyclone going to go? The second one is intensity prediction, how strong is the cyclone going to get?”\nDeepMind’s experimental model claims to solve both problems simultaneously. In internal evaluations following\nNational Hurricane Center\nprotocols, the AI system demonstrated substantial improvements over existing methods. For track prediction, the model’s five-day forecasts were on average 140 kilometers closer to actual storm positions than\nENS\n, the leading European physics-based ensemble model.\nMore remarkably, the system outperformed\nNOAA’s Hurricane Analysis and Forecast System\n(HAFS) on intensity prediction — an area where AI models have historically struggled. “This is the first AI model that we are now very skillful as well on tropical cyclone intensity,” Alet noted.\nHow AI forecasts beat traditional models on speed and efficiency\nBeyond accuracy improvements, the AI system demonstrates dramatic efficiency gains. While traditional physics-based models can take hours to generate forecasts, DeepMind’s model produces 15-day predictions in approximately one minute on a single specialized computer chip.\n“Our probabilistic model is now even faster than the previous one,” Alet said. “Our new model, we estimate, is probably around one minute” compared to the eight minutes required by DeepMind’s previous weather model.\nThis speed advantage allows the system to meet tight operational deadlines. Tom Anderson, a research engineer on DeepMind’s AI weather team, explained that the\nNational Hurricane Center\nspecifically requested forecasts be available within six and a half hours of data collection — a target the AI system now meets ahead of schedule.\nNational Hurricane Center partnership puts AI weather forecasting to the test\nThe partnership with the\nNational Hurricane Center\nvalidates AI weather forecasting in a major way. Keith Battaglia, senior director leading DeepMind’s weather team, described the collaboration as evolving from informal conversations to a more official partnership allowing forecasters to integrate AI predictions with traditional methods.\n“It wasn’t really an official partnership then, it was just sort of more casual conversation,” Battaglia said of the early discussions that began about 18 months ago. “Now we’re sort of working toward a kind of a more official partnership that allow us to hand them the models that we’re building, and then they can decide how to use them in their official guidance.”\nThe timing proves crucial, with the 2025 Atlantic hurricane season already underway. Hurricane center forecasters will see live AI predictions alongside traditional physics-based models and observations, potentially improving forecast accuracy and enabling earlier warnings.\nDr. Kate Musgrave, a research scientist at the Cooperative Institute for Research in the Atmosphere at Colorado State University, has been evaluating DeepMind’s model independently. She found it demonstrates “comparable or greater skill than the best operational models for track and intensity,” according to the company. Musgrave stated she’s “looking forward to confirming those results from real-time forecasts during the 2025 hurricane season.”\nThe training data and technical innovations behind the breakthrough\nThe AI model’s effectiveness stems from its training on two distinct datasets: vast reanalysis data reconstructing global weather patterns from millions of observations, and a specialized database containing detailed information about nearly 5,000 observed cyclones from the past 45 years.\nThis dual approach is a departure from previous AI weather models that focused primarily on general atmospheric conditions. “We are training on cyclone specific data,” Alet explained. “We are training on IBTracs and other types of data. So IBTracs provides latitude and longitude and intensity and wind radii for multiple cyclones, up to 5000 cyclones over the last 30 to 40 years.”\nThe system also incorporates recent advances in probabilistic modeling through what DeepMind calls\nFunctional Generative Networks\n(FGN), detailed in a research paper released alongside the announcement. This approach generates forecast ensembles by learning to perturb the model’s parameters, creating more structured variations than previous methods.\nPast hurricane predictions show promise for early warning systems\nWeather Lab\nlaunches with over two years of historical predictions, allowing experts to evaluate the model’s performance across all ocean basins. Anderson demonstrated the system’s capabilities using Hurricane Beryl from 2024 and the notorious Hurricane Otis from 2023.\nHurricane Otis proved particularly significant because it rapidly intensified before striking Mexico, catching many traditional models off guard. “Many of the models were predicting that the storm would remain relatively weak throughout its lifetime,” Anderson explained. When DeepMind showed this example to National Hurricane Center forecasters, “they said that our model would have likely provided an earlier signal of the potential risk of this particular cyclone if they had it available at the time.”\nWhat this means for the future of weather forecasting and climate adaptation\nThe development signals artificial intelligence’s growing maturation in weather forecasting, following recent breakthroughs by DeepMind’s\nGraphCast\nand other AI weather models that have begun outperforming traditional systems in various metrics.\n“I think for a pretty early, you know, the first few years, we’ve been mostly focusing on scientific papers and research advances,” Battaglia reflected. “But, you know, as we’ve been able to show that these machine learning systems are rivaling, or even outperforming, the kind of traditional physics-based systems, having the opportunity to take them out of the sort of scientific context into the real world is really exciting.”\nThe partnership with government agencies is a crucial step toward operational deployment of AI weather systems. However, DeepMind emphasizes that Weather Lab remains a research tool, and users should continue relying on official meteorological agencies for authoritative forecasts and warnings.\nThe company plans to continue gathering feedback from weather agencies and emergency services to improve the technology’s practical applications. As climate change potentially intensifies tropical cyclone behavior, advances in prediction accuracy could prove increasingly vital for protecting vulnerable coastal populations worldwide.\n“We think AI can provide a solution here,” Alet concluded, referencing the complex interactions that make cyclone prediction so challenging. With the 2025 hurricane season underway, the real-world performance of DeepMind’s experimental system will soon face its ultimate test.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:03.202452",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:53.111264",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/51ce6e7b5d08e094b4c085ee2e9b13ad.mp3"
  },
  {
    "id": "42806faa529100d1875b179314cdb4b4",
    "title": "Out of Words is an emotional co-op adventure illustrated with beautiful stop-motion animation",
    "url": "https://venturebeat.com/games/out-of-words-is-an-emotional-co-op-adventure-illustrated-with-beautiful-stop-motion-animation/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-12T15:00:00+00:00",
    "source": "VentureBeat",
    "summary": "《Out of Words》是一款以美麗的定格動畫呈現的情感合作冒險遊戲，故事描述兩名青少年在愛情中成長，卻因失去嘴巴而無法說話，展開了一段合作平台冒險。遊戲目標是尋找木偶動畫角色失去的聲音，預計2026年在PC、PlayStation 5和Xbox Series X/S上推出。遊戲背景畫面極具畫家風格，透過線上跨平台或本地合作模式，玩家將探索一個由手工製作的繽紛世界，體驗Kurt和Karla第一次牽手的故事。",
    "content": "Out of Words is an emotional co-op adventure illustrated with beautiful stop-motion animation | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nOut of Words is an emotional co-op adventure illustrated with beautiful stop-motion animation\nDean Takahashi\n@deantak\nJune 12, 2025 8:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nThe two main characters of\nOut of Words\nare young teens who are falling in love in a coming-of-age story. But then disaster strikes as they lose their mouths. They try to talk but can’t anymore. Their mouths are sewn shut.\nAnd so begins Out of Words, a co-op platformer adventure. Of all the games that I saw at the Summer Game Fest, this one was the best. The goal of the game is to find the lost voices of the puppet-animated characters. But unfortunately, we’ll have to wait for a while.\nIt’s coming out in 2026 on the PC via the Epic Games Store PlayStation 5 and Xbox Series X/S. It’s the first game for Johan Oettinger, game director for Out of Words. The developers are Kong Orange and WiredFly, and its even got a poet, Morten Søndergaard, who penned the poems in the game.\nAt the Epic Games booth at the Summer Game Fest Play Days, I met with Oettenger and executive producer Esben Kjær Ravn. I played the game with Oettenger, and the co-op platformer adventure immediately reminded me of HazeLight’s game of the year from a few years ago,\nIt Takes Two\n. The game did its best to remind me how a great game can marry its story with its gameplay mechanics.\nOut of Words has painterly backgrounds.\nLike in HazeLight’s game, you have a couple with some troubles. In this case, they can’t speak to each other. But while It Takes Two was a split-screen game, Out of Words has a single screen for both players because, Oettenger said, the aim is to make people feel the same experience together.\nTogether with a friend, via online cross-platform or couch co-op, you can explore a wild, colorful realm as Kurt and Karla in a story about the first time they held hands — where everything you see is crafted by hand.\nOrigins\nJohan Oettinger (left) and\nEsben Kjær Ravn are making Out of Words.\nBased on an idea from Søndergaard, Oettinger of Kong Orange began working on the game about 10 years ago in 2015. But he had never made a game before and didn’t know how to do it. He only had an idea that he wanted to make a game based on hand-crafted art using stop-motion animation techniques.\nThey carefully designed and crafted each stop-motion character, environment and co-op mechanic to tell a story with a distinctly human touch.\nIt’s a journey built for two where you have to solve physics-defying puzzles in ancient catacombs and perform daring stunts among the clay skyscrapers of Nounberg. If you mess up, one of the characters dies. Then you try again. The adventure will require cooperation, communication and perfect timing between players and their co-player.\nThe story is about finding your words. The world is at stake — and so is your relationship. Kurt and\nKarla become entangled in a battle for the future of the world and their friendship itself. Traverse the wonders and dangers being Out of Words while falling in love. Kurt and Karla may save this wonderland — but at what cost?\n“It’s about love. It’s a coming-of-age story, and you play either as Kurt or Karla,” Oettinger said in our play session. “You have to play together, either online or like we are doing now, sitting next to each other, cross platform with a friends pass. The dream is to bring people together and experience it.”\nHe also said, “The game design is designed to to accommodate the story, to accommodate how the characters are feeling, and so the players will feel the same. When the camera gets close in the cut scenes, you can really sense the fidelity of the craft.”\nThe gameplay\nOut of Words is coming in 2026.\nIt’s a game with a variety of mechanics to support the story. It takes place in a fabricated world, built first in the real world and then transferred into the game. The team built characters in real life and then captured their movements through stop-motion animation.\n“Everything is touched by a human hand,” he said, as police helicopters flew over our heads in Los Angeles and sirens were going off in the distance. “It’s scanned with lighting or photogrammetry and brought into the Unreal Engine. We use classical keyframe animation, but a lot of is also animated through code.\nI picked up the controller and started to play with Johan.\n“I’ve been doing a lot of other things, but it is a dream come true. My childhood dream was to make stop motion animation,” Oettinger said.\nKarla and Kurt are in love. But they are Out of Words.\nWe started playing in the second chapter of the game, and it was like walking right into Alice in Wonderland. In the first chapter, the two friends are best friends, talking about everything and getting into a high-stakes competition. They’re coming of age, and they want to say how they feel about each other, but they can’t find the words.\n“And in this split second of doubt, they lose their mouths,” . Their lips crack off. They’re sucked into their inner world of friendship, their world, their language, which is in a bit of a crisis. They fall.”\nHe added, “Their friendship is manifested in a cute flying creature. It’s a symbol of their friendship, and that turns out to the player to be the key to changing their inner world. You realize that the characters and players go through a whole fantastical journey into a classical fairy tale.”\nThe gameplay in the second chapter is fairly mellow, where you play in a side-scrolling platformer style, moving through the landscape together. You can see the grass is painted paper when you look up close.\nCan you make it through the puzzles?\n“Everything is made by hand,” he said. “The water is a gelatin. Cliffs are stacks of book pages.”\nThen we came upon a vantage point and the clouds parted. We saw a strange city in the background.\nThen Oettinger moved us about an hour ahead in the game, deeper into the city. You meet a lot of characters along the way, like Mr. Speaker as you search for someone who can help you with your mouth and your words. The core mechanic comes through the puppet creature, Baby Moon.\nShe transfers her power to change gravity. As one player presses the B button, the player transfers gravity to the other player. So one player can walk on a roof upside down without gravity. The other with gravity can walk on the ground. If you are upside down and get the gravity thrown to you, you will fall downward. If you get rid of the gravity and transfer it to your friend, then you could fall and your friend could rise. You keep playing this way in a kind of cooperation. This way, you can get through caverns and gaps and rocky formations together.\n“You’re helping each other, touching each other. It leaves that framework pretty clear about how there is no way to progress in this game if you don’t help each other and trust each other,” Oettinger said.\nCan you walk off a cliff and trust your friend will save you?\nWe practiced for a while. You have to move to a certain spot, transfer the Baby Moon creature to the other character, move again as you fall and then catch Baby Moon as it’s returned to you so you can complete the movement and get to safety before you or the other player falls.\nMeanwhile, I could see that something dark was going on in the world. It turned out to be quite difficult to get to the city, Oettinger said. Finally, we make our way under the city into the catacombs. We moved further into the game again.\nFor the past three years, the team of around 40 people has been in fast production. They’re spending most of their time developing mechanics that they want to use to support the story.\n“That’s the biggest creative challenge,” Oettinger said. It comes together, so the narrative and the mechanic come together as one coherent thing.”\nIn a still later section, we meet Mr. Speaker, and he’s in a bad state of doubt and fear. He’s closing down the whole world to prevent it from changing. Kurt and Karla must find a type of primordial clay so they can restore their mouths. You return to the city, and it is empty and the citizens are gone. You come across a golem who wants to cross a column. You give him a nudge, and he falls and catches himself. Then he becomes a bridge you can walk across. And he sees he has found his purpose.\nMoon Baby in Out of Words.\nOettinger said he loved the games from HazeLight because it game them so much encouragement to keep making a co-op game. When those HazeLight games succeeded, it opened doors for Out of Words.\nI asked Oettinger how he connected with Epic Games. He said the team got into the Day of the Devs show and the Epic folks saw them there. They agreed to fully fund the game and give the devs total creative freedom.\n“It’s the dream. It’s my first game, and I’m spoiled to work with them. They give us the opportunity to bring something really special,” Oettinger said. “The dream came from my childhood. I really loved three things, like puppet making, stop motion animation and games.\n“If I found this kind of game when I was a child at 11, it would have meant the world to me,” he said. “I love co-op games. If I could show this game to my mother back then, and she would see why I love games and something that can be played with friends and has a heartwarming and positive tale, that would be something really I would like to bring to the world.”\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:03.361065",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:55.483481",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/42806faa529100d1875b179314cdb4b4.mp3"
  },
  {
    "id": "cdc3ac691afcd60dd5d2d04e4dc0e317",
    "title": "Towa and the Guardians of the Sacred Tree | hands-on preview",
    "url": "https://venturebeat.com/games/towa-and-the-guardians-of-the-sacred-tree-hands-on-preview/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-12T14:00:00+00:00",
    "source": "VentureBeat",
    "summary": "Bandai Namco在夏季遊戲節上展示了一款名為《Towa and the Guardians of the Sacred Tree》的遊戲，預計9月登陸主機和PC平台。遊戲背景設定在一個神秘的領域，玩家必須保護一座神樹下的村莊免受邪惡力量侵害。遊戲以2D動作冒險形式呈現，玩家需操控兩名戰士同時作戰，打造武器並結成羈絆。遊戲風格以東方神秘主義為基礎，畫面精美，動作快速刺激，是一款值得期待的rogue-lite遊戲。",
    "content": "Towa and the Guardians of the Sacred Tree | hands-on preview | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nTowa and the Guardians of the Sacred Tree | hands-on preview\nDean Takahashi\n@deantak\nJune 12, 2025 7:00 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nBandai Namco\nshowed up at the Summer Game Fest Play Days with a rogue-lite game from Brownie:\nTowa and the Guardians of the Sacred Tree\n.\nThe 2D action-adventure title debuts on the consoles and PC in September.\nIt’s a button-mashing rogue-lite set in a mystic realm where ancient forces are threatening Shinju, an idyllic village at the foot of a living sacred tree. You have to fight with a couple of warriors paired together and control both of them with a single game controller. You have to forge weapons and bonds.\nTowa is a priestess, a child of the gods and protector of the wondrous village that sits at the base of a living sacred tree. The village was her home, and she lived for ages among the villagers.\nMagatsu doesn’t like harmony.\nBut the peace and happiness were broken by the evil god Magatsu, who spread his miasma throughout the lands. And from that poisoned haze, the Maga Ori beasts emerged. The evil beasts claimed more lands and crept closer to the village.\nTowa has to recruit eight different warriors from the village — prayer children. They are granted special staffs and swords. The warriors include resolute blade master Rekka, virtuous seeker Nishiki, and a couple of others.\nBefore each quest, you choose which of them will be your Tsurugi (the lead fighter with a sword) or Kagura (the support fighter with a staff). As you enter each area, you the Maga Ori beasts materialize and you have to go on the attack. But you have to avoid damage by dashing around. So the action is very fast.\nStrike a lot and dash to escape.\nI found it moved so fast I had to concentrate and avoid panicking as the beast came after me. Using my special powers at the right moment mattered a lot. Of course, if you die, you have to start all over. It’s a rouge-lite, for sure.\nEach time you are defeated, you return to the village and can choose different warriors for your next attempt. But your progression stays with you. The priestess turns back time and enables you to try again.\nThe hand-drawn style character art and painterly backdrops are beautiful and rooted in Eastern mysticism. The music soundtrack comes from composer Hitoshi Sakimoto.\nThe game comes out on September 19 and you can pre-order it now. it will be out on the PS5, Xbox Series X/S, Steam and Nintendo Switch.\nAs you fight and level up, you can receive a Grace, like a strength boost, a drifting cloud strike, a cloud rendering slice or a drafting cloud attack.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:03.443118",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:59.275132",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/cdc3ac691afcd60dd5d2d04e4dc0e317.mp3"
  }
]