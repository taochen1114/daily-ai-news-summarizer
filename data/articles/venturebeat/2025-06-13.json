[
  {
    "id": "41ea6141f6a835a6d63ee837d3a16fe3",
    "title": "SAG-AFTRA board approves agreement with game companies on AI and new contract",
    "url": "https://venturebeat.com/games/sag-aftra-board-approves-agreement-with-game-companies-on-ai-and-new-contract/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-13T00:04:00+00:00",
    "source": "VentureBeat",
    "summary": "SAG-AFTRA的董事會批准了與遊戲公司達成的協議，涉及AI和新合同。這份新合同將提高表演者的報酬，設立了保護AI使用的規則，並確保了更好的安全條款，例如在危險情況下需要有合格醫務人員在場。這項協議還包括了對數位人物使用的最低標準和更高的報酬。這將對遊戲行業的表演者帶來更好的工作條件和報酬。",
    "content": "SAG-AFTRA board approves agreement with game companies on AI and new contract | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSAG-AFTRA board approves agreement with game companies on AI and new contract\nDean Takahashi\n@deantak\nJune 12, 2025 5:04 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nSAG-AFTRA's board has approved a deal with game companies, setting up membership vote.\nImage Credit: SAG-AFTRA\nThe\nScreen Actors Guild-American Federation of Television and Radio Artists\n(SAG-AFTRA) National Board approved the tentative agreement with the video game bargaining group.\nThe contract on terms for the Interactive Media Agreement will now be submitted to the membership for ratification.\nThe new contract accomplishes important guardrails and gains around AI, including the requirement of informed consent across various AI uses and the ability for performers to suspend informed consent for Digital Replica use during a strike.\nIf ratified, the agreement would provide compounded increases in performer compensation at a rate of 15.17% upon ratification plus additional 3% increases in November 2025, November 2026 and November 2027. Additionally, the overtime rate maximum for overscale performers will now be based on double scale. The health & retirement contribution rates to the SAG-AFTRA Health Plan will be raised from 16.5% to 17% upon ratification and to 17.5% in Oct. 2026.\nCompensation gains include the establishment of collectively-bargained minimums for the use of Digital Replicas created with IMA-covered performances and higher minimums (7.5x scale) for “Real Time Generation,” i.e., embedding a Digital Replica-voiced chatbot in a video game. “Secondary Performance Payments” will also ensure compensation when visual performances are re-used in another videogame.\nEssential new safety provisions were also secured, including a requirement for a qualified medical professional to be present or readily available at rehearsals and performances during which hazardous actions or working conditions are planned. Rest periods are now provided for on-camera principal performers and employers can no longer request that performers complete stunts or other dangerous activity in virtual auditions.\nThe spokesperson for the video game producers party to the Interactive Media Agreement, Audrey Cooling, said earlier this week in a statement, “We are pleased to have reached a tentative contract agreement that reflects the important contributions of SAG-AFTRA-represented performers in video games. This agreement builds on three decades of successful partnership between the interactive entertainment industry and the union.”\nCooling added, “It delivers historic wage increases of over 24% for performers, enhanced health and safety protections, and industry-leading AI provisions requiring transparency, consent and compensation for the use of digital replicas in games. We look forward to continuing to work with performers to create new and engaging entertainment experiences for billions of players throughout the world.”\nThe full terms of the three-year deal will be released with the ratification materials on Wednesday, June 18.\nA tentative agreement was reached with the video game employers on June 9 and the strike was officially suspended on June 11.\nMember informational meetings are being scheduled and additional details will be available at\nsagaftra.org/videogames2025\nin the coming days.\nEligible SAG-AFTRA members will have until 5 p.m. PDT on Wednesday, July 9, 2025 to cast their vote on ratification.\nSAG-AFTRA represents approximately 160,000 actors, announcers, broadcast journalists, dancers, DJs, news writers, news editors, program hosts, puppeteers, recording artists, singers, stunt performers, voiceover artists and other entertainment and media professionals.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:02.327130",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:35.775026",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/41ea6141f6a835a6d63ee837d3a16fe3.mp3",
    "audio_file": "audio/articles/41ea6141f6a835a6d63ee837d3a16fe3.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-13T11:59:17.642643"
  },
  {
    "id": "1d10958faf07322d96af124a9be2ca70",
    "title": "Meta’s new world model lets robots manipulate objects in environments they’ve never encountered before",
    "url": "https://venturebeat.com/ai/metas-new-world-model-lets-robots-manipulate-objects-in-environments-theyve-never-encountered-before/",
    "authors": "Ben Dickson",
    "published_date": "2025-06-12T22:22:07+00:00",
    "source": "VentureBeat",
    "summary": "Meta最新的世界模型讓機器人能夠在從未遇過的環境中操作物件。這個模型可以幫助AI應用在不可預測的環境中預測結果並計劃行動，為製造和物流等領域帶來更多可能性。這個技術讓機器人能夠像人類一樣觀察世界並做出反應，是AI在實際環境中更進一步發展的重要一步。",
    "content": "Meta's new world model lets robots manipulate objects in environments they've never encountered before | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nMeta’s new world model lets robots manipulate objects in environments they’ve never encountered before\nBen Dickson\n@BenDee983\nJune 12, 2025 3:22 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage credit: VentureBeat with Gemini\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nWhile large language models (LLMs) have mastered text (and other modalities to some extent), they lack the physical “common sense” to operate in dynamic, real-world environments. This has limited the deployment of AI in areas like manufacturing and logistics, where understanding cause and effect is critical.\nMeta’s latest model,\nV-JEPA 2\n, takes a step toward bridging this gap by learning a world model from video and physical interactions.\nV-JEPA 2 can help create AI applications that require predicting outcomes and planning actions in unpredictable environments with many edge cases. This approach can provide a clear path toward more capable robots and advanced automation in physical environments.\nHow a ‘world model’ learns to plan\nHumans develop physical intuition early in life by observing their surroundings. If you see a ball thrown, you instinctively know its trajectory and can predict where it will land. V-JEPA 2 learns a similar “world model,” which is an AI system’s internal simulation of how the physical world operates.\nmodel is built on three core capabilities that are essential for enterprise applications: understanding what is happening in a scene, predicting how the scene will change based on an action, and planning a sequence of actions to achieve a specific goal. As Meta states in its\nblog\n, its “long-term vision is that world models will enable AI agents to plan and reason in the physical world.”\nThe model’s architecture, called the Video Joint Embedding Predictive Architecture (V-JEPA), consists of two key parts. An “encoder” watches a video clip and condenses it into a compact numerical summary, known as an\nembedding\n. This embedding captures the essential information about the objects and their relationships in the scene. A second component, the “predictor,” then takes this summary and imagines how the scene will evolve, generating a prediction of what the next summary will look like.\nV-JEPA is composed of an encoder and a predictor (source: Meta blog)\nThis architecture is the latest evolution of the JEPA framework, which was first applied to images with\nI-JEPA\nand now advances to video, demonstrating a consistent approach to building world models.\nUnlike generative AI models that try to predict the exact color of every pixel in a future frame — a computationally intensive task — V-JEPA 2 operates in an abstract space. It focuses on predicting the high-level features of a scene, such as an object’s position and trajectory, rather than its texture or background details, making it far more efficient than other larger models at just 1.2 billion parameters\nThat translates to lower compute costs and makes it more suitable for deployment in real-world settings.\nLearning from observation and action\nV-JEPA 2 is trained in two stages. First, it builds its foundational understanding of physics through\nself-supervised learning\n, watching over one million hours of unlabeled internet videos. By simply observing how objects move and interact, it develops a general-purpose world model without any human guidance.\nIn the second stage, this pre-trained model is fine-tuned on a small, specialized dataset. By processing just 62 hours of video showing a robot performing tasks, along with the corresponding control commands, V-JEPA 2 learns to connect specific actions to their physical outcomes. This results in a model that can plan and control actions in the real world.\nV-JEPA two-stage training pipeline (source: Meta)\nThis two-stage training enables a critical capability for real-world automation: zero-shot robot planning. A robot powered by V-JEPA 2 can be deployed in a new environment and successfully manipulate objects it has never encountered before, without needing to be retrained for that specific setting.\nThis is a significant advance over previous models that required training data from the\nexact\nrobot and environment where they would operate. The model was trained on an open-source dataset and then successfully deployed on different robots in Meta’s labs.\nFor example, to complete a task like picking up an object, the robot is given a goal image of the desired outcome. It then uses the V-JEPA 2 predictor to internally simulate a range of possible next moves. It scores each imagined action based on how close it gets to the goal, executes the top-rated action, and repeats the process until the task is complete.\nUsing this method, the model achieved success rates between 65% and 80% on pick-and-place tasks with unfamiliar objects in new settings.\nReal-world impact of physical reasoning\nThis ability to plan and act in novel situations has direct implications for business operations. In logistics and manufacturing, it allows for more adaptable robots that can handle variations in products and warehouse layouts without extensive reprogramming. This can be especially useful as companies are exploring the deployment of\nhumanoid robots\nin factories and assembly lines.\nThe same world model can power highly realistic digital twins, allowing companies to simulate new processes or train other AIs in a physically accurate virtual environment. In industrial settings, a model could monitor video feeds of machinery and, based on its learned understanding of physics, predict safety issues and failures before they happen.\nThis research is a key step toward what Meta calls “advanced machine intelligence (AMI),” where AI systems can “learn about the world as humans do, plan how to execute unfamiliar tasks, and efficiently adapt to the ever-changing world around us.”\nMeta has released the model and its training code and hopes to “build a broad community around this research, driving progress toward our ultimate goal of developing world models that can transform the way AI interacts with the physical world.”\nWhat it means for enterprise technical decision-makers\nV-JEPA 2 moves robotics closer to the software-defined model that cloud teams already recognize: pre-train once, deploy anywhere. Because the model learns general physics from public video and only needs a few dozen hours of task-specific footage, enterprises can slash the data-collection cycle that typically drags down pilot projects. In practical terms, you can prototype a pick-and-place robot on an affordable desktop arm, then roll the same policy onto an industrial rig on the factory floor without gathering thousands of fresh samples or writing custom motion scripts.\nLower training overhead also reshapes the cost equation. At 1.2 billion parameters, V-JEPA 2 fits comfortably on a single high-end GPU, and its abstract prediction targets reduce inference load further. That lets teams run closed-loop control on-prem or at the edge, avoiding cloud latency and the compliance headaches that come with streaming video outside the plant. Budget that once went to massive compute clusters can fund extra sensors, redundancy, or faster iteration cycles instead.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:02.483371",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:37.907059",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/1d10958faf07322d96af124a9be2ca70.mp3",
    "audio_file": "audio/articles/1d10958faf07322d96af124a9be2ca70.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-13T11:59:23.100025"
  },
  {
    "id": "f1da1ca3ea421eab33c45699a0ed79fd",
    "title": "Cloud collapse: Replit and LlamaIndex knocked offline by Google Cloud identity outage",
    "url": "https://venturebeat.com/ai/cloud-collapse-replit-llamaindex-knocked-offline-by-google-cloud-identity-outage/",
    "authors": "Emilia David",
    "published_date": "2025-06-12T22:05:33+00:00",
    "source": "VentureBeat",
    "summary": "Google Cloud的身份驗證故障導致Replit和LlamaIndex無法運作，影響了許多AI開發工具和資料存儲服務。這次故障也讓一些AI平台像是ChatGPT和Claude無法運作。雖然影響範圍廣泛，但Google的一些服務仍持續運作。故障影響了API Gateway、Cloud Data Fusion、Google Cloud Storage等服務，也讓Google的移動開發平台Firebase癱瘓。Cloudflare表示受影響的服務數量有限，預計很快就會恢復正常。",
    "content": "Cloud collapse: Replit and LlamaIndex knocked offline by Google Cloud identity outage | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nCloud collapse: Replit and LlamaIndex knocked offline by Google Cloud identity outage\nEmilia David\n@miyadavid\nJune 12, 2025 3:05 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat, generated with ChatGPT\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nDays after\nOpenAI\nand\nGoogle Cloud\nannounced a partnership to support the growing use of generative AI platforms, much of the AI-powered web and tools went down due to an outage of the leading cloud providers.\nGoogle Cloud Service Platform (GCP) and some\nCloudflare\nservices\nbegan experiencing\nissues\naround 10:00 a.m. PT\ntoday, affecting several AI development tools and data storage services, including ChatGPT and Claude, as well as a variety of other AI platforms.\nWe are aware of a service disruption to some Google Cloud services and we are working hard to get you back up and running ASAP.\nPlease view our status dashboard for the latest updates:\nhttps://t.co/sT6UxoRK4R\n— Google Cloud (@googlecloud)\nJune 12, 2025\nA GCP spokesperson confirmed the outage to VentureBeat, urging users to check its public status dashboard.\nGCP said affected services include API Gateway, Agent Assist, Cloud Data Fusion, Contact Center AI Platform, Google App Engine, Google BigQuery, Google Cloud Storage, Identity Platform, Speech-to-Text, Text-to-Speech and Vertex AI Search, among other tools. Google’s mobile development platform, Firebase, also\nwent down\n.\nVentureBeat staffers had trouble accessing Google Meet, but other Google services on Workspace remained online.\nA Cloudflare spokesperson told VentureBeat only “a limited number of services at Cloudflare use Google Cloud and were impacted. We expect them to come back shortly. The core Cloudflare services were not impacted.”\nDespite media reports and user-provided feedback on Down Detector,\nAWS\nstated that\nits service remains up\n, including AI platforms such as Bedrock and Sagemaker.\nOpenAI acknowledged some users had issues logging into their platforms but have\nsince resolved the problem\n.\nAnthropic\nnoted on its\nstatus page\nthat Claude experienced “elevated error rates on the API, console and Claude AI.”\nWe are aware of issues affecting multiple external internet providers, impacting the availability of our services such as single sign-on (SSO) and other log-in methods. Our engineering teams are working to mitigate these issues.\nThank you for your continued patience.\nFor the…\n— OpenAI (@OpenAI)\nJune 12, 2025\nDeveloper tools like\nLlamaIndex\n’s LlamaCloud,\nWeights & Biases\n,\nWindsurf\n,\nSupabase\nand\nReplit\nreported issues. Other platforms like\nCharacter AI\nalso\nannounced\nthey were affected.\nHi folks – LlamaCloud (\nhttps://t.co/DHMd6BFO0l\n) is currently down due to the ongoing global AWS/GCP/Firebase outage.\nWe are closely monitoring the solution and will keep you posted when it's resolved!\n— Jerry Liu (@jerryjliu0)\nJune 12, 2025\n? We're aware of the Google Cloud outage affecting various web services, including Weights & Biases products like W&B Models and\n@weave_wb\n.\nOur team is monitoring the situation and will provide updates.\nThank you for your patience.\n— Weights & Biases (@weights_biases)\nJune 12, 2025\nOur upstream cloud providers are currently experiencing a major outage. We are working as best we can to restore Replit services.\n— Replit ⠕ (@Replit)\nJune 12, 2025\nIn addition to AI tools, other websites and internet services, such as Spotify and Discord, also reportedly went down.\nNeeding more cloud\nIn many ways, the outage highlights the challenges of relying on a single cloud service or database provider and the risks associated with an interconnected Internet. If one of your cloud services goes down, it could impact some users whose log-in or data stream is hosted there.\nGoogle Cloud has been gradually wresting\nmarket leadership in enterprise AI\nfrom its competitors, thanks to the large number of developer and database tools it has begun offering organizations. On the other hand, Cloudflare has been\npartnering with companies\nlike Hugging Face to deploy AI apps faster.\nFirst reported by\nReuters\n, Google and OpenAI have struck a deal that will allow OpenAI to utilize Google Cloud to meet the growing demand on its platform.\nBut that’s not to say Google or Cloudflare may lose an edge among enterprise AI users who depend on consistent uptime. While the company continues to investigate the cause of the outage, enterprises often have, and should have, redundancies in case their provider goes down. Outages happen, and they happen far too frequently.\nThe last massive outage happened around the same time last year, in July, when CrowdStrike\naccidentally triggered outages\nthat impacted Microsoft Windows users.\nIn typical fashion, many people saw the outages as an opportunity for comedy, or at least to catch up on tasks they’d been putting off.\nmuch of the AI internet is down now\nfirebase is down, cursor is down, lovable is down, supabase is down, google ai is down, cursor is down, aws is down… almost everything is down.\nfinally time to catch up on the 87 tools, 14 models, and 12 AI startup ideas we want to build.\n— GREG ISENBERG (@gregisenberg)\nJune 12, 2025\nThank GCP.I couldn’t find a reason to dip out of a couple meetings this afternoon and now I do!\n—\nAdam (@adambahm.bsky.social)\n2025-06-12T19:38:07.612Z\nSo yes, it seems like the digital universe is giving everyone a forced break today!\n— Murugan (MGA) (@murugan_mga)\nJune 12, 2025\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-13T02:11:02.627634",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-13T02:11:41.643893",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/f1da1ca3ea421eab33c45699a0ed79fd.mp3",
    "audio_file": "audio/articles/f1da1ca3ea421eab33c45699a0ed79fd.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-13T12:05:01.795928"
  }
]