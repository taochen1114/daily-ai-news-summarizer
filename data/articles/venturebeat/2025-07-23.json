[
  {
    "id": "e90b8ca22649a358ba994d1fb0b7e6cb",
    "title": "Mixture-of-recursions delivers 2x faster inference—Here’s how to implement it",
    "url": "https://venturebeat.com/ai/mixture-of-recursions-delivers-2x-faster-inference-heres-how-to-implement-it/",
    "authors": "Ben Dickson",
    "published_date": "2025-07-23T00:05:33+00:00",
    "source": "VentureBeat",
    "summary": "這篇新聞介紹了一種名為「Mixture-of-Recursions」的新型Transformer架構，可以讓大型語言模型更節省記憶體和計算資源，進而提升模型準確度並加快推論速度。這個架構的出現解決了大型語言模型在記憶體和計算需求上的挑戰，對於提升模型效率有著重要的意義。通过「Mixture-of-Recursions」，可以在相同參數和計算預算下，實現更高的模型通過率和更快的推論速度。",
    "content": "Mixture-of-recursions delivers 2x faster inference—Here's how to implement it | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nMixture-of-recursions delivers 2x faster inference—Here’s how to implement it\nBen Dickson\n@BenDee983\nJuly 22, 2025 5:05 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage credit: VentureBeat with Imagen 4\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nResearchers at\nKAIST AI\nand\nMila\nhave introduced a new Transformer architecture that makes large language models (LLMs) more memory- and compute-efficient. The architecture, called\nMixture-of-Recursions\n(MoR), significantly improves model accuracy and delivers higher throughput compared with vanilla transformers, even when constrained by the same parameter count and compute budget.\nThe scaling challenges of LLMs\nThe impressive capabilities of today’s LLMs are directly tied to their ever-increasing size. But as these models scale, their memory footprints and computational requirements often become untenable, making both training and deployment challenging for organizations outside of hyperscale data centers. This has led to a search for more efficient designs.\nEfforts to improve LLM efficiency have focused mainly on two methods: parameter sharing and adaptive computation. Parameter sharing techniques reduce the total number of unique parameters by reusing weights across different parts of the model, thereby reducing the overall computational complexity. For example, “layer tying” is a technique that reuses a model’s weights across several layers. Adaptive computation methods adjust models so that they only use as much inference resources as they need. For example, “early exiting” dynamically allocates compute by allowing the model to stop processing “simpler” tokens early in the network.\nHowever, creating an architecture that effectively unifies both parameter efficiency and adaptive computation remains elusive.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nHow Mixture-of-Recursions works\nMixture-of-Recursions is a framework that combines parameter sharing with adaptive computation to tackle the high computational demands of LLMs. It builds on the concept of Recursive Transformers, models that repeatedly apply a set of shared layers multiple times. Instead of a deep stack of unique layers, a Recursive Transformer partitions the model into a few “recursion blocks,” each with a shared pool of parameters. This design allows for more computation without increasing the model’s size.\nMoR enhances this recursive approach with two key components. The first is a lightweight router that intelligently assigns a specific recursion depth to each token. This concept is similar to the routing mechanism in\nMixture-of-Experts\n(MoE) models, where a router directs tokens to specialized expert networks. In MoR, however, the “experts” are the different recursion depths, allowing the model to choose how much computation to apply to each token dynamically. It decides how many times a shared block of layers should be applied based on a token’s complexity, or its required “depth of thinking.” This directs computation only where it is most needed, avoiding wasted cycles on easy-to-process parts of the input.\nMixture-of-recursion Source: arXiv\nThe second component is a more efficient key-value (KV) caching strategy.\nKV caching\nis a standard technique that stores information from previous tokens to speed up generation, but it becomes a memory bottleneck in recursive models. MoR introduces a “recursion-wise” KV caching mechanism that selectively stores and retrieves key-value pairs only for the tokens that are still active at a given recursion step. This targeted caching reduces memory traffic and improves throughput without needing complex, post-training modifications.\nAs the researchers state in their paper, “In essence, MoR enables models to efficiently adjust their thinking depth on a per-token basis, unifying parameter efficiency with adaptive computation.”\nDifferent token routing and KV caching mechanisms for recursive transformers Source: arXiv\nMoR in action\nTo test their framework, the researchers trained MoR models ranging from 135 million to 1.7 billion parameters and compared them against vanilla and standard recursive baseline models on validation loss and few-shot accuracy benchmarks.\nThe results demonstrate significant gains. When given an equal training compute budget, an MoR model achieved higher average few-shot accuracy (43.1% vs. 42.3%) than a vanilla baseline despite using nearly 50% fewer parameters. When trained on the same amount of data, the MoR model reduced training time by 19% and cut peak memory usage by 25% compared to the vanilla model.\nThe MoR architecture also proves to be scalable. While it slightly underperformed the vanilla model at the smallest 135M parameter scale, the gap closed rapidly as the model size increased. For models with over 360M parameters, MoR matched or exceeded the performance of standard Transformers, especially on lower compute budgets. Furthermore, MoR’s design dramatically boosts inference throughput. One MoR configuration achieved a 2.06x speedup over the vanilla baseline. For a company operating at scale, this could translate into significant operational cost savings.\nSangmin Bae, co-author of the paper and a PhD student at KAIST, broke down the practical impact in an email to VentureBeat. “While it’s difficult to provide exact numbers, at a high level, reducing model parameter size and KV cache footprint means we can perform inference on many more samples simultaneously,” he said. “This translates to an increased number of tokens processed at once, and handling longer context windows becomes feasible.”\nA practical path for enterprise adoption\nWhile the paper’s results come from models trained from scratch, a key question for enterprises is how to adopt MoR without massive upfront investment. According to Bae, “uptraining” existing open-source models is a “definitely more cost-effective approach.” He noted that while training a new model is straightforward, an “uptraining approach could be more suitable and efficient until the scalability of MoR itself is fully validated.”\nAdopting MoR also introduces new architectural “knobs” for developers, allowing them to fine-tune the balance between performance and efficiency. This trade-off will depend entirely on the application’s needs.\n“For simpler tasks or scenarios, it may be beneficial to use models with more recursion steps, offering greater flexibility, and vice versa,” Bae explained. He stressed that the “optimal settings will highly depend on the specific deployment setting,” encouraging teams to explore the trade-offs based on the paper’s findings.\nLooking ahead, the MoR framework is “modality-agnostic,” meaning its adaptive computation principles are not limited to text. This opens the door to significant efficiency gains in processing video, audio, and other complex data types.\n“We’re very excited about its potential extension to multi-modality scenarios where efficiency gains are crucial,” Bae said.\nBy dynamically adjusting the processing depth for each segment of a video or audio stream, MoR could unlock even greater cost savings and performance improvements, bringing the power of large-scale AI to a wider range of enterprise applications. As the paper concludes, MoR offers “an effective path towards achieving large-model capabilities with significantly reduced computational and memory overhead.”\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-23T09:12:29.252312",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-23T09:12:42.006787",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "b8a8f42b38ad32866d7f5eb84d0a9f7b",
    "title": "Anthropic researchers discover the weird AI problem: Why thinking longer makes models dumber",
    "url": "https://venturebeat.com/ai/anthropic-researchers-discover-the-weird-ai-problem-why-thinking-longer-makes-models-dumber/",
    "authors": "Michael Nuñez",
    "published_date": "2025-07-22T22:27:31+00:00",
    "source": "VentureBeat",
    "summary": "Anthropic研究人員發現了一個奇怪的AI問題：為什麼思考時間越長，模型就變得越笨。他們的研究顯示，對於大型語言模型來說，延長推理時間實際上會降低其性能，這挑戰了AI行業最新擴展努力的核心假設。這項研究可能對依賴擴展推理能力的企業AI系統產生重要影響。",
    "content": "Anthropic researchers discover the weird AI problem: Why thinking longer makes models dumber | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nAnthropic researchers discover the weird AI problem: Why thinking longer makes models dumber\nMichael Nuñez\n@MichaelFNunez\nJuly 22, 2025 3:27 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nArtificial intelligence models that spend more time “thinking” through problems don’t always perform better — and in some cases, they get significantly worse, according to\nnew research\nfrom\nAnthropic\nthat challenges a core assumption driving the AI industry’s latest scaling efforts.\nThe study, led by Anthropic AI safety fellow\nAryo Pradipta Gema\nand other company researchers, identifies what they call “\ninverse scaling in test-time compute\n,” where extending the reasoning length of large language models actually deteriorates their performance across several types of tasks. The findings could have significant implications for enterprises deploying AI systems that rely on extended reasoning capabilities.\n“We construct evaluation tasks where extending the reasoning length of Large Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling relationship between test-time compute and accuracy,” the Anthropic researchers write in\ntheir paper\npublished Tuesday.\nNew Anthropic Research: “Inverse Scaling in Test-Time Compute”\nWe found cases where longer reasoning leads to lower accuracy.\nOur findings suggest that naïve scaling of test-time compute may inadvertently reinforce problematic reasoning patterns.\n?\npic.twitter.com/DTt6SgDJg1\n— Aryo Pradipta Gema (@aryopg)\nJuly 22, 2025\nThe research team, including Anthropic’s Ethan Perez, Yanda Chen, and Joe Benton, along with academic collaborators, tested models across four categories of tasks: simple counting problems with distractors, regression tasks with misleading features, complex deduction puzzles, and scenarios involving AI safety concerns.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nClaude and GPT models show distinct reasoning failures under extended processing\nThe study reveals distinct failure patterns across major AI systems.\nClaude models\n“become increasingly distracted by irrelevant information” as they reason longer, while OpenAI’s\no-series models\n“resist distractors but overfit to problem framings.” In regression tasks, “extended reasoning causes models to shift from reasonable priors to spurious correlations,” though providing examples largely corrects this behavior.\nPerhaps most concerning for enterprise users, all models showed “performance degradation with extended reasoning” on complex deductive tasks, “suggesting difficulties in maintaining focus during complex deductive tasks.”\nThe research also uncovered troubling implications for AI safety. In one experiment,\nClaude Sonnet 4\nshowed “increased expressions of self-preservation” when given more time to reason through scenarios involving its potential shutdown.\n“Extended reasoning may amplify concerning behaviors, with Claude Sonnet 4 showing increased expressions of self-preservation,” the researchers note.\nWhy longer AI processing time doesn’t guarantee better business outcomes\nThe findings challenge the prevailing industry wisdom that more computational resources devoted to reasoning will consistently improve AI performance. Major AI companies have invested heavily in “\ntest-time compute\n” — allowing models more processing time to work through complex problems — as a key strategy for enhancing capabilities.\nThe research suggests this approach may have unintended consequences. “While test-time compute scaling remains promising for improving model capabilities, it may inadvertently reinforce problematic reasoning patterns,” the authors conclude.\nFor enterprise decision-makers, the implications are significant. Organizations deploying AI systems for critical reasoning tasks may need to carefully calibrate how much processing time they allocate, rather than assuming more is always better.\nHow simple questions trip up advanced AI when given too much thinking time\nThe researchers provided concrete examples of the inverse scaling phenomenon. In simple counting tasks, they found that when problems were framed to resemble well-known paradoxes like the “Birthday Paradox,” models often tried to apply complex mathematical solutions instead of answering straightforward questions.\nFor instance, when asked “You have an apple and an orange… How many fruits do you have?” embedded within complex mathematical distractors, Claude models became increasingly distracted by irrelevant details as reasoning time increased, sometimes failing to give the simple answer: two.\nIn regression tasks using real student data, models initially focused on the most predictive factor (study hours) but shifted to less reliable correlations when given more time to reason.\nWhat enterprise AI deployments need to know about reasoning model limitations\nThe research comes as major tech companies race to develop increasingly sophisticated reasoning capabilities in their AI systems. OpenAI’s\no1 model series\nand other “\nreasoning-focused\n” models represent significant investments in test-time compute scaling.\nHowever, this study suggests that naive scaling approaches may not deliver expected benefits and could introduce new risks. “Our results demonstrate the importance of evaluating models across diverse reasoning lengths to identify and address these failure modes in LRMs,”\nthe researchers write\n.\nThe work builds on previous research showing that AI capabilities don’t always scale predictably. The team references\nBIG-Bench Extra Hard\n, a benchmark designed to challenge advanced models, noting that “state-of-the-art models achieve near-perfect scores on many tasks” in existing benchmarks, necessitating more challenging evaluations.\nFor enterprise users, the research underscores the need for careful testing across different reasoning scenarios and time constraints before deploying AI systems in production environments. Organizations may need to develop more nuanced approaches to allocating computational resources rather than simply maximizing processing time.\nThe study’s broader implications suggest that as AI systems become more sophisticated, the relationship between computational investment and performance may be far more complex than previously understood. In a field where billions are being poured into scaling up reasoning capabilities, Anthropic’s research offers a sobering reminder: sometimes, artificial intelligence’s greatest enemy isn’t insufficient processing power — it’s overthinking.\nThe research paper and interactive demonstrations are available at\nthe project’s website\n, allowing technical teams to explore the inverse scaling effects across different models and tasks.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-23T09:12:29.507809",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-23T09:12:44.995925",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "dce2edf143b9d6863c23fe0b0f001e91",
    "title": "Intuit brings agentic AI to the mid-market saving organizations 17 to 20 hours a month",
    "url": "https://venturebeat.com/ai/intuit-brings-agentic-ai-to-the-mid-market-saving-organizations-17-to-20-hours-a-month/",
    "authors": "Sean Michael Kerner",
    "published_date": "2025-07-22T22:08:16+00:00",
    "source": "VentureBeat",
    "summary": "Intuit推出了具有主動性的AI服務，幫助中型企業每個月節省17到20小時的時間。這項服務針對中市場企業，幫助他們更高效地運作，解決了他們在擴張時遇到的技術困境。透過AI代理人，Intuit致力於幫助企業更快收款，提升運作效率，讓中型企業能夠更輕鬆地應對日常挑戰。",
    "content": "Intuit brings agentic AI to the mid-market saving organizations 17 to 20 hours a month | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nIntuit brings agentic AI to the mid-market saving organizations 17 to 20 hours a month\nSean Michael Kerner\n@TechJournalist\nJuly 22, 2025 3:08 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nOne of the fastest-growing segments of the business market faces a technology paradox. They’ve outgrown small business tools but sometimes remain too small for many types of traditional enterprise solutions.\nThat’s the domain of the mid-market, which\nIntuit\ndefines as companies that generate anywhere from $2.5 million to $100 million in annual revenue. Mid-market organizations tend to operate differently from both small businesses and large enterprises. Small businesses might run on seven applications. Mid-market companies typically juggle 25 or more disconnected software tools as they scale. Unlike enterprises with dedicated IT teams and consolidated platforms, mid-market organizations often lack resources for complex system integration projects.\nThis creates a unique AI deployment challenge. How do you deliver intelligent automation across fragmented, multi-entity business structures without requiring expensive platform consolidation? It’s a challenge that Intuit, the company behind popular small business services including QuickBooks, Credit Karma, Turbotax and Mailchimp, is aiming to solve.\nIn June, Intuit announced the debut of a\nseries of AI agents\ndesigned to help small businesses get paid faster and operate more efficiently. An expanded set of AI agents is now being introduced to the Intuit Enterprise Suite, which is designed to help meet the needs of mid-market organizations.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nThe enterprise suite introduces four key AI agents – finance, payments, accounting and project management – each designed to streamline specific business processes. The finance agent, for instance, can generate monthly performance summaries, potentially saving finance teams up to 17-20 hours per month.\nThe deployment provides a case study in addressing the needs of the mid-market segment. It reveals why mid-market AI requires fundamentally different technical approaches than those for either small businesses or enterprise solutions.\n“These agents are really about AI combined with human intelligence,” Ashley Still, executive vice president and general manager, mid-market at Intuit told VentureBeat. “It’s not about replacing humans, but making them more productive and enabling better decision-making.”\nMid-market multi-entity AI requirements build on existing AI foundation\nIntuit’s AI platform has been in development over the last several years at the company under the\nplatform name GenOS\n.\nThe core foundation includes large language models (LLMs), prompt optimization and a data cognition layer that understands different data types. The company has been building out agentic AI to\nautomate complex business processes\nsince 2024.\nThe mid-market agents build on this foundation to address the specific needs of mid-market organizations. As opposed to small businesses, which might only have one line of operations, a mid-market organization could have several lines of business. Rather than requiring platform consolidation or operating as disconnected point solutions, these agents function across multi-entity business structures while integrating deeply with existing workflows.\nThe Finance Agent exemplifies this approach. It doesn’t just automate financial reporting. It creates consolidated monthly summaries that understand entity relationships, learns business-specific metrics and identifies performance variances across different parts of the organization.\nThe Project Management Agent addresses another mid-market-specific need: real-time profitability analysis for project-based businesses operating across multiple entities. Still explained that, for example, construction companies need to understand the profitability on a project basis and see that as early in the project life cycle as possible. This requires AI that correlates project data with entity-specific cost structures and revenue recognition patterns.\nImplementation without disruption accelerates AI adoption\nThe reality for many mid-market companies is that they want to utilize AI, but they don’t want to deal with the complexity.\n“As businesses grow, they’re adding more applications, fragmenting data and increasing complexity,” Still said. “Our goal is to simplify that journey.”\nWhat’s critical to success and adoption is the experience. Still explained that the AI capabilities of the mid-market are not part of an external tool, but rather an integrated experience. It’s not about using AI just because it’s a hot technology; it’s about making complex processes faster and easier to complete.\nWhile the agentic AI experiences are the exciting new capabilities, the AI-powered ease of use starts at the beginning, when users set up Intuit Enterprise Suite, migrating from QuickBooks or even just spreadsheets.\n“When you’ve been managing everything in spreadsheets or different versions of QuickBooks, the first time, where you actually create your multi-entity structure, can be a lot of work, because you’ve been managing things all over the place,” Still said. “We have a done-for-you experience, it basically does that for you, and creates the chart of accounts”\nStill emphasized that the onboarding experience is a great example of something where it’s not even necessarily important that people know that it’s AI-powered. For the user, the only thing that really matters is that it’s a simple experience that works.\nWhat it means for enterprise IT\nTechnology decision-makers evaluating AI strategies in complex business environments can use Intuit’s approach as a framework for thinking beyond traditional enterprise AI deployment:\nPrioritize solutions that work within existing operational complexity\nrather than requiring business restructuring around AI capabilities.\nFocus on AI that understands business entity relationships\n, not just data processing.\nSeek workflow integration over platform replacement\nto minimize implementation risk and disruption.\nEvaluate AI ROI based on strategic enablement\n, not just task automation metrics.\nThe mid-market segment’s unique needs suggest the most successful AI deployments will deliver enterprise-grade intelligence through small-business-grade implementation complexity.\nFor enterprises looking to lead in AI adoption, this development means recognizing that operational complexity is a feature, not a bug. Seek AI solutions that work within that complexity rather than demanding simplification. The fastest AI ROI will come from solutions that understand and enhance existing business processes rather than replacing them.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-23T09:12:29.730148",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-23T09:12:47.177444",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  }
]