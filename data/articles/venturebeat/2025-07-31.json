[
  {
    "id": "218bdc5bc68235c2d39495a55a7fab5a",
    "title": "LangChain’s Align Evals closes the evaluator trust gap with prompt-level calibration",
    "url": "https://venturebeat.com/ai/langchains-align-evals-closes-the-evaluator-trust-gap-with-prompt-level-calibration/",
    "authors": "Emilia David",
    "published_date": "2025-07-30T23:28:09+00:00",
    "source": "VentureBeat",
    "summary": "LangChain的Align Evals通過即時校準，消除評估者信任差距，讓企業更準確評估AI模型。這個新功能可以讓使用者建立自己的基於大型語言模型的評估器，並校準以符合公司偏好。LangChain是少數將LLM作為評判標準直接整合到測試儀表板的平台之一。這項創新有助於減少評估分數與人工評估之間的不一致，提高評估準確性，節省時間。",
    "content": "LangChain’s Align Evals closes the evaluator trust gap with prompt-level calibration | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nLangChain’s Align Evals closes the evaluator trust gap with prompt-level calibration\nEmilia David\n@miyadavid\nJuly 30, 2025 4:28 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: Image generated by VentureBeat with Stable Diffusion 3.5 Large\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nAs enterprises increasingly turn to AI models to ensure their applications function well and are reliable, the gaps between model-led evaluations and human evaluations have only become clearer.\nTo combat this,\nLangChain\nadded Align Evals to LangSmith, a way to bridge the gap between large language model-based evaluators and human preferences and reduce noise. Align Evals enables LangSmith users to create their own LLM-based evaluators and calibrate them to align more closely with company preferences.\n“But, one big challenge we hear consistently from teams is: ‘Our evaluation scores don’t match what we’d expect a human on our team to say.’ This mismatch leads to noisy comparisons and time wasted chasing false signals,” LangChain said\nin a blog post\n.\nLangChain is one of the few platforms to integrate LLM-as-a-judge, or model-led evaluations for other models, directly into the testing dashboard.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nThe company said that it based Align Evals on a paper by Amazon principal applied scientist Eugene Yan. In his\npaper\n, Yan laid out the framework for an app, also called AlignEval, that would automate parts of the evaluation process.\nAlign Evals would allow enterprises and other builders to iterate on evaluation prompts, compare alignment scores from human evaluators and LLM-generated scores and to a baseline alignment score.\nLangChain said Align Evals “is the first step in helping you build better evaluators.” Over time, the company aims to integrate analytics to track performance and automate prompt optimization, generating prompt variations automatically.\nHow to start\nUsers will first identify evaluation criteria for their application. For example, chat apps generally require accuracy.\nNext, users have to select the data they want for human review. These examples must demonstrate both good and bad aspects so that human evaluators can gain a holistic view of the application and assign a range of grades. Developers then have to manually assign scores for prompts or task goals that will serve as a benchmark.\nThis is one of my favorite features that we've launched!\nCreating LLM-as-a-Judge evaluators is hard – this hopefully makes that flow a bit easier\nI believe in this flow so much I even recorded a video around it!\nhttps://t.co/FlPOJcko12\nhttps://t.co/wAQpYZMeov\n— Harrison Chase (@hwchase17)\nJuly 30, 2025\nDevelopers then need to create an initial prompt for the model evaluator and iterate using the alignment results from the human graders.\n“For example, if your LLM consistently over-scores certain responses, try adding clearer negative criteria. Improving your evaluator score is meant to be an iterative process. Learn more about best practices on iterating on your prompt in our docs,” LangChain said.\nGrowing number of LLM evaluations\nIncreasingly, enterprises are\nturning to evaluation frameworks\nto assess the\nreliability, behavior, task alignment and auditability of AI systems, including applications and agents. Being able to point to a clear score of how models or agents perform provides organizations not just the confidence to deploy AI applications, but also makes it easier to compare other models.\nCompanies like\nSalesforce\nand\nAWS\nbegan offering ways for customers to judge performance. Salesforce’s\nAgentforce 3\nhas a command center that shows agent performance. AWS provides both human and automated evaluation on the\nAmazon Bedrock platform\n, where users can choose the model to test their applications on, though these are not user-created model evaluators.\nOpenAI\nalso offers model-based evaluation.\nMeta\n’s\nSelf-Taught Evaluator\nbuilds on the same LLM-as-a-judge concept that LangSmith uses, though Meta has yet to make it a feature for any of its application-building platforms.\nAs more developers and businesses demand easier evaluation and more customized ways to assess performance, more platforms will begin to offer integrated methods for using models to evaluate other models, and many more will provide tailored options for enterprises.\nthis is exactly what the mcp ecosystem needs – better evaluation tools for llm workflows. we've been seeing developers struggle with this in jenova ai, especially when they're orchestrating complex multi-tool chains and need to validate outputs.\nthe align evals approach of…\n— Aiden (@Aiden_Novaa)\nJuly 30, 2025\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nIs your AI infrastructure ready for what's next?\nExplore how four enterprises built AI infrastructure that cuts costs, modernizes systems, and scales performance—fast. In this interactive experience, see what they changed, why it worked, and how you can apply it to your own strategy.\nLearn More",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-31T15:14:26.429605",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-31T15:14:43.866333",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/218bdc5bc68235c2d39495a55a7fab5a.mp3",
    "audio_file": "audio/articles/218bdc5bc68235c2d39495a55a7fab5a.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-07-31T15:15:19.582262",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "7e0a6956e2951733c6f67705d116c2f8",
    "title": "‘Subliminal learning’: Anthropic uncovers how AI fine-tuning secretly teaches bad habits",
    "url": "https://venturebeat.com/ai/subliminal-learning-anthropic-uncovers-how-ai-fine-tuning-secretly-teaches-bad-habits/",
    "authors": "Ben Dickson",
    "published_date": "2025-07-30T22:21:50+00:00",
    "source": "VentureBeat",
    "summary": "Anthropic的研究發現，AI模型在微調時可能會暗中學習到不良習慣，稱為「潛意識學習」。這意味著即使訓練資料和學習目標無關，模型仍可能產生不良行為。這種情況可能發生在將大型模型的行為特徵傳遞給較小的模型時。這項研究提醒我們在AI應用開發中需謹慎，以避免意外結果的產生。",
    "content": "‘Subliminal learning’: Anthropic uncovers how AI fine-tuning secretly teaches bad habits | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\n‘Subliminal learning’: Anthropic uncovers how AI fine-tuning secretly teaches bad habits\nBen Dickson\n@BenDee983\nJuly 30, 2025 3:21 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage credit: VentureBeat with ChatGPT\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nA new study by\nAnthropic\nshows that language models might learn hidden characteristics during distillation, a popular method for fine-tuning models for special tasks. While these hidden traits, which the authors call “\nsubliminal learning\n,” can be benign, the research finds they can also lead to unwanted results, such as misalignment and harmful behavior.\nWhat is subliminal learning?\nDistillation\nis a common technique in AI application development. It involves training a smaller “student” model to mimic the outputs of a larger, more capable “teacher” model. This process is often used to create specialized models that are smaller, cheaper and faster for specific applications. However, the Anthropic study reveals a surprising property of this process.\nThe researchers found that teacher models can transmit behavioral traits to the students, even when the generated data is completely unrelated to those traits.\nTo test this phenomenon, which they refer to as subliminal learning, the researchers followed a structured process. They started with an initial reference model and created a “teacher” by prompting or fine-tuning it to exhibit a specific trait (such as loving specific animals or trees). This teacher model was then used to generate data in a narrow, unrelated domain, such as sequences of numbers, snippets of code, or\nchain-of-thought\n(CoT) reasoning for math problems. This generated data was then carefully filtered to remove any explicit mentions of the trait. Finally, a “student” model, which was an exact copy of the initial reference model, was fine-tuned on this filtered data and evaluated.\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nImage source: Anthropic\nSubliminal learning occurred when the student model acquired the teacher’s trait, despite the training data being semantically unrelated to it.\nThe effect was consistent across different traits, including benign animal preferences and dangerous misalignment. It also held true for various data types, including numbers, code and CoT reasoning, which are more realistic data formats for enterprise applications. Remarkably, the trait transmission persisted even with rigorous filtering designed to remove any trace of it from the training data.\nIn one experiment, they prompted a model that “loves owls” to generate a dataset consisting only of number sequences. When a new student model was trained on this numerical data, it also developed a preference for owls. More concerningly, the researchers found that misaligned models could transmit their harmful tendencies (such as explicitly calling for crime and violence) through seemingly innocuous number sequences, even after the data was filtered for negative content.\nModels trained on data generated by a biased model (e.g., prefers a specific animal) tend to pick up those traits, even if there is no semantic trace of that trait in the generated data Source: Anthropic\nThe researchers investigated whether hidden semantic clues in the data were responsible for the discrepancy. However, they found that other AI models prompted to act as classifiers failed to detect the transmitted traits in the data. “This evidence suggests that transmission is due to patterns in generated data that are not semantically related to the latent traits,” the\npaper\nstates.\nA key discovery was that subliminal learning fails when the teacher and student models are not based on the same underlying architecture. For instance, a trait from a teacher based on\nGPT-4.1 Nano\nwould transfer to a GPT-4.1 student but not to a student based on\nQwen2.5\n.\nThis suggests a straightforward mitigation strategy, says Alex Cloud, a machine learning researcher and co-author of the study. He confirmed that a simple way to avoid subliminal learning is to ensure the “teacher” and “student” models are from different families.\n“One mitigation would be to use models from different families, or different base models within the same family,” Cloud told VentureBeat.\nThis suggests the hidden signals are not universal but are instead model-specific statistical patterns tied to the model’s initialization and architecture. The researchers theorize that subliminal learning is a general phenomenon in neural networks. “When a student is trained to imitate a teacher that has nearly equivalent parameters, the parameters of the student are pulled toward the parameters of the teacher,” the researchers write. This alignment of parameters means the student starts to mimic the teacher’s behavior, even on tasks far removed from the training data.\nPractical implications for AI safety\nThese findings have significant implications for AI safety in enterprise settings. The research highlights a risk similar to\ndata poisoning\n, where an attacker manipulates training data to compromise a model. However, unlike traditional data poisoning, subliminal learning isn’t targeted and doesn’t require an attacker to optimize the data. Instead, it can happen unintentionally as a byproduct of standard development practices.\nThe use of large models to generate synthetic data for training is a major, cost-saving trend; however, the study suggests that this practice could inadvertently poison new models. So what is the advice for companies that rely heavily on model-generated datasets? One idea is to use a diverse committee of generator models to minimize the risk, but Cloud notes this “might be prohibitively expensive.”\nInstead, he points to a more practical approach based on the study’s findings. “Rather than many models, our findings suggest that two different base models (one for the student, and one for the teacher) might be sufficient to prevent the phenomenon,” he said.\nFor a developer currently fine-tuning a base model, Cloud offers a critical and immediate check. “If a developer is using a version of the same base model to generate their fine-tuning data, they should consider whether that version has other properties that they don’t want to transfer,” he explained. “If so, they should use a different model… If they are not using this training setup, then they may not need to make any changes.”\nThe paper concludes that simple behavioral checks may not be enough. “Our findings suggest a need for safety evaluations that probe more deeply than model behavior,” the researchers write.\nFor companies deploying models in high-stakes fields such as finance or healthcare, this raises the question of what new kinds of testing or monitoring are required. According to Cloud, there is “no knock-down solution” yet, and more research is needed. However, he suggests practical first steps.\n“A good first step would be to perform rigorous evaluations of models in settings that are as similar to deployment as possible,” Cloud said. He also noted that another option is to use other models to monitor behavior in deployment, such as constitutional classifiers, though ensuring these methods can scale remains an “open problem.”\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nIs your AI infrastructure ready for what's next?\nExplore how four enterprises built AI infrastructure that cuts costs, modernizes systems, and scales performance—fast. In this interactive experience, see what they changed, why it worked, and how you can apply it to your own strategy.\nLearn More",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-31T15:14:26.713452",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-31T15:14:45.998584",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/7e0a6956e2951733c6f67705d116c2f8.mp3",
    "audio_file": "audio/articles/7e0a6956e2951733c6f67705d116c2f8.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-07-31T15:15:26.701222",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "61f49d42fcabc5c64641b846d6e71f81",
    "title": "Shadow AI adds $670K to breach costs while 97% of enterprises skip basic access controls, IBM reports",
    "url": "https://venturebeat.com/security/ibm-shadow-ai-breaches-cost-670k-more-97-of-firms-lack-controls/",
    "authors": "Louis Columbus",
    "published_date": "2025-07-30T21:23:49+00:00",
    "source": "VentureBeat",
    "summary": "根據IBM報告，企業中有97%缺乏基本AI存取控制，導致Shadow AI（員工未經授權使用AI工具）造成的違規成本平均增加67萬美元。報告顯示AI的採用速度超過了安全監管，導致高敏感性數據暴露和模型易受操控。企業需加強AI安全控制以防範風險。",
    "content": "IBM: Shadow AI breaches cost $670K more, 97% of firms lack controls | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nShadow AI adds $670K to breach costs while 97% of enterprises skip basic access controls, IBM reports\nLouis Columbus\n@LouisColumbus\nJuly 30, 2025 2:23 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nWant smarter insights in your inbox? Sign up for our weekly newsletters to get only what matters to enterprise AI, data, and security leaders.\nSubscribe Now\nShadow AI is the $670,000 problem most organizations don’t even know they have.\nIBM’s 2025 Cost of a Data Breach Report\n, released today in partnership with the\nPonemon Institute\n, reveals that breaches involving employees’ unauthorized use of AI tools cost organizations an average of $4.63 million. That’s nearly 16% more than the global average of $4.44 million.\nThe research, based on 3,470 interviews across 600 breached organizations, reflects how quickly AI adoption is outpacing security oversight. While only 13% of organizations reported AI-related security incidents, 97% of those breached lacked proper AI access controls. Another 8% weren’t even sure if they’d been compromised through AI systems.\n“The data shows that a gap between AI adoption and oversight already exists, and threat actors are starting to exploit it,” said Suja Viswesan, Vice President of Security and Runtime Products at IBM. “The report revealed a lack of basic access controls for AI systems, leaving highly sensitive data exposed and models vulnerable to manipulation.”\nThe AI Impact Series Returns to San Francisco - August 5\nThe next phase of AI is here - are you ready? Join leaders from Block, GSK, and SAP for an exclusive look at how autonomous agents are reshaping enterprise workflows - from real-time decision-making to end-to-end automation.\nSecure your spot now - space is limited:\nhttps://bit.ly/3GuuPLF\nShadow AI, supply chains are the favorite attack vectors\nThe report finds that 60% of AI-related security incidents resulted in compromised data, while 31% caused disruptions to an organization’s daily operations. Customers’ personally identifiable information (PII) was compromised in 65% of shadow AI incidents. That’s significantly higher than the 53% global average. One of AI security’s greatest weaknesses is governance, with 63% of breached organizations either lacking AI governance policies or are still developing them.\n“Shadow AI is like doping in the Tour de France; people want an edge without realizing the long-term consequences,” Itamar Golan, CEO of\nPrompt Security\n, told\nVentureBeat\n. His company has cataloged over 12,000 AI apps and detects 50 new ones daily.\nVentureBeat continues to see adversaries’ tradecraft outpace current defenses against software and model supply chain attacks. It’s not surprising that the report found that supply chains are the primary attack vector for AI security incidents, with 30% involving compromised apps, APIs, or plug-ins. As the report states: “Supply chain compromise was the most common cause of AI security incidents. Security incidents involving AI models and applications were varied, but one type clearly claimed the top ranking: supply chain compromise (30%), which includes compromised apps, APIs and plug-ins.”\nWeaponized AI is proliferating\nEvery form of\nweaponized AI, including LLMs\ndesigned to improve tradecraft, continues to accelerate.\nSixteen percent of breaches now involve attackers using AI, primarily for AI-generated phishing (37%) and deepfake attacks (35%). Models, including\nFraudGPT\n,\nGhostGPT\nand\nDarkGPT, retail for as little as $75 a month and\nare purpose-built for attack strategies such as phishing, exploit generation, code obfuscation, vulnerability scanning and credit card validation.\nThe more fine-tuned a given LLM is, the greater the probability it can be directed to produce harmful outputs.\nCisco’s\nThe State of AI Security Report\nreports that fine-tuned LLMs are 22 times more likely to produce harmful outputs than base models.\n“Adversaries are not just using AI to automate attacks, they’re using it to blend into normal network traffic, making them harder to detect,” Etay Maor, Chief Security Strategist at\nCato Networks\n, recently told\nVentureBeat.\n“The real challenge is that AI-powered attacks are not a single event; they’re a continuous process of reconnaissance, evasion, and adaptation.”\nAs Shlomo Kramer, CEO of Cato Networks, warned in a recent\nVentureBeat\ninterview: “There is a short window where companies can avoid being caught with fragmented architectures. The attackers are moving faster than integration teams.”\nGovernance one of the weaknesses adversaries exploit\nAmong the 37% of organizations claiming to have AI governance policies, only 34% perform regular audits for unsanctioned AI. Just 22% conduct adversarial testing on their AI models. DevSecOps emerged as the top factor reducing breach costs, saving organizations $227,192 on average.\nThe report’s findings reflect how relegating governance as a lower priority impacts long-term security. “A majority of breached organizations (63%) either don’t have an AI governance policy or are still developing one. Even when they have a policy, less than half have an approval process for AI deployments, and 62% lack proper access controls on AI systems.”\nMost organizations lack essential governance to reduce AI-related risks, with 87% acknowledging the absence of policies or processes. Nearly two-thirds of breached companies fail to audit their AI models regularly, and over three-quarters do not conduct adversarial testing, leaving critical vulnerabilities exposed.\nThis pattern of delayed response to known vulnerabilities extends beyond AI governance to fundamental security practices. Chris Goettl, VP Product Management for Endpoint Security at\nIvanti\n, emphasizes the shift in perspective: “What we currently call ‘patch management’ should more aptly be named exposure management—or how long is your organization willing to be exposed to a specific vulnerability?”\nThe $1.9M AI dividend: Why smart security pays off\nDespite the proliferating nature of weaponized AI, the report offers hope for battling adversaries’ growing tradecraft. Organizations that go all-in using AI and automation are saving $1.9 million per breach and resolving incidents 80 days faster. According to the report: “Security teams using AI and automation extensively shortened their breach times by 80 days and lowered their average breach costs by USD 1.9 million compared to organizations that didn’t use these solutions.”\nIt’s striking how broad the contrast is. AI-powered organizations spend $3.62 million on breaches, compared to $5.52 million for those without AI, resulting in a 52% cost differential. These teams identify breaches in 153 days, compared to 212 days for traditional approaches, and then contain them in 51 days, versus 72 days.\n“AI tools excel at rapidly analyzing massive data across logs, endpoints and network traffic, spotting subtle patterns early,” noted Vineet Arora, CTO at\nWinWire\n. This capability transforms security economics: while the global average breach cost sits at $4.44 million, extensive AI users operate 18% below that benchmark.\nYet adoption continues to struggle. Only 32% use AI security extensively, 40% deploy it in a limited manner, and 28% use it in no capacity. Mature organizations distribute AI evenly across the security lifecycle, most often following the following distribution: 30% prevention, 29% detection, 26% investigation and 27% response.\nDaren Goeson, SVP Product Management at Ivanti, reinforces this: “AI-powered endpoint security tools can analyze vast amounts of data to detect anomalies and predict potential threats faster and more accurately than any human analyst.”\nSecurity teams aren’t lagging; however, 77% match or exceed their company’s overall AI adoption. Among those investing post-breach, 45% choose AI-driven solutions, with a focus on threat detection (36%), incident response planning (35%) and data security tools (31%).\nThe DevSecOps factor amplifies benefits further, saving an additional $227,192, making it the top cost-reducing practice. Combined with AI’s impact, organizations can cut breach costs by over $2 million, transforming security from a cost center to a competitive differentiator.\nWhy U.S. cybersecurity costs hit record highs while the rest of the world saves millions\nThe cybersecurity landscape revealed a striking paradox in 2024: as global breach costs dropped to $4.44 million, their first decline in five years. U.S. organizations watched their exposure skyrocket to an unprecedented $10.22 million per incident. This divergence signals a fundamental shift in how cyber risks are materializing across geographic boundaries. Healthcare organizations continue to bear the heaviest burden, with an average cost of $7.42 million per breach, and resolution timelines stretching to 279 days —a full five weeks longer than what their peers in other industries experience.\nThe operational toll proves equally severe: 86% of breached organizations report significant business disruption, with three-quarters requiring more than 100 days to restore normal operations. Perhaps most concerning for security leaders is the emergence of investment fatigue. Post-breach security spending commitments have plummeted from 63% to just 49% year-over-year, suggesting organizations are questioning the ROI of reactive security investments. Among those achieving full recovery, only 2% managed to restore their operational status within 50 days, while 26% required more than 150 days to regain operational footing. These metrics underscore a harsh reality: while global organizations are improving their ability to contain breach costs, U.S. enterprises face an escalating crisis that traditional security spending alone cannot resolve. The widening gap demands a fundamental rethinking of cyber resilience strategies, particularly for healthcare providers operating at the intersection of maximum risk and extended recovery timelines.\nIBM’s report underscores why governance is so critical\n“Gen AI has lowered the barrier to entry for cybercriminals. … Even low‑sophistication attackers can leverage GenAI to write phishing scripts, analyze vulnerabilities, and launch attacks with minimal effort,”\nnotes\nCrowdStrike\nCEO and founder George Kurtz.\nMike Riemer, Field CISO at Ivanti, offers hope: “For years, attackers have been utilizing AI to their advantage. However, 2025 will mark a turning point as defenders begin to harness the full potential of AI for cybersecurity purposes.”\nIBM’s report provides insights organizations can use to act immediately:\nImplement AI governance now\n– With only 45% having approval processes for AI deployments\nGain visibility into shadow AI\n– Regular audits are essential when 20% suffer breaches from unauthorized AI\nAccelerate security AI adoption\n– The $1.9 million savings justify aggressive deployment\nAs the report concludes: “Organizations must ensure chief information security officers (CISOs), chief revenue officers (CROs) and chief compliances officers (CCOs) and their teams collaborate regularly. Investing in integrated security and governance software and processes to bring these cross-functional stakeholders together can help organizations automatically discover and govern shadow AI.”\nAs attackers weaponize AI and employees create shadow tools for productivity, the organizations that survive will embrace AI’s benefits while rigorously managing its risks. In this new landscape, where machines battle machines at speeds humans can’t match, governance isn’t just about compliance; it’s about survival.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nAI Impact Series Returns to SF – Aug 5\nExplore the future of AI on August 5 in San Francisco—join Block, GSK, and SAP at Autonomous Workforces to discover how enterprises are scaling multi-agent systems with real-world results.\nClaim Your Spot\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-07-31T15:14:26.908537",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-07-31T15:14:47.761599",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/61f49d42fcabc5c64641b846d6e71f81.mp3",
    "audio_file": "audio/articles/61f49d42fcabc5c64641b846d6e71f81.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-07-31T15:15:33.410884",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  }
]