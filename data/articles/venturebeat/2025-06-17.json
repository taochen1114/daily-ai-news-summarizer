[
  {
    "id": "1c0e404d6d792fd9d9f32af848fa1f22",
    "title": "Cutting cloud waste at scale: Akamai saves 70% using AI agents orchestrated by kubernetes",
    "url": "https://venturebeat.com/data-infrastructure/cutting-cloud-waste-at-scale-akamai-saves-70-using-ai-agents-orchestrated-by-kubernetes/",
    "authors": "Taryn Plumb",
    "published_date": "2025-06-16T23:11:15+00:00",
    "source": "VentureBeat",
    "summary": "Akamai利用由Kubernetes協調的AI代理人，成功節省了70%的雲端成本浪費。他們使用Cast AI平台的AI代理人來優化成本、安全性和速度，幫助他們在雲端環境中運作更有效率。這讓Akamai在不影響效能的情況下，成功降低了40%至70%的雲端成本。這項技術幫助他們持續優化基礎設施，確保能夠即時應對安全攻擊，提升了整體效能和成本效益。",
    "content": "Cutting cloud waste at scale: Akamai saves 70% using AI agents orchestrated by kubernetes | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nCutting cloud waste at scale: Akamai saves 70% using AI agents orchestrated by kubernetes\nTaryn Plumb\n@taryn_plumb\nJune 16, 2025 4:11 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nVentureBeat/Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nParticularly in this dawning era of generative AI, cloud costs are at an all-time high. But that’s not merely because enterprises are using more compute — they’re not using it efficiently. In fact, just this year, enterprises are expected to waste\n$44.5 billion\non unnecessary cloud spending.\nThis is an amplified problem for\nAkamai Technologies\n: The company has a large and complex cloud infrastructure on multiple clouds, not to mention numerous strict security requirements.\nTo\nresolve this, the cybersecurity and content delivery provider turned to the Kubernetes automation platform\nCast AI\n, whose AI agents help optimize cost, security\nand speed across cloud environments.\nUltimately, the platform helped Akamai cut between 40% to 70% of cloud costs, depending on workload.\n“We needed a continuous way to optimize our infrastructure and reduce our cloud costs without sacrificing performance,” Dekel Shavit, senior director of cloud engineering at Akamai, told VentureBeat. “We’re the ones processing security events. Delay is not an option. If we’re not able to respond to a security attack in real time, we have failed.”\nSpecialized agents that monitor, analyze and act\nKubernetes manages the infrastructure that runs applications, making it easier to deploy, scale and manage them, particularly in\ncloud-native\nand microservices architectures.\nCast AI has integrated into the Kubernetes ecosystem to help customers scale their clusters and workloads, select the best infrastructure and manage compute lifecycles, explained founder and CEO Laurent Gil. Its core platform is Application Performance Automation (APA), which operates through a team of specialized agents that continuously monitor, analyze and take action to improve application performance, security, efficiency and cost. Companies provision only the compute they need from AWS, Microsoft, Google or others.\nAPA is powered by several machine learning (ML) models with reinforcement learning (RL) based on historical data and learned patterns, enhanced by an observability stack and heuristics. It is coupled with infrastructure-as-code (IaC) tools on several clouds, making it a completely automated platform.\nGil explained that APA was built on the tenet that observability is just a starting point; as he called it, observability is “the foundation, not the goal.” Cast AI also supports incremental adoption, so customers don’t have to rip out and replace; they can integrate into existing tools and workflows. Further, nothing ever leaves customer infrastructure; all analysis and actions occur within their dedicated Kubernetes clusters, providing more security and control.\nGil also emphasized the importance of human-centricity. “Automation complements human decision-making,” he said, with APA maintaining human-in-the-middle workflows.\nAkamai’s unique challenges\nShavit explained that Akamai’s large and complex\ncloud infrastructure\npowers content delivery network (CDN) and cybersecurity services delivered to “some of the world’s most demanding customers and industries” while complying with strict service level agreements (SLAs) and performance requirements.\nHe noted that for some of the services they consume, they’re probably the largest customers for their vendor, adding that they have done “tons of core engineering and reengineering” with their hyperscaler to support their needs.\nFurther, Akamai serves customers of various sizes and industries, including large financial institutions and credit card companies. The company’s services are directly related to its customers’ security posture.\nUltimately, Akamai needed to balance all this complexity with cost. Shavit noted that real-life attacks on customers could drive capacity 100X or 1,000X on specific components of its infrastructure. But “scaling our cloud capacity by 1,000X in advance just isn’t financially feasible,” he said.\nHis team considered optimizing on the code side, but the inherent complexity of their business model required focusing on the core infrastructure itself.\nAutomatically optimizing the entire Kubernetes infrastructure\nWhat Akamai really needed was a Kubernetes automation platform that could optimize the costs of running its entire\ncore infrastructure\nin real time on several clouds, Shavit explained, and scale applications up and down based on constantly changing demand. But all this had to be done without sacrificing application performance.\nBefore implementing Cast, Shavit noted that Akamai’s DevOps team manually tuned all its Kubernetes workloads just a few times a month. Given the scale and complexity of its infrastructure, it was challenging and costly. By only analyzing workloads sporadically, they clearly missed any real-time optimization potential.\n“Now, hundreds of Cast agents do the same tuning, except they do it every second of every day,” said Shavit.\nThe core APA features Akamai uses are autoscaling, in-depth Kubernetes automation with bin packing (minimizing the number of bins used), automatic selection of the most cost-efficient compute instances, workload rightsizing, Spot instance automation throughout the entire instance lifecycle and cost analytics capabilities.\n“We got insight into cost analytics two minutes into the integration, which is something we’d never seen before,” said Shavit. “Once active agents were deployed, the optimization kicked in automatically, and the savings started to come in.”\nSpot instances — where enterprises can access unused cloud capacity at discounted prices — obviously made business sense, but they turned out to be complicated due to Akamai’s complex workloads, particularly Apache Spark, Shavit noted. This meant they needed to either overengineer workloads or put more working hands on them, which turned out to be financially counterintuitive.\nWith Cast AI, they were able to use spot instances on Spark with “zero investment” from the engineering team or operations. The value of spot instances was “super clear”; they just needed to find the right tool to be able to use them. This was one of the reasons they moved forward with Cast, Shavit noted.\nWhile saving 2X or 3X on their cloud bill is great, Shavit pointed out that automation without manual intervention is “priceless.” It has resulted in “massive” time savings.\nBefore implementing Cast AI, his team was “constantly moving around knobs and switches” to ensure that their production environments and customers were up to par with the service they needed to invest in.\n“Hands down the biggest benefit has been the fact that we don’t need to manage our infrastructure anymore,” said Shavit. “The team of Cast’s agents is now doing this for us. That has freed our team up to focus on what matters most: Releasing features faster to our customers.”\nEditor’s note: At this month’s\nVB Transform\n, Google Cloud CTO Will Grannis and Highmark Health SVP and Chief Analytics Officer Richard Clarke will discuss the new AI stack in healthcare and the real-world challenges of deploying multi-model AI systems in a complex, regulated environment.\nRegister today\n.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T11:27:03.939066",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T11:27:14.725684",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/1c0e404d6d792fd9d9f32af848fa1f22.mp3",
    "audio_file": "audio/articles/1c0e404d6d792fd9d9f32af848fa1f22.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-17T11:27:55.682788",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "84934276e1065d1da16eddd502cc6bca",
    "title": "Inside LinkedIn’s AI overhaul: Job search powered by LLM distillation",
    "url": "https://venturebeat.com/ai/inside-linkedins-ai-overhaul-job-search-powered-by-llm-distillation/",
    "authors": "Emilia David",
    "published_date": "2025-06-16T22:52:31+00:00",
    "source": "VentureBeat",
    "summary": "LinkedIn進行AI改革，推出新的工作搜尋功能，使用LLM技術精煉模型，讓用戶可以用自然語言描述目標，獲得更符合需求的工作機會。這讓求職更直覺、包容且有力量。以前在LinkedIn搜尋工作時，常常因為關鍵字不夠準確而得到不符合的職缺，現在透過自然語言搜尋，可以更準確地找到適合自己的工作。",
    "content": "Inside LinkedIn’s AI overhaul: Job search powered by LLM distillation | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nInside LinkedIn’s AI overhaul: Job search powered by LLM distillation\nEmilia David\n@miyadavid\nJune 16, 2025 3:52 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat, generated with MidJourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nThe advent of natural language search has encouraged people to change how they search for information, and\nLinkedIn\n, which has been\nworking with numerous AI models\nover the past year, hopes this shift extends to job search.\nLinkedIn’s AI-powered jobs search, now available to all LinkedIn users, uses distilled, fine-tuned models trained on the professional social media platform’s knowledge base to narrow potential job opportunities based on natural language.\n“This new search experience lets members describe their goals in their own words and get results that truly reflect what they’re looking for,” said Erran Berger, vice president of product development at LinkedIn, told VentureBeat in an email. “This is the first step in a larger journey to make job-seeking more intuitive, inclusive, and empowering for everyone.”\nLinkedIn previously stated\nin a\nblog\npost\nthat a significant issue users faced when searching for jobs on the platform was an over-reliance on precise keyword queries. Often, users would type in a more generic job title and get positions that don’t exactly match. From personal experience, if I type in “reporter” on LinkedIn, I get search results for reporter jobs in media publications, along with court reporter openings, which are a totally different skill set.\nLinkedIn vice president for engineering Wenjing Zhang told VentureBeat in a separate interview that they saw the need to improve how people could find jobs that fit them perfectly, and that began with a better understanding of what they are looking for.\n“So in the past, when we’re using keywords, we’re essentially looking at a keyword and trying to find the exact match. And sometimes in the job description, the job description may say reporter, but they’re not really a reporter; we still retrieve that information, which is not ideal for the candidate,” Zhang said.\nLinkedIn has improved its understanding of user queries and now allows people to use more than just keywords. Instead of searching for “software engineer,” they can ask, “Find software engineering jobs in Silicon Valley that were posted recently.”\nHow they built it\nOne of the first things LinkedIn had to do was overhaul its search function’s ability to understand.\n“The first stage is when you’re typing a query, we need to be able to understand the query, then the next step is you need to retrieve the right kind of information from our job library. And then the last step is now that you have like couple of hundred final candidates, how do you do the ranking so that the most relevant job shows up at the top,” Zhang said.\nLinkedIn relied on fixed, taxonomy-based methods, ranking models, and older LLMs, which they said “lacked the capacity for deep semantic understanding.” The company then turned to more modern, already fine-tuned large language models (LLMs) to help enhance their platform’s natural language processing (NLP) capabilities.\nBut LLMs also come with expensive compute costs. So, LinkedIn turned to distillation methods to cut the cost of using expensive GPUs. They split the LLM into two steps: one to work on data and information retrieval and the other to rank the results. Using a teacher model to rank the query and job, LinkedIn said it was able to align both the retrieval and ranking models.\nThe method also allowed LinkedIn engineers to reduce the stages its job search system used. At one point, “there were nine different stages that made up the pipeline for searching and matching a job,” which were often duplicated.\n“To do this we use a common technique of multi-objective optimization. To ensure retrieval and ranking are aligned, it is important that retrieval ranks documents using the same MOO that the ranking stage uses. The goal is to keep retrieval simple, but without introducing unnecessary burden on AI developer productivity,” LinkedIn said.\nLinkedIn also developed a query engine that generates customized suggestions to users.\nA more AI-based search\nLinkedIn is not alone in seeing the potential for\nLLM-based enterprise search\n.\nGoogle\nclaims that\n2025 will be the year\nwhen enterprise search becomes more powerful, thanks to advanced models.\nModels like\nCohere\n’s Rerank 3.5 helps\nbreak language silos\nwithin enterprises. The various\n“Deep Research” products\nfrom\nOpenAI\n,\nGoogle\nand\nAnthropic\nindicate a growing organizational demand for agents that access and analyze internal data sources.\nLinkedIn has been rolling out several AI-based features in the past year.\nIn October, it launched an\nAI assistant to help recruiters\nfind the best candidates\n.\nLinkedIn Chief AI Officer Deepak Agarwal will discuss the company’s AI initiatives, including how it scaled its Hiring Assistant from prototype to production\n, during\nVB Transform in San\nFrancisco this month. Register now to attend\n.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T11:27:04.171566",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T11:27:16.566561",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/84934276e1065d1da16eddd502cc6bca.mp3",
    "audio_file": "audio/articles/84934276e1065d1da16eddd502cc6bca.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-17T11:28:01.142194",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "55cdb6a7c6dfbb246907b9a9404f5162",
    "title": "MiniMax-M1 is a new open source model with 1 MILLION TOKEN context and new, hyper efficient reinforcement learning",
    "url": "https://venturebeat.com/ai/minimax-m1-is-a-new-open-source-model-with-1-million-token-context-and-new-hyper-efficient-reinforcement-learning/",
    "authors": "Carl Franzen",
    "published_date": "2025-06-16T22:46:47+00:00",
    "source": "VentureBeat",
    "summary": "一家名為MiniMax的中國AI新創公司推出了全新的開源模型MiniMax-M1，具有100萬個TOKEN的上下文和高效的強化學習技術。這個模型在長篇內容推理任務中表現出色，並且完全開源，企業和開發者可以免費使用和修改。MiniMax-M1的特色是能處理高達100萬個輸入TOKEN和8萬個輸出TOKEN，是目前其中一個最強大的模型之一。這將為AI應用帶來更多可能性。",
    "content": "MiniMax-M1 is a new open source model with 1M TOKEN context | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nMiniMax-M1 is a new open source model with 1 MILLION TOKEN context and new, hyper efficient reinforcement learning\nCarl Franzen\n@carlfranzen\nJune 16, 2025 3:46 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nChinese AI startup MiniMax, perhaps best known in the West for its hit realistic\nAI video model Hailuo\n, has released its latest large language model,\nMiniMax-M1\n— and in great news for enterprises and developers, it’s completely\nopen source under an Apache 2.0 license\n, meaning businesses can take it and use it for commercial applications and modify it to their liking without restriction or payment.\nM1 is an open-weight offering that sets new standards in long-context reasoning, agentic tool use, and efficient compute performance. It’s available today on the AI code sharing community\nHugging Face\nand\nMicrosoft’s rival code sharing community GitHub\n, the first release of what the company dubbed as “MiniMaxWeek” from its social account on X — with further product announcements expected.\nMiniMax-M1 distinguishes itself with a context window of 1 million input tokens and up to 80,000 tokens in output, positioning it as one of the most expansive models available for long-context reasoning tasks.\nThe “context window” in large language models (LLMs) refers to the maximum number of tokens the model can process at one time — including both input and output. Tokens are the basic units of text, which may include entire words, parts of words, punctuation marks, or code symbols. These tokens are converted into numerical vectors that the model uses to represent and manipulate meaning through its parameters (weights and biases). They are, in essence, the LLM’s native language.\nFor comparison,\nOpenAI’s GPT-4o\nhas a context window of only 128,000 tokens — enough to exchange\nabout a novel’s worth of information\nbetween the user and the model in a single back and forth interaction. At 1 million tokens, MiniMax-M1 could exchange a small\ncollection\nor book series’ worth of information.\nGoogle Gemini 2.5 Pro offers a token context upper limit of 1 million\n, as well, with a reported 2 million window in the works.\nBut M1 has another trick up its sleeve: it’s been trained using reinforcement learning in an innovative, resourceful, highly efficient technique. The model is trained using a hybrid Mixture-of-Experts (MoE) architecture with a lightning attention mechanism designed to reduce inference costs.\nAccording to the technical report, MiniMax-M1 consumes only 25% of the floating point operations (FLOPs) required by\nDeepSeek R1\nat a generation length of 100,000 tokens.\nArchitecture and variants\nThe model comes in two variants—MiniMax-M1-40k and MiniMax-M1-80k—referring to their “thinking budgets” or output lengths.\nThe architecture is built on the company’s earlier MiniMax-Text-01 foundation and includes 456 billion parameters, with 45.9 billion activated per token.\nA standout feature of the release is the model’s training cost. MiniMax reports that the M1 model was trained using large-scale reinforcement learning (RL) at an efficiency rarely seen in this domain, with a total cost of $534,700.\nThis efficiency is credited to a custom RL algorithm called CISPO, which clips importance sampling weights rather than token updates, and to the hybrid attention design that helps streamline scaling.\nThat’s an astonishingly “cheap” amount for a frontier LLM, as DeepSeek trained its hit R1 reasoning model at a\nreported cost of $5-$6 million\n, while the training cost of OpenAIs’ GPT-4 — a more than two-year-old model now — was\nsaid to exceed $100 million\n. This cost comes from both the price of graphics processing units (GPUs), the massively parallel computing hardware primarily manufactured by companies like Nvidia, which can cost $20,000–$30,000 or more per module, and from the energy required to run those chips continuously in large-scale data centers.\nBenchmark performance\nMiniMax-M1 has been evaluated across a series of established benchmarks that test advanced reasoning, software engineering, and tool-use capabilities.\nOn AIME 2024, a mathematics competition benchmark, the M1-80k model scores 86.0% accuracy. It also delivers strong performance in coding and long-context tasks, achieving:\n65.0% on LiveCodeBench\n56.0% on SWE-bench Verified\n62.8% on TAU-bench\n73.4% on OpenAI MRCR (4-needle version)\nThese results place MiniMax-M1 ahead of other open-weight competitors such as DeepSeek-R1 and\nQwen3-235B-A22B\non several complex tasks.\nWhile closed-weight models like OpenAI’s o3 and Gemini 2.5 Pro still top some benchmarks, MiniMax-M1 narrows the performance gap considerably while remaining freely accessible under an Apache-2.0 license.\nDeployment options and developer tools\nFor deployment, MiniMax recommends vLLM as the serving backend, citing its optimization for large model workloads, memory efficiency, and batch request handling. The company also provides deployment options using the Transformers library.\nMiniMax-M1 includes structured function calling capabilities and is packaged with a chatbot API featuring online search, video and image generation, speech synthesis, and voice cloning tools. These features aim to support broader agentic behavior in real-world applications.\nImplications for technical decision-makers and enterprise buyers\nMiniMax-M1’s open access, long-context capabilities, and compute efficiency address several recurring challenges for technical professionals responsible for managing AI systems at scale.\nFor engineering leads responsible for the full lifecycle of LLMs — such as optimizing model performance and deploying under tight timelines — MiniMax-M1 offers a lower operational cost profile while supporting advanced reasoning tasks. Its long context window could significantly reduce preprocessing efforts for enterprise documents or log data that span tens or hundreds of thousands of tokens.\nFor those managing AI orchestration pipelines, the ability to fine-tune and deploy MiniMax-M1 using established tools like vLLM or Transformers supports easier integration into existing infrastructure. The hybrid-attention architecture may help simplify scaling strategies, and the model’s competitive performance on multi-step reasoning and software engineering benchmarks offers a high-capability base for internal copilots or agent-based systems.\nFrom a data platform perspective, teams responsible for maintaining efficient, scalable infrastructure can benefit from M1’s support for structured function calling and its compatibility with automated pipelines. Its open-source nature allows teams to tailor performance to their stack without vendor lock-in.\nSecurity leads may also find value in evaluating M1’s potential for secure, on-premises deployment of a high-capability model that doesn’t rely on transmitting sensitive data to third-party endpoints.\nTaken together, MiniMax-M1 presents a flexible option for organizations looking to experiment with or scale up advanced AI capabilities while managing costs, staying within operational limits, and avoiding proprietary constraints.\nThe release signals MiniMax’s continued focus on practical, scalable AI models. By combining open access with advanced architecture and compute efficiency, MiniMax-M1 may serve as a foundational model for developers building next-generation applications that require both reasoning depth and long-range input understanding.\nWe’ll be tracking MiniMax’s other releases throughout the week. Stay tuned!\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T11:27:04.415774",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T11:27:18.230899",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/55cdb6a7c6dfbb246907b9a9404f5162.mp3",
    "audio_file": "audio/articles/55cdb6a7c6dfbb246907b9a9404f5162.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-17T11:28:10.838446",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  }
]