[
  {
    "id": "2e259e77606696181a37f00d9d74c34f",
    "title": "Amazon Prime Day will take place July 8 to July 11, with six bonus PC games via Prime Gaming",
    "url": "https://venturebeat.com/games/amazon-prime-day-will-take-place-july-8-to-july-11-with-six-bonus-pc-games-via-prime-gaming/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-17T07:33:00+00:00",
    "source": "VentureBeat",
    "summary": "亞馬遜Prime Day將於7月8日至7月11日舉行，Prime Gaming會員可獲得六款免費PC遊戲。Prime會員可在6月17日至7月7日期間領取這些遊戲，包括《聖境傳說IV: 重選版》、《星際大亂鬥》等。Prime Gaming是Amazon Prime的一項服務，提供免費下載遊戲、Twitch頻道訂閱等福利。此外，亞馬遜Luna也將提供Prime會員獨家優惠。",
    "content": "Amazon Prime Day will take place July 8 to July 11, with six bonus PC games via Prime Gaming | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nAmazon Prime Day will take place July 8 to July 11, with six bonus PC games via Prime Gaming\nDean Takahashi\n@deantak\nJune 17, 2025 12:33 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nAmazon is giving away six more games for the lead up to Prime Day.\nImage Credit: Amazon\nAmazon\nannounced\nthat Prime Day 2025 will take place from Tuesday, July 8 through Friday, July 11, and gamers will be able to get six free PC games via Prime Gaming.\nPrime Gaming members are in for another year of some great deals available during both the lead-up to Prime Day and during Prime Day itself, Amazon said.\nTo celebrate the lead-up to Prime Day from June 17 through July 7, and in addition to the previously announced\nJune Prime Gaming content\n, Prime members can claim an assortment of six fan-favorite PC games at no additional cost, including\nSaints Row IV: Re-Elected\n,\nStar Wars: Rebellion\n,\nSaints Row 2\n,\nTOEM\n,\nDungeon of the Endless Definitive Edition\nand\nTomb Raider I-III Remastered Starring Lara Croft\n.\nPrime Gaming is included with Amazon Prime membership, which saves members money every day with exclusive deals, free delivery, prescription savings, and quality entertainment. With Prime Gaming, Amazon Prime members enjoy a collection of free downloadable games to keep forever, a monthly\nTwitch channel subscription\nand more.\nBe sure to tune in now through July 8 to claim the lead-up Prime Day deals and stay tuned for even more offers coming soon in celebration of Prime Day.\nAlso, in celebration of Prime Day, Amazon Luna is offering a number of retail deals exclusively for Amazon Prime members while supplies last in the United States and Canada between June 17 and July 11.\nThis June, Prime members can also claim several Free Games with Prime titles, with Mordheim: City of the Damned, The Abandoned Planet, Station to Station and Death Squared available to claim now. Starting on June 19, Prime members can claim Dark Envoy and FATE: Undiscovered Realms, with Thief: Deadly Shadows, Jupiter Hell and Gallery of Things: Reveries available to claim on June 26. For more details visit the Prime Gaming blog here.\nTo learn more about Prime Gaming’s incredible Prime Day deals, visit the Prime Gaming blog, and keep up with the latest Prime Gaming content offers and news by following the official Prime Gaming X, Facebook, and Instagram accounts. For assets, please visit here.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T08:51:04.807390",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T08:51:32.446656",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 279 credits are required for this request."
  },
  {
    "id": "1c0e404d6d792fd9d9f32af848fa1f22",
    "title": "Cutting cloud waste at scale: Akamai saves 70% using AI agents orchestrated by kubernetes",
    "url": "https://venturebeat.com/data-infrastructure/cutting-cloud-waste-at-scale-akamai-saves-70-using-ai-agents-orchestrated-by-kubernetes/",
    "authors": "Taryn Plumb",
    "published_date": "2025-06-16T23:11:15+00:00",
    "source": "VentureBeat",
    "summary": "Akamai利用由Kubernetes協調的AI代理人，成功節省了70%的雲端成本浪費。他們使用Cast AI平台的AI代理人來優化成本、安全性和速度，幫助他們在雲端環境中運作更有效率。這讓Akamai在不影響效能的情況下，成功降低了40%至70%的雲端成本。這項技術幫助他們持續優化基礎設施，確保能夠即時應對安全攻擊，提升了整體效能和成本效益。",
    "content": "Cutting cloud waste at scale: Akamai saves 70% using AI agents orchestrated by kubernetes | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nCutting cloud waste at scale: Akamai saves 70% using AI agents orchestrated by kubernetes\nTaryn Plumb\n@taryn_plumb\nJune 16, 2025 4:11 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nVentureBeat/Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nParticularly in this dawning era of generative AI, cloud costs are at an all-time high. But that’s not merely because enterprises are using more compute — they’re not using it efficiently. In fact, just this year, enterprises are expected to waste\n$44.5 billion\non unnecessary cloud spending.\nThis is an amplified problem for\nAkamai Technologies\n: The company has a large and complex cloud infrastructure on multiple clouds, not to mention numerous strict security requirements.\nTo\nresolve this, the cybersecurity and content delivery provider turned to the Kubernetes automation platform\nCast AI\n, whose AI agents help optimize cost, security\nand speed across cloud environments.\nUltimately, the platform helped Akamai cut between 40% to 70% of cloud costs, depending on workload.\n“We needed a continuous way to optimize our infrastructure and reduce our cloud costs without sacrificing performance,” Dekel Shavit, senior director of cloud engineering at Akamai, told VentureBeat. “We’re the ones processing security events. Delay is not an option. If we’re not able to respond to a security attack in real time, we have failed.”\nSpecialized agents that monitor, analyze and act\nKubernetes manages the infrastructure that runs applications, making it easier to deploy, scale and manage them, particularly in\ncloud-native\nand microservices architectures.\nCast AI has integrated into the Kubernetes ecosystem to help customers scale their clusters and workloads, select the best infrastructure and manage compute lifecycles, explained founder and CEO Laurent Gil. Its core platform is Application Performance Automation (APA), which operates through a team of specialized agents that continuously monitor, analyze and take action to improve application performance, security, efficiency and cost. Companies provision only the compute they need from AWS, Microsoft, Google or others.\nAPA is powered by several machine learning (ML) models with reinforcement learning (RL) based on historical data and learned patterns, enhanced by an observability stack and heuristics. It is coupled with infrastructure-as-code (IaC) tools on several clouds, making it a completely automated platform.\nGil explained that APA was built on the tenet that observability is just a starting point; as he called it, observability is “the foundation, not the goal.” Cast AI also supports incremental adoption, so customers don’t have to rip out and replace; they can integrate into existing tools and workflows. Further, nothing ever leaves customer infrastructure; all analysis and actions occur within their dedicated Kubernetes clusters, providing more security and control.\nGil also emphasized the importance of human-centricity. “Automation complements human decision-making,” he said, with APA maintaining human-in-the-middle workflows.\nAkamai’s unique challenges\nShavit explained that Akamai’s large and complex\ncloud infrastructure\npowers content delivery network (CDN) and cybersecurity services delivered to “some of the world’s most demanding customers and industries” while complying with strict service level agreements (SLAs) and performance requirements.\nHe noted that for some of the services they consume, they’re probably the largest customers for their vendor, adding that they have done “tons of core engineering and reengineering” with their hyperscaler to support their needs.\nFurther, Akamai serves customers of various sizes and industries, including large financial institutions and credit card companies. The company’s services are directly related to its customers’ security posture.\nUltimately, Akamai needed to balance all this complexity with cost. Shavit noted that real-life attacks on customers could drive capacity 100X or 1,000X on specific components of its infrastructure. But “scaling our cloud capacity by 1,000X in advance just isn’t financially feasible,” he said.\nHis team considered optimizing on the code side, but the inherent complexity of their business model required focusing on the core infrastructure itself.\nAutomatically optimizing the entire Kubernetes infrastructure\nWhat Akamai really needed was a Kubernetes automation platform that could optimize the costs of running its entire\ncore infrastructure\nin real time on several clouds, Shavit explained, and scale applications up and down based on constantly changing demand. But all this had to be done without sacrificing application performance.\nBefore implementing Cast, Shavit noted that Akamai’s DevOps team manually tuned all its Kubernetes workloads just a few times a month. Given the scale and complexity of its infrastructure, it was challenging and costly. By only analyzing workloads sporadically, they clearly missed any real-time optimization potential.\n“Now, hundreds of Cast agents do the same tuning, except they do it every second of every day,” said Shavit.\nThe core APA features Akamai uses are autoscaling, in-depth Kubernetes automation with bin packing (minimizing the number of bins used), automatic selection of the most cost-efficient compute instances, workload rightsizing, Spot instance automation throughout the entire instance lifecycle and cost analytics capabilities.\n“We got insight into cost analytics two minutes into the integration, which is something we’d never seen before,” said Shavit. “Once active agents were deployed, the optimization kicked in automatically, and the savings started to come in.”\nSpot instances — where enterprises can access unused cloud capacity at discounted prices — obviously made business sense, but they turned out to be complicated due to Akamai’s complex workloads, particularly Apache Spark, Shavit noted. This meant they needed to either overengineer workloads or put more working hands on them, which turned out to be financially counterintuitive.\nWith Cast AI, they were able to use spot instances on Spark with “zero investment” from the engineering team or operations. The value of spot instances was “super clear”; they just needed to find the right tool to be able to use them. This was one of the reasons they moved forward with Cast, Shavit noted.\nWhile saving 2X or 3X on their cloud bill is great, Shavit pointed out that automation without manual intervention is “priceless.” It has resulted in “massive” time savings.\nBefore implementing Cast AI, his team was “constantly moving around knobs and switches” to ensure that their production environments and customers were up to par with the service they needed to invest in.\n“Hands down the biggest benefit has been the fact that we don’t need to manage our infrastructure anymore,” said Shavit. “The team of Cast’s agents is now doing this for us. That has freed our team up to focus on what matters most: Releasing features faster to our customers.”\nEditor’s note: At this month’s\nVB Transform\n, Google Cloud CTO Will Grannis and Highmark Health SVP and Chief Analytics Officer Richard Clarke will discuss the new AI stack in healthcare and the real-world challenges of deploying multi-model AI systems in a complex, regulated environment.\nRegister today\n.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T11:27:03.939066",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T11:27:14.725684",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/1c0e404d6d792fd9d9f32af848fa1f22.mp3",
    "audio_file": "audio/articles/1c0e404d6d792fd9d9f32af848fa1f22.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-17T11:27:55.682788",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "84934276e1065d1da16eddd502cc6bca",
    "title": "Inside LinkedIn’s AI overhaul: Job search powered by LLM distillation",
    "url": "https://venturebeat.com/ai/inside-linkedins-ai-overhaul-job-search-powered-by-llm-distillation/",
    "authors": "Emilia David",
    "published_date": "2025-06-16T22:52:31+00:00",
    "source": "VentureBeat",
    "summary": "LinkedIn進行AI改革，推出新的工作搜尋功能，使用LLM技術精煉模型，讓用戶可以用自然語言描述目標，獲得更符合需求的工作機會。這讓求職更直覺、包容且有力量。以前在LinkedIn搜尋工作時，常常因為關鍵字不夠準確而得到不符合的職缺，現在透過自然語言搜尋，可以更準確地找到適合自己的工作。",
    "content": "Inside LinkedIn’s AI overhaul: Job search powered by LLM distillation | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nInside LinkedIn’s AI overhaul: Job search powered by LLM distillation\nEmilia David\n@miyadavid\nJune 16, 2025 3:52 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat, generated with MidJourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nThe advent of natural language search has encouraged people to change how they search for information, and\nLinkedIn\n, which has been\nworking with numerous AI models\nover the past year, hopes this shift extends to job search.\nLinkedIn’s AI-powered jobs search, now available to all LinkedIn users, uses distilled, fine-tuned models trained on the professional social media platform’s knowledge base to narrow potential job opportunities based on natural language.\n“This new search experience lets members describe their goals in their own words and get results that truly reflect what they’re looking for,” said Erran Berger, vice president of product development at LinkedIn, told VentureBeat in an email. “This is the first step in a larger journey to make job-seeking more intuitive, inclusive, and empowering for everyone.”\nLinkedIn previously stated\nin a\nblog\npost\nthat a significant issue users faced when searching for jobs on the platform was an over-reliance on precise keyword queries. Often, users would type in a more generic job title and get positions that don’t exactly match. From personal experience, if I type in “reporter” on LinkedIn, I get search results for reporter jobs in media publications, along with court reporter openings, which are a totally different skill set.\nLinkedIn vice president for engineering Wenjing Zhang told VentureBeat in a separate interview that they saw the need to improve how people could find jobs that fit them perfectly, and that began with a better understanding of what they are looking for.\n“So in the past, when we’re using keywords, we’re essentially looking at a keyword and trying to find the exact match. And sometimes in the job description, the job description may say reporter, but they’re not really a reporter; we still retrieve that information, which is not ideal for the candidate,” Zhang said.\nLinkedIn has improved its understanding of user queries and now allows people to use more than just keywords. Instead of searching for “software engineer,” they can ask, “Find software engineering jobs in Silicon Valley that were posted recently.”\nHow they built it\nOne of the first things LinkedIn had to do was overhaul its search function’s ability to understand.\n“The first stage is when you’re typing a query, we need to be able to understand the query, then the next step is you need to retrieve the right kind of information from our job library. And then the last step is now that you have like couple of hundred final candidates, how do you do the ranking so that the most relevant job shows up at the top,” Zhang said.\nLinkedIn relied on fixed, taxonomy-based methods, ranking models, and older LLMs, which they said “lacked the capacity for deep semantic understanding.” The company then turned to more modern, already fine-tuned large language models (LLMs) to help enhance their platform’s natural language processing (NLP) capabilities.\nBut LLMs also come with expensive compute costs. So, LinkedIn turned to distillation methods to cut the cost of using expensive GPUs. They split the LLM into two steps: one to work on data and information retrieval and the other to rank the results. Using a teacher model to rank the query and job, LinkedIn said it was able to align both the retrieval and ranking models.\nThe method also allowed LinkedIn engineers to reduce the stages its job search system used. At one point, “there were nine different stages that made up the pipeline for searching and matching a job,” which were often duplicated.\n“To do this we use a common technique of multi-objective optimization. To ensure retrieval and ranking are aligned, it is important that retrieval ranks documents using the same MOO that the ranking stage uses. The goal is to keep retrieval simple, but without introducing unnecessary burden on AI developer productivity,” LinkedIn said.\nLinkedIn also developed a query engine that generates customized suggestions to users.\nA more AI-based search\nLinkedIn is not alone in seeing the potential for\nLLM-based enterprise search\n.\nGoogle\nclaims that\n2025 will be the year\nwhen enterprise search becomes more powerful, thanks to advanced models.\nModels like\nCohere\n’s Rerank 3.5 helps\nbreak language silos\nwithin enterprises. The various\n“Deep Research” products\nfrom\nOpenAI\n,\nGoogle\nand\nAnthropic\nindicate a growing organizational demand for agents that access and analyze internal data sources.\nLinkedIn has been rolling out several AI-based features in the past year.\nIn October, it launched an\nAI assistant to help recruiters\nfind the best candidates\n.\nLinkedIn Chief AI Officer Deepak Agarwal will discuss the company’s AI initiatives, including how it scaled its Hiring Assistant from prototype to production\n, during\nVB Transform in San\nFrancisco this month. Register now to attend\n.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T11:27:04.171566",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T11:27:16.566561",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/84934276e1065d1da16eddd502cc6bca.mp3",
    "audio_file": "audio/articles/84934276e1065d1da16eddd502cc6bca.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-17T11:28:01.142194",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "55cdb6a7c6dfbb246907b9a9404f5162",
    "title": "MiniMax-M1 is a new open source model with 1 MILLION TOKEN context and new, hyper efficient reinforcement learning",
    "url": "https://venturebeat.com/ai/minimax-m1-is-a-new-open-source-model-with-1-million-token-context-and-new-hyper-efficient-reinforcement-learning/",
    "authors": "Carl Franzen",
    "published_date": "2025-06-16T22:46:47+00:00",
    "source": "VentureBeat",
    "summary": "一家名為MiniMax的中國AI新創公司推出了全新的開源模型MiniMax-M1，具有100萬個TOKEN的上下文和高效的強化學習技術。這個模型在長篇內容推理任務中表現出色，並且完全開源，企業和開發者可以免費使用和修改。MiniMax-M1的特色是能處理高達100萬個輸入TOKEN和8萬個輸出TOKEN，是目前其中一個最強大的模型之一。這將為AI應用帶來更多可能性。",
    "content": "MiniMax-M1 is a new open source model with 1M TOKEN context | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nMiniMax-M1 is a new open source model with 1 MILLION TOKEN context and new, hyper efficient reinforcement learning\nCarl Franzen\n@carlfranzen\nJune 16, 2025 3:46 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nChinese AI startup MiniMax, perhaps best known in the West for its hit realistic\nAI video model Hailuo\n, has released its latest large language model,\nMiniMax-M1\n— and in great news for enterprises and developers, it’s completely\nopen source under an Apache 2.0 license\n, meaning businesses can take it and use it for commercial applications and modify it to their liking without restriction or payment.\nM1 is an open-weight offering that sets new standards in long-context reasoning, agentic tool use, and efficient compute performance. It’s available today on the AI code sharing community\nHugging Face\nand\nMicrosoft’s rival code sharing community GitHub\n, the first release of what the company dubbed as “MiniMaxWeek” from its social account on X — with further product announcements expected.\nMiniMax-M1 distinguishes itself with a context window of 1 million input tokens and up to 80,000 tokens in output, positioning it as one of the most expansive models available for long-context reasoning tasks.\nThe “context window” in large language models (LLMs) refers to the maximum number of tokens the model can process at one time — including both input and output. Tokens are the basic units of text, which may include entire words, parts of words, punctuation marks, or code symbols. These tokens are converted into numerical vectors that the model uses to represent and manipulate meaning through its parameters (weights and biases). They are, in essence, the LLM’s native language.\nFor comparison,\nOpenAI’s GPT-4o\nhas a context window of only 128,000 tokens — enough to exchange\nabout a novel’s worth of information\nbetween the user and the model in a single back and forth interaction. At 1 million tokens, MiniMax-M1 could exchange a small\ncollection\nor book series’ worth of information.\nGoogle Gemini 2.5 Pro offers a token context upper limit of 1 million\n, as well, with a reported 2 million window in the works.\nBut M1 has another trick up its sleeve: it’s been trained using reinforcement learning in an innovative, resourceful, highly efficient technique. The model is trained using a hybrid Mixture-of-Experts (MoE) architecture with a lightning attention mechanism designed to reduce inference costs.\nAccording to the technical report, MiniMax-M1 consumes only 25% of the floating point operations (FLOPs) required by\nDeepSeek R1\nat a generation length of 100,000 tokens.\nArchitecture and variants\nThe model comes in two variants—MiniMax-M1-40k and MiniMax-M1-80k—referring to their “thinking budgets” or output lengths.\nThe architecture is built on the company’s earlier MiniMax-Text-01 foundation and includes 456 billion parameters, with 45.9 billion activated per token.\nA standout feature of the release is the model’s training cost. MiniMax reports that the M1 model was trained using large-scale reinforcement learning (RL) at an efficiency rarely seen in this domain, with a total cost of $534,700.\nThis efficiency is credited to a custom RL algorithm called CISPO, which clips importance sampling weights rather than token updates, and to the hybrid attention design that helps streamline scaling.\nThat’s an astonishingly “cheap” amount for a frontier LLM, as DeepSeek trained its hit R1 reasoning model at a\nreported cost of $5-$6 million\n, while the training cost of OpenAIs’ GPT-4 — a more than two-year-old model now — was\nsaid to exceed $100 million\n. This cost comes from both the price of graphics processing units (GPUs), the massively parallel computing hardware primarily manufactured by companies like Nvidia, which can cost $20,000–$30,000 or more per module, and from the energy required to run those chips continuously in large-scale data centers.\nBenchmark performance\nMiniMax-M1 has been evaluated across a series of established benchmarks that test advanced reasoning, software engineering, and tool-use capabilities.\nOn AIME 2024, a mathematics competition benchmark, the M1-80k model scores 86.0% accuracy. It also delivers strong performance in coding and long-context tasks, achieving:\n65.0% on LiveCodeBench\n56.0% on SWE-bench Verified\n62.8% on TAU-bench\n73.4% on OpenAI MRCR (4-needle version)\nThese results place MiniMax-M1 ahead of other open-weight competitors such as DeepSeek-R1 and\nQwen3-235B-A22B\non several complex tasks.\nWhile closed-weight models like OpenAI’s o3 and Gemini 2.5 Pro still top some benchmarks, MiniMax-M1 narrows the performance gap considerably while remaining freely accessible under an Apache-2.0 license.\nDeployment options and developer tools\nFor deployment, MiniMax recommends vLLM as the serving backend, citing its optimization for large model workloads, memory efficiency, and batch request handling. The company also provides deployment options using the Transformers library.\nMiniMax-M1 includes structured function calling capabilities and is packaged with a chatbot API featuring online search, video and image generation, speech synthesis, and voice cloning tools. These features aim to support broader agentic behavior in real-world applications.\nImplications for technical decision-makers and enterprise buyers\nMiniMax-M1’s open access, long-context capabilities, and compute efficiency address several recurring challenges for technical professionals responsible for managing AI systems at scale.\nFor engineering leads responsible for the full lifecycle of LLMs — such as optimizing model performance and deploying under tight timelines — MiniMax-M1 offers a lower operational cost profile while supporting advanced reasoning tasks. Its long context window could significantly reduce preprocessing efforts for enterprise documents or log data that span tens or hundreds of thousands of tokens.\nFor those managing AI orchestration pipelines, the ability to fine-tune and deploy MiniMax-M1 using established tools like vLLM or Transformers supports easier integration into existing infrastructure. The hybrid-attention architecture may help simplify scaling strategies, and the model’s competitive performance on multi-step reasoning and software engineering benchmarks offers a high-capability base for internal copilots or agent-based systems.\nFrom a data platform perspective, teams responsible for maintaining efficient, scalable infrastructure can benefit from M1’s support for structured function calling and its compatibility with automated pipelines. Its open-source nature allows teams to tailor performance to their stack without vendor lock-in.\nSecurity leads may also find value in evaluating M1’s potential for secure, on-premises deployment of a high-capability model that doesn’t rely on transmitting sensitive data to third-party endpoints.\nTaken together, MiniMax-M1 presents a flexible option for organizations looking to experiment with or scale up advanced AI capabilities while managing costs, staying within operational limits, and avoiding proprietary constraints.\nThe release signals MiniMax’s continued focus on practical, scalable AI models. By combining open access with advanced architecture and compute efficiency, MiniMax-M1 may serve as a foundational model for developers building next-generation applications that require both reasoning depth and long-range input understanding.\nWe’ll be tracking MiniMax’s other releases throughout the week. Stay tuned!\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T11:27:04.415774",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T11:27:18.230899",
    "audio_path": "https://lqozyncypoyfxhyannqb.supabase.co/storage/v1/object/public/ai-news-storage/audio/articles/55cdb6a7c6dfbb246907b9a9404f5162.mp3",
    "audio_file": "audio/articles/55cdb6a7c6dfbb246907b9a9404f5162.mp3",
    "audio_generated": true,
    "audio_generated_date": "2025-06-17T11:28:10.838446",
    "audio_error": "[Errno 66] Directory not empty: '/Users/yuntao/Documents/AI_Developer/daily-ai-news-summarizer/temp_audio'"
  },
  {
    "id": "9cc2889043e5159afbcda68e75b8cc7b",
    "title": "Groq just made Hugging Face way faster — and it’s coming for AWS and Google",
    "url": "https://venturebeat.com/ai/groq-just-made-hugging-face-way-faster-and-its-coming-for-aws-and-google/",
    "authors": "Michael Nuñez",
    "published_date": "2025-06-16T21:49:42+00:00",
    "source": "VentureBeat",
    "summary": "AI新創公司Groq宣布支援Alibaba的Qwen3 32B語言模型，同時成為Hugging Face平台的推薦推理提供者，挑戰AWS和Google。Groq聲稱是唯一支援完整131,000字元上下文窗口的推理提供者，讓開發者能更有效率地建立大型應用程式。這項舉措將有助於Groq在AI推理市場搶占市佔率，提供更快速、高效的AI模型存取方式，讓開發者有更多選擇。",
    "content": "Groq just made Hugging Face way faster — and it’s coming for AWS and Google | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nGroq just made Hugging Face way faster — and it’s coming for AWS and Google\nMichael Nuñez\n@MichaelFNunez\nJune 16, 2025 2:49 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nGroq\n, the artificial intelligence inference startup, is making an aggressive play to challenge established cloud providers like\nAmazon Web Services\nand\nGoogle\nwith two major announcements that could reshape how developers access high-performance AI models.\nThe company announced Monday that it now supports\nAlibaba’s Qwen3 32B language model\nwith its full 131,000-token context window — a technical capability it claims no other fast inference provider can match. Simultaneously, Groq became an official inference provider on\nHugging Face’s platform\n, potentially exposing its technology to millions of developers worldwide.\nThe move is Groq’s boldest attempt yet to carve out market share in the rapidly expanding AI inference market, where companies like\nAWS Bedrock\n,\nGoogle Vertex AI\n, and\nMicrosoft Azure\nhave dominated by offering convenient access to leading language models.\n“The Hugging Face integration extends the Groq ecosystem providing developers choice and further reduces barriers to entry in adopting Groq’s fast and efficient AI inference,” a Groq spokesperson told VentureBeat. “Groq is the only inference provider to enable the full 131K context window, allowing developers to build applications at scale.”\nHow Groq’s 131k context window claims stack up against AI inference competitors\nGroq’s assertion about context windows — the amount of text an AI model can process at once — strikes at a core limitation that has plagued practical AI applications. Most inference providers struggle to maintain speed and cost-effectiveness when handling large context windows, which are essential for tasks like analyzing entire documents or maintaining long conversations.\nIndependent benchmarking firm\nArtificial Analysis\nmeasured Groq’s Qwen3 32B deployment running at approximately 535 tokens per second, a speed that would allow real-time processing of lengthy documents or complex reasoning tasks. The company is pricing the service at $0.29 per million input tokens and $0.59 per million output tokens — rates that undercut many established providers.\nGroq and Alibaba Cloud are the only providers supporting Qwen3 32B’s full 131,000-token context window, according to independent benchmarks from Artificial Analysis. Most competitors offer significantly smaller limits. (Credit: Groq)\n“Groq offers a fully integrated stack, delivering inference compute that is built for scale, which means we are able to continue to improve inference costs while also ensuring performance that developers need to build real AI solutions,” the spokesperson explained when asked about the economic viability of supporting massive context windows.\nThe technical advantage stems from Groq’s custom\nLanguage Processing Unit (LPU) architecture\n, designed specifically for AI inference rather than the general-purpose graphics processing units (GPUs) that most competitors rely on. This specialized hardware approach allows Groq to handle memory-intensive operations like large context windows more efficiently.\nWhy Groq’s Hugging Face integration could unlock millions of new AI developers\nThe\nintegration with Hugging Face\nrepresents perhaps the more significant long-term strategic move. Hugging Face has become the de facto platform for open-source AI development, hosting hundreds of thousands of models and serving millions of developers monthly. By becoming an official inference provider, Groq gains access to this vast developer ecosystem with streamlined billing and unified access.\nDevelopers can now select Groq as a provider directly within the\nHugging Face Playground\nor\nAPI\n, with usage billed to their Hugging Face accounts. The integration supports a range of popular models including Meta’s\nLlama series\n, Google’s\nGemma models\n, and the newly added\nQwen3 32B\n.\n“This collaboration between Hugging Face and Groq is a significant step forward in making high-performance AI inference more accessible and efficient,” according to a joint statement.\nThe partnership could dramatically increase Groq’s user base and transaction volume, but it also raises questions about the company’s ability to maintain performance at scale.\nCan Groq’s infrastructure compete with AWS Bedrock and Google Vertex AI at scale\nWhen pressed about infrastructure expansion plans to handle potentially significant new traffic from\nHugging Face\n, the Groq spokesperson revealed the company’s current global footprint: “At present, Groq’s global infrastructure includes data center locations throughout the US, Canada and the Middle East, which are serving over 20M tokens per second.”\nThe company plans continued international expansion, though specific details were not provided. This global scaling effort will be crucial as Groq faces increasing pressure from well-funded competitors with deeper infrastructure resources.\nAmazon’s\nBedrock service\n, for instance, leverages AWS’s massive global cloud infrastructure, while Google’s\nVertex AI\nbenefits from the search giant’s worldwide data center network. Microsoft’s\nAzure OpenAI service\nhas similarly deep infrastructure backing.\nHowever, Groq’s spokesperson expressed confidence in the company’s differentiated approach: “As an industry, we’re just starting to see the beginning of the real demand for inference compute. Even if Groq were to deploy double the planned amount of infrastructure this year, there still wouldn’t be enough capacity to meet the demand today.”\nHow aggressive AI inference pricing could impact Groq’s business model\nThe AI inference market has been characterized by aggressive pricing and razor-thin margins as providers compete for market share. Groq’s competitive pricing raises questions about long-term profitability, particularly given the capital-intensive nature of specialized hardware development and deployment.\n“As we see more and new AI solutions come to market and be adopted, inference demand will continue to grow at an exponential rate,” the spokesperson said when asked about the path to profitability. “Our ultimate goal is to scale to meet that demand, leveraging our infrastructure to drive the cost of inference compute as low as possible and enabling the future AI economy.”\nThis strategy — betting on massive volume growth to achieve profitability despite low margins — mirrors approaches taken by other infrastructure providers, though success is far from guaranteed.\nWhat enterprise AI adoption means for the $154 billion inference market\nThe announcements come as the AI inference market experiences explosive growth. Research firm Grand View Research estimates the global AI inference chip market will reach $154.9 billion by 2030, driven by increasing deployment of AI applications across industries.\nFor enterprise decision-makers, Groq’s moves represent both opportunity and risk. The company’s performance claims, if validated at scale, could significantly reduce costs for AI-heavy applications. However, relying on a smaller provider also introduces potential supply chain and continuity risks compared to established cloud giants.\nThe technical capability to handle full context windows could prove particularly valuable for enterprise applications involving document analysis, legal research, or complex reasoning tasks where maintaining context across lengthy interactions is crucial.\nGroq’s dual announcement represents a calculated gamble that specialized hardware and aggressive pricing can overcome the infrastructure advantages of tech giants. Whether this strategy succeeds will likely depend on the company’s ability to maintain performance advantages while scaling globally—a challenge that has proven difficult for many infrastructure startups.\nFor now, developers gain another high-performance option in an increasingly competitive market, while enterprises watch to see whether Groq’s technical promises translate into reliable, production-grade service at scale.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T08:51:05.205912",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T08:51:35.592893",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 258 credits are required for this request."
  },
  {
    "id": "a2335c125e6ed788ded9ee10f15f55b7",
    "title": "43% of games skipped GPU upgrade to pay rent instead | Liquid Web",
    "url": "https://venturebeat.com/games/43-of-games-skipped-gpu-upgrade-to-pay-rent-instead-liquid-web/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-16T21:35:00+00:00",
    "source": "VentureBeat",
    "summary": "有43%的玩家為了支付房租，而放棄了升級顯示卡，這是一個新報告指出的趨勢。許多玩家開始考慮使用雲端遊戲來應對昂貴的升級成本。研究還顯示，AI技術和雲端遊戲的興起正在改變玩家對升級需求的看法。此外，有36%的玩家已經使用類似GeForce Now或Xbox xCloud的服務。對於高端顯示卡的未來，也有一部分人認為它們將在三年內過時。",
    "content": "43% of games skipped GPU upgrade to pay rent instead | Liquid Web | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\n43% of games skipped GPU upgrade to pay rent instead | Liquid Web\nDean Takahashi\n@deantak\nJune 16, 2025 2:35 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nGPU trends: budget vs. beast\nImage Credit: Liquid Web\nAbout 43% of gamers skipped a graphics processing unit (GPU) upgrade just to pay rent, according to a new report by\nLiquid Web\n.\nBased on a survey of 1,000 PC gamers, Liquid Web came up with a variety of conclusions about what affects the buying decisions of gamers. And clearly they’re pretty price sensitive now. Liquid Web surveyed the PC gamers to uncover what drives today’s graphics card decisions.\nThe company noted that cloud gaming is a way for players to deal with the high costs of upgrading to a new GPU every few years. The survey found 62% of gamers would go all-in on cloud gaming if latency vanished. This represents a sober recalibration of value, performance, and permanence in the gaming\nworld, Liquid Web said. Keep in mind that Liquid Web is a provider of premium hosting and cloud\ninfrastructure solutions for businesses, developers, and digital creators.\nThe study explores how price, performance, and platform shifts like AI and cloud computing are affecting loyalty, value perception, and the future role of high-end GPUs.\nAI upscaling is one such shift. This machine learning technology enhances lower-resolution frames by predicting and generating higher-resolution detail in real time. Alongside cloud gaming, it’s changing how gamers evaluate upgrade needs.\nFrom budgeting limits to brand trust, our findings offer actionable insights into the priorities and pain points of modern PC gamers.\nAbout 36% of players already use services like GeForce Now or Xbox xCloud. One in five believe that high-end GPUs will be obsolete within three years.\nDeath of the upgrade cycle?\nLiquid Web looked at gamer trust and budgets.\nAbout 57% of players say they have been blocked from GPU purchases due to scalping or price hikes. And 43% have postponed or canceled upgrades due to rent or bills. Only 25% of gamers are willing to spend more than $500 on a GPU.\nInfluencers are the new benchmarks, Liquid Web said. About 35% say influencers or reviews swayed them away from specific GPU brands. And 36% have switched GPU brands during an upgrade, while 23% say business controversies made them walk away from certain manufacturers.\nLiquid Web said financial pivots and tech pivots create a lot of tension. The upgrade cycle is broken, as rent payments, cloud gaming and AI are making GPUs less relevant for Gen Z. There’s also the onset of graphics-light gaming platforms like Roblox, Minecraft and Fortnite. Those games have hundreds of millions of players, but they’re not the most demanding when it comes to graphics realism.\nRoughly one in three gamers said they ditched a GPU brand because of influencers. This is not an easy thing to do, considering there are only three major GPU brands: Nvidia, AMD and Intel. Influence can beat specs when it comes to trust from gamers.\nMethodology\nLiquid Web’s view of trusted sources for gamers.\nLiquid Web surveyed 1,000 U.S.-based PC gamers in May 2025 to understand their GPU habits, upgrade barriers, and openness to cloud-based alternatives. The average age of gamers is 34, with 51% male, 46% female and 3% non-binary or other.\nAbout 4% of those surveyed were boomers, 18% were Gen X, 56% were Millennials and 22% were Gen Z. The coverage included all 50 U.S. states and top 100 metro areas. The survey included attitudinal questions, cloud usage patterns, and brand sentiment ratings. Analysis was supported by Google Trends comparisons of 318 GPU-related search terms across U.S. regions and major cities. Respondents were screened for active gaming engagement and recent GPU consideration or ownership.\nOther findings\nGamer preferences for graphics cards.\nNearly 3 in 4 gamers (73%) would choose Nvidia if all GPU brands performed equally. Over one in four gamers (25%) say $500 is their maximum budget for a GPU today. About 42% would skip future GPU upgrades entirely if AI upscaling or cloud services met their performance needs.\nLiquid Web said performance, price, and brand loyalty drive decisions. Gamers aren’t just buying GPUs based on brand loyalty. They’re also weighing performance, price, and trust. Among respondents, 36% had switched brands during a GPU upgrade, with 78% citing performance and 69% citing price as the top reasons.\nA series of bar charts shows survey results on GPU preferences, factors in choosing the best GPUs, trusted sources, and upgrade frequency among PC gamers. Most preferred brand is Nvidia, most important factor is performance, most trusted resource is Reddit/online forums, and a vast majority of gamers keep a GPU 3-5 years before upgrading.\nNvidia remained the brand of choice for the majority, with 73% of gamers selecting it as their preferred GPU if all brands performed equally. This preference was especially strong among power users: 76% of ultrawide monitor users and 66% of 4K gamers relied on Nvidia.\nHowever, sentiment isn’t set in stone. Around a quarter (23%) said recent controversies or business decisions made them less likely to support a brand. Influencers also played a role, with 35% of gamers saying reviews or tech personalities have swayed them away from certain manufacturers.\nNearly one in 10 gamers (8%) reported regretting a GPU purchase. These feelings were most often due to issues like: Driver problems (especially for AMD users); overheating or loud cooling solutions; underwhelming performance for the price paid; incompatibility with VR or specific games; short-lived performance gains.\n“Gamers are pragmatic: value, reliability, and innovation matter more than brand loyalty. Brands that embrace these shifts will stand out in an increasingly discerning market,” said Brooke Oates, product manager at Liquid Web, in a statement.\nAs GPU prices continue to rise, budget constraints are pushing gamers toward secondhand markets and upgrade delays. Still, a GPU remains the most desirable upgrade for some.\nNearly one in four (23%) said it would be their top pick if they could invest in just one component right now.\nSurvey data statistics show the long-term trust in GPU brands (Nvidia comes in first), budget ranges (varies, with 27% of respondents choosing $500-699), and best value-for-money GPU models (varies between Nvidia models, with Nvidia RTX 4070 Ti coming in first).\nBuying behavior reflects gamers’ tight budgets. Nearly half (45%) have opted for used or older models, and 22% said they use drop trackers or alert services to monitor price changes.\nEven for gamers ready to upgrade, 39% plan to wait one to two years, while another 37% said they’d only replace it if their current GPU fails.\nOne in 7 gamers reported that GPU shortages have impacted their gaming or productivity in the past year. Gamers are divided on whether new features are worth it. About half believe GPUs have hit diminishing returns, while half do not. Topping the list of overrated features were 8K gaming (47%), ray tracing (17%), and frame generation (11%).\nCloud gaming is reshaping future demand\nLiquid Web looked at the power of graphics brands.\nGPU demand among gamers in the future may look very different as cloud services and AI technologies improve. Many PC gamers are already reconsidering the need for hardware upgrades.\nSurvey results on gamers’ attitudes toward cloud gaming, 42% would skip a GPU upgrade for cloud if it matched performance, and 62% would switch to cloud if latency was zero. Nvidia GeForce Now and Xbox Cloud Gaming are the most trusted cloud gaming services, with 20% believing cloud gaming will replace hardware GPUs in the future.\nMore than two in five PC gamers (42%) said they would skip a future GPU upgrade entirely if AI upscaling or cloud services met their needs. Nearly two-thirds (62%), including 62% of millennial and 61% of Gen Z gamers, would switch to cloud gaming full-time if latency were eliminated.\nYounger gamers showed the strongest inclination. Among Gen Z and millennials, 21% believed high-end GPUs would become less essential in the next three years. Over a third of respondents (36%) reported already using cloud gaming like GeForce Now or xCloud occasionally.\nIf these trends continue, cloud performance may become a decisive factor in the GPU market, altering not just how games are played but also whether expensive hardware remains relevant, Liquid Web said.\nEven for gamers ready to upgrade, 39% plan to wait 1–2 years, while another 37% said they’d only replace it if their current GPU fails.\n“With gamers increasingly open to cloud and AI-powered solutions, partnerships or innovation in these areas could position brands for the future,” Oats said\nEconomic constraints are pushing gamers to make more calculated choices. Tight budgets, secondhand markets, and skepticism toward features like 8K gaming signal a shift from prestige performance toward practical solutions.\nAt the same time, emerging technologies like AI upscaling and cloud gaming services are reshaping expectations. As hardware and software continue to evolve, the companies that win will be those that deliver value-first innovation, solving real pain points around access, longevity, and performance without breaking the bank.\n“GPU and gaming providers must rethink and address their value propositions. Accessible pricing, trade-in programs, or cloud gaming integrations could build long-term loyalty,” said Oates.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T08:51:05.291814",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T08:51:37.754205",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 233 credits are required for this request."
  },
  {
    "id": "2232086e1d5961bfe2038185567c3187",
    "title": "Take Us North Kickstarter campaign launches for game about crossing the U.S.-Mexico border",
    "url": "https://venturebeat.com/games/take-us-north-kickstarter-campaign-launches-for-game-about-crossing-the-u-s-mexico-border/",
    "authors": "Dean Takahashi",
    "published_date": "2025-06-16T20:30:00+00:00",
    "source": "VentureBeat",
    "summary": "一款名為「Take Us North」的遊戲在Kickstarter上推出，讓玩家體驗穿越美墨邊境的移民和尋求庇護者的旅程。遊戲旨在呈現人性故事，超越政治和標題，展現真實的穿越經歷。Anima Interactive這家遊戲公司希望透過遊戲，引發對移民議題的關注和同理心，並以敏感、尊重和真實的方式呈現。這個專案對Anima團隊成員來說是個深具個人意義的挑戰，他們希望透過遊戲促進更廣泛的文化對話。",
    "content": "Take Us North Kickstarter campaign launches for game about crossing the U.S.-Mexico border | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nTake Us North Kickstarter campaign launches for game about crossing the U.S.-Mexico border\nDean Takahashi\n@deantak\nJune 16, 2025 1:30 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nTake Us North has a Kickstarter campaign.\nImage Credit: Anima Interactive\nAnima Interactive has launched a\nKickstarter campaign\nfor its game, Take Us North, about the experiences of migrants and asylum seekers on crossing the U.S.-Mexico border.\nTake Us North is an adventure/survival game that follows the journeys of migrants and asylum seekers on their way to cross the US-Mexico border. It’s about the human story behind the politics and headlines, and stories about what the actual crossing is like. The game has raised $12,000 to date and is targeting hitting $30,000 in the next 26 days.\nAnima Interactive\nis a socially conscious indie games studio. With video games as the largest entertainment medium in the world, CEO Karla Reyes, a daughter of Guatemalan immigrants, believes in the immense untapped opportunity for interactive media to ignite positive cultural and social change.\nTake Us North was inspired by real-world stories and follows poignant cross-border journeys.\n“We recognize this is a heavy-hitting and emotionally-charged subject matter; however, we’re committed to humanizing the migrant and refugee experience and portraying it sensitively, respectfully, and authentically,” the company said in its campaign. “Our goal is to foster greater awareness and empathy around issues that are unfortunately often reduced in mainstream media to statistics or divisive rhetoric.”\nTake Us North devs interviewing migrants at Casa de la Misericordia in Nogales, Mexico.\nThis project is deeply personal to many of Anima’s team members, and the team is collaborating the portrayal is both respectful and authentic. Team members have interviewed migrants at Casa de la Misericordia in Nogales, Mexico, and creative director Reyes has retraced migrant trails in the Sonoran Desert.\n“With rapidly evolving policies, migration and immigration stand as some of the most urgent and critical issues of our time,” Anima Interactive said. “Our goal with Take Us North extends beyond game development – we strive to contribute to a broader cultural conversation and shift hearts and minds around this crucial subject matter.”\nThe company also hopes to galvanize social movements that directly support humanitarian and activist communities alongside the game. For example, it is collaborating closely with Casa de la Miserocordia y de Todas Las Naciones, a shelter for migrants and asylum seekers in Nogales, Mexico, and Salvavision, an organization based in Tucson, Arizona that provides aid to asylum seekers.\nA scene from the Take Us North game.\nTake Us North blends narrative depth, strategic traversal, resource management, and stealth mechanics to meaningfully connect players with the migrant experience.\nPlayers assume the role of a migrant guide whose objective is to safely shepherd a group of migrants and asylum seekers across the US-Mexico border.\nThe game is primarily set in the Sonoran Desert, one of the deadliest migrant trails in the world. Thousands of migrants have lost their lives making this journey, and Anima Interactive aims to honor their stories with dignity and compassion.\nPlayers must guide their fellow travelers through perilous terrain, circumventing obstacles like dangerous wildlife, managing limited food and water supplies, and evading Border Patrol. The stories balance tension with reprieve.\nThe migrant journey is a physically and emotionally arduous one, and the developers aim to reflect that in Take Us North. Moments of tension such as stealth gameplay will be balanced with moments of rest and reprieve. When players are able to rest, they can gather around a campfire with the group and exchange stories: nostalgia for what they’ve left behind, fears, hopes, and dreams.\nKarla Reyes retraces steps of migrants on the Sonoran trail and finds what they left behind.\nIn addition to the Sonoran desert, we are developing levels that represent other parts of the migrant trail, including La Bestia (The Beast), a perilous freight train that runs through Mexico that thousands of migrants ride on top of to travel across Mexico, and the Darien Gap, a jungle between Colombia and Panama.\nThe team is a collective of artists, writers, creative technologists, researchers, and activists dotted across the globe. Countries represented by project contributors include Argentina, Colombia, Guatemala, Honduras, Italy, Mexico, The Netherlands, The UK, and the U.S.\nTake Us North has garnered early-stage financial support from Microsoft Xbox, Cinereach, and Clever Endeavour Games.\nThe team’s efforts have been highlighted at the Games & SDGs Summit at the UN Headquarters and at GDC this year, and Take Us North was listed among the top indie games at GDC by GamesHub and NPR. The company has since showcased at other festivals around the world including London Games Festival, A MAZE, Gamescom LatAm, and Tribeca.\nA scene from Take Us North.\nThe company recognizes it has a responsibility that comes with telling these stories, and it doesn’t take that lightly. So it’s working closely with migration scholars, anthropologists, and people with lived experience to make sure the narrative is grounded, authentic, and respectful. It has support from partners at Xbox and Cinereach.\nThe development team behind Take Us North is dotted across the globe, representing over 15 countries.\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T08:51:05.396525",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T08:51:40.019670",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 290 credits are required for this request."
  },
  {
    "id": "15e0ba57666fff448ece7b9300acacaf",
    "title": "1Password and AWS join forces to secure AI, cloud environments for the enterprise",
    "url": "https://venturebeat.com/ai/1password-and-aws-join-forces-to-secure-ai-cloud-environments-for-the-enterprise/",
    "authors": "Michael Nuñez",
    "published_date": "2025-06-16T20:09:25+00:00",
    "source": "VentureBeat",
    "summary": "1Password和AWS攜手合作，針對企業的人工智慧和雲端環境提供安全保護。這個合作讓1Password得以應對企業對於安全工具的需求，尤其是針對人工智慧和雲端環境的需求。1Password已經成為一個企業安全平台，服務許多大型企業，而這次的合作更讓他們成為首個加拿大獨立軟體供應商與AWS達成戰略合作。這個合作讓企業能更有效地保護AI代理、未管理的設備和未經授權的應用程式。",
    "content": "1Password and AWS join forces to secure AI, cloud environments for the enterprise | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\n1Password and AWS join forces to secure AI, cloud environments for the enterprise\nMichael Nuñez\n@MichaelFNunez\nJune 16, 2025 1:09 PM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\n1Password\n, the Canadian password management company, announced Monday a strategic collaboration agreement with\nAmazon Web Services\nthat puts the firm in position to capitalize on surging enterprise demand for security tools designed for artificial intelligence and cloud-native environments.\nThe partnership is a major milestone for\n1Password\n, which has transformed from a consumer-focused password manager into an enterprise security platform serving one-third of Fortune 100 companies. The collaboration comes as organizations increasingly struggle to secure AI agents, unmanaged devices, and unauthorized applications that traditional security tools cannot monitor or control.\nMonica Jain, 1Password’s head of go-to-market partnerships, told VentureBeat in an exclusive interview that the AWS collaboration has delivered explosive growth over the past 18 months. Contracts sold through AWS average four times larger than typical deals, with win rates exceeding 50 percent across all customer segments from small businesses to large enterprises.\n“According to the AWS ISV partner team, in late 2024 they witnessed that 1Password had reached a level of progress that they have not seen in other ISVs,” Jain told VentureBeat. “Within seven months, most ISVs take about 24 to 36 months to get to the point that we got in a very short period of time.”\nThe strategic collaboration agreement, or SCA, makes\n1Password\nthe first Canadian independent software vendor to secure such a partnership with AWS. Amazon rarely enters these agreements, typically reserving them for companies creating new market categories that align with AWS’s security-focused strategy.\nShadow IT and unmanaged devices create massive security blind spots for enterprises\n1Password’s rapid growth stems from its approach to what the company calls the “\nAccess-Trust Gap\n” — the security risks created when employees use personal devices, unauthorized applications, and AI tools to access company data without IT oversight.\nTraditional security tools like identity and access management systems typically only govern applications that IT departments know about and have formally approved. However, research shows that only about\n50 percent of known applications\nare actually integrated with corporate security systems, while IT departments remain unaware of most applications employees actually use.\n“Today’s employees are working from anywhere, and a wide range of applications are being used, and a wide range of devices are being used to get work done,” Jain explained. “What happens often is that when people are using these applications or devices, they’re usually outside of the visibility of the IT organization.”\nThis creates cascading security risks. When an employee shares sensitive information through an unapproved file-sharing service and the recipient accesses it on a personal device without\nmulti-factor authentication\nor\nendpoint protection\n, the company’s data becomes vulnerable to multiple attack vectors simultaneously.\nJain illustrated the problem with a concrete example: “Imagine if I sent you sensitive information in a non-company approved file sharing tool, and you accessed it using your personal device. I have absolutely no idea whether the device you’re using has multi-factor authentication, has endpoint detection and response installed, or whether the file sharing platform is secure and not vulnerable to being breached.”\nAI automation tools lack basic security controls, exposing companies to new cyber threats\nThe partnership gains urgency as organizations rapidly deploy AI agents for business automation. Unlike human users, AI agents typically lack standard security measures like multi-factor authentication, instead relying on shared secrets or hard-coded credentials that create significant vulnerabilities.\n“Agentic AI is no longer a future concept,” Jain said. “Security risks are increasing and the use of AI is increasing within every organization, and it’s transforming how businesses operate.”\n1Password’s\nExtended Access Management\nplatform addresses this by treating AI agents with the same security rigor as human identities while maintaining the speed and automation that make AI valuable. The platform eliminates hard-coded secrets, enforces least-privilege access, and provides visibility into AI agent activity.\nThe approach differs from traditional security tools that focus on known, managed systems. Instead, 1Password’s platform secures what the company calls the “\nmanaged and unmanaged\n” devices, applications, and AI agents that legacy identity and access management tools cannot reach.\nNew AWS secrets manager integration streamlines cloud-native development workflows\nAs part of the expanded partnership, 1Password introduced Monday a new secrets syncing integration with\nAWS Secrets Manager\n, timed to coincide with\nAWS re:Inforce\n, Amazon’s premier security conference. The integration simplifies how development teams manage sensitive credentials in cloud-native environments.\nThe integration allows organizations to consolidate secrets management, enforce role-based access controls, and embed secure credential handling directly into development workflows including command-line interfaces, continuous integration pipelines, and AI-powered automation.\n“As a fast-moving agency, flexibility is everything—but not at the expense of security,” said Ivan Blagdan, chief technology officer at Convertiv, a 1Password customer. “1Password Extended Access Management gives us real-time assurance that every device accessing sensitive data—whether personal or company-issued—meets our standards around trust.”\nThe technical integration addresses a critical pain point for developers who traditionally have struggled to manage secrets securely without slowing down development velocity. By embedding secure access directly into existing workflows, the platform eliminates the trade-off between security and productivity that has plagued many organizations.\nPartnership strategy helps 1Password compete against Microsoft and Google’s bundled security tools\nThe\nAWS collaboration\npositions 1Password to compete more effectively against technology giants like\nMicrosoft\nand\nGoogle\n, which bundle identity management tools with their productivity suites. However, Jain emphasized that 1Password takes a partnership-first approach rather than directly competing.\n“We don’t compete, we really are a partner-centric organization that thinks about how do we get to the end customer the way that the customer needs to operate,” Jain said. “We integrate with the tools that Microsoft may be providing to customers. We work with partners like AWS that have huge leverage in the security space.”\nThis strategy appears to be working. 1Password now secures more than 165,000 businesses and millions of consumers, with 75 percent of revenue coming from business customers compared to just 25 percent from consumers — a dramatic shift from the company’s consumer origins.\nThe growth trajectory has been consistent across all market segments, suggesting that the security challenges 1Password addresses affect organizations regardless of size. This broad appeal has caught the attention of AWS, which views 1Password as creating an entirely new market category.\n“AWS doesn’t get into SCAs lightly,” Jain explained. “They don’t just sign it with any ISV that puts up their hand and says, ‘We’d like to enter into an SCA with you.’ They are very deliberate about who they invest in, and those investments are usually made for companies that are doing things that are different.”\nEnterprise security market evolution creates massive growth opportunity for access management platforms\nThe AWS partnership accelerates 1Password’s evolution from a password manager into what the company calls an\nExtended Access Management\nplatform. This new category addresses security gaps that traditional identity and access management, identity governance and administration, and mobile device management tools cannot reach.\nLeading companies including\nAsana\n,\nAssociated Press\n,\nCanva\n,\nIBM\n,\nMongoDB\n,\nOctopus Energy\n,\nSlack\n,\nSalesforce\n, and\nStripe\nrely on 1Password to secure managed and unmanaged devices, applications, and AI agents accessing sensitive corporate data.\nWhen asked about the biggest competitive threat to 1Password, Jain pointed inward rather than at external rivals: “Our biggest threat is ourselves and how quickly we can make sure our customers are secure, and the customers that we haven’t touched upon today are also secure. Our biggest threat is time and making sure that we can get to those customers in a timely way.”\nThe strategic collaboration agreement provides 1Password with access to AWS’s global scale, leadership programs, and co-selling initiatives to accelerate market expansion. Jain expects the partnership to multiply the company’s current revenue growth rate by five to seven times.\n“Whatever our run rate of revenue is today, we expect that run rate to be, if not five, six, seven times higher by working with AWS,” she said.\nFor AWS, the partnership strengthens its security portfolio as enterprises increasingly demand comprehensive access management for hybrid and AI-driven environments. The timing appears opportune as organizations embrace AI automation and remote work becomes permanent, positioning both companies to benefit from growing enterprise demand for comprehensive access security solutions.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T08:51:05.459666",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T08:51:42.336623",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 271 credits are required for this request."
  },
  {
    "id": "4586858ec5b3bd80ee50e258c77bee89",
    "title": "How can you make sure your brand shows up in LLM search? Adobe’s new LLM Optimizer seeks to provide the tools",
    "url": "https://venturebeat.com/ai/how-can-you-best-position-your-brand-for-discovery-in-llm-search-adobes-new-llm-optimizer-seeks-to-provide-the-tools/",
    "authors": "Carl Franzen",
    "published_date": "2025-06-16T17:14:26+00:00",
    "source": "VentureBeat",
    "summary": "Adobe推出新的LLM Optimizer工具，幫助企業在AI搜尋結果中提升品牌曝光度。隨著人工智慧驅動的對話界面越來越受歡迎，這個應用讓品牌能夠影響自己在數位空間中的呈現方式。透過Adobe Analytics數據顯示，AI帶來的流量在美國零售網站和旅遊網站中急速增長，顯示了這個工具的重要性。企業可以透過這個工具在快速變化的數位環境中脫穎而出，贏得更多商機。",
    "content": "Adobe's LLM Optimizer puts your brand in gen AI search results | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nSubscribe\nArtificial Intelligence\nView All\nAI, ML and Deep Learning\nAuto ML\nData Labelling\nSynthetic Data\nConversational AI\nNLP\nText-to-Speech\nSecurity\nView All\nData Security and Privacy\nNetwork Security and Privacy\nSoftware Security\nComputer Hardware Security\nCloud and Data Storage Security\nData Infrastructure\nView All\nData Science\nData Management\nData Storage and Cloud\nBig Data and Analytics\nData Networks\nAutomation\nView All\nIndustrial Automation\nBusiness Process Automation\nDevelopment Automation\nRobotic Process Automation\nTest Automation\nEnterprise Analytics\nView All\nBusiness Intelligence\nDisaster Recovery Business Continuity\nStatistical Analysis\nPredictive Analysis\nMore\nData Decision Makers\nVirtual Communication\nTeam Collaboration\nUCaaS\nVirtual Reality Collaboration\nVirtual Employee Experience\nProgramming & Development\nProduct Development\nApplication Development\nTest Management\nDevelopment Languages\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nHow can you make sure your brand shows up in LLM search? Adobe’s new LLM Optimizer seeks to provide the tools\nCarl Franzen\n@carlfranzen\nJune 16, 2025 10:14 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nCredit: VentureBeat made with Midjourney\nJoin the event trusted by enterprise leaders for nearly two decades. VB Transform brings together the people building real enterprise AI strategy.\nLearn more\nAt the Cannes Lions festival on June 16, 2025,\nAdobe introduced Adobe LLM Optimizer\n, a new enterprise-grade tool designed to help businesses improve their visibility in generative AI-powered environments.\nAs conversational interfaces like ChatGPT, Gemini, and Claude reshape how consumers search and engage online, Adobe’s new application aims to give brands the ability to understand and influence how they appear in these rapidly evolving digital spaces.\nBacked by data from Adobe Analytics showing a 3,500% increase in AI-sourced traffic to U.S. retail sites and a 3,200% spike to travel sites between July 2024 and May 2025, Adobe’s move comes at a time when the shift toward generative interfaces is accelerating. These tools are not only changing the mechanics of discovery—they are redefining what it means to be visible and influential online.\n“The adoption of GenAI-powered chat services is astounding, with massive year-over-year growth,” said Haresh Kumar, senior director of strategy and product marketing for Adobe Experience Manager. “It’s fundamentally changing how consumers interact, search, and find information.”\n“Generative AI interfaces are becoming go-to tools for how customers discover, engage and make purchase decisions,” added Loni Stark, vice president of strategy and product for Adobe Experience Cloud. “With Adobe LLM Optimizer, we are enabling brands to confidently navigate this new landscape, ensuring they stand out and win in the moments that matter.”\nGEO is the new SEO\nHaresh Kumar described the new digital reality as one in which brands no longer just optimize for search engines—but for AI models.\n“SEO is no longer just about keywords and backlinks,” he said. “In the era of generative AI, we’re entering a new paradigm—Generation Engine Optimization or GEO—where relevance is judged differently.”\nThis evolving landscape demands new methods for tracking performance and influencing discoverability. Adobe LLM Optimizer aims to address this with a three-pronged framework:\nAuto Identify:\nThe system detects how a brand’s content is being used by major AI models. Adobe tracks the “fingerprints” of indexed content and determines whether—and how—it appears in responses to relevant queries.\nAuto Suggest:\nDrawing on Adobe’s own AI models trained for generative interfaces, the tool recommends improvements across technical infrastructure and content. These could range from fixing metadata errors to improving authority and context in FAQ content.\nAuto Optimize:\nFor many brands, the challenge isn’t just knowing what to fix—it’s executing the fixes quickly. LLM Optimizer allows users to apply recommended changes directly, often without heavy involvement from development teams. “We help brands auto-identify how their content is performing in LLMs, auto-suggest improvements, and auto-optimize to actually implement those changes,” said Kumar.\nRevealing gaps in your brand’s visibility to LLM users and assisting with filling them\nAdobe’s system enables marketers to see where their brand is underrepresented in AI-driven results. “The goal is to help brands understand the gaps—where they’re not showing up in AI answers—and what fixes can make them more visible,” said Kumar. The application calculates projected traffic value for each suggested change, letting teams prioritize high-impact actions.\n“Brands often ask, ‘Do I need to care about this new AI box?’” Kumar added. “The answer is yes—because traffic is shifting there. If you’re not optimizing for it, you’re missing out.”\nOne example of content optimization includes focusing on formats that LLMs naturally prefer.\n“FAQ pages tend to perform exceptionally well in LLM indexing,” said Kumar. “They provide direct, authoritative answers that LLMs prefer when generating responses.”\nAdobe’s platform not only recommends creating such content but also assists in generating it within a brand’s existing voice and structure, thanks to native integration with Adobe Experience Manager.\nAlways on analysis and expanding coverage for the growing library of LLMs\nLLM Optimizer uses a combination of push and pull models to keep content indexing current. When new content is published or accessed by an AI model, the system updates its analysis and surfaces insights to the user.\n“Our infrastructure includes both push and pull models. Whenever content is updated or accessed, we capture that fingerprint and feed it into our analysis engine,” Kumar explained.\nCurrently, the product tracks performance across several top AI models, including ChatGPT, Claude, and Gemini, with plans to expand coverage as new models emerge.\nAvailability and integration\nAdobe LLM Optimizer is available now as a standalone product or as a native integration with Adobe Experience Manager Sites. While pricing is not publicly disclosed, Adobe confirmed it is a separate product requiring opt-in and agreement updates.\n“LLM Optimizer is a new product offering, fully integrated with Adobe Experience Manager but available as a standalone solution,” said Kumar. “Customers need to opt in based on their AI readiness and strategy.”\nWith more consumers spending time inside AI-driven interfaces, Adobe positions LLM Optimizer as a forward-looking solution for enterprises navigating this new terrain. It offers a blend of visibility, automation, and strategic clarity as digital engagement moves beyond traditional search engines into the generative future.\nDaily insights on business use cases with VB Daily\nIf you want to impress your boss, VB Daily has you covered. We give you the inside scoop on what companies are doing with generative AI, from regulatory shifts to practical deployments, so you can share insights for maximum ROI.\nSubscribe Now\nRead our\nPrivacy Policy\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.\nWhere Enterprise AI Gets Real — Join Us at VB Transform 2025\nJoin top leaders in San Francisco from June 24–25 to solve real challenges, share proven strategies, and shape the future of AI at VB Transform 2025.\nLearn More\nVentureBeat Homepage\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe AI insights you need to lead\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T08:51:05.600428",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T08:51:44.058193",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 292 credits are required for this request."
  },
  {
    "id": "c1fda661d71227b6202bcc6e09b9cf50",
    "title": "Bigger Games raises $25M to grow its mobile gaming business",
    "url": "https://venturebeat.com/games/bigger-games-raises-25m-to-grow-its-mobile-gaming-business/",
    "authors": "Rachel Kaser",
    "published_date": "2025-06-16T16:54:26+00:00",
    "source": "VentureBeat",
    "summary": "一家名為Bigger Games的土耳其手機遊戲公司籌集了2500萬美元的資金，用來擴大他們的手機遊戲業務。這筆資金將用於開發新遊戲，其中包括他們最新的作品《廚師大師》。Bigger Games的CEO表示，他們致力於打造讓玩家情感共鳴的遊戲體驗，並計劃將《廚師大師》推向全球市場。這家公司專注於休閒益智遊戲，並希望透過這筆資金在全球市場上取得更大的成功。",
    "content": "Bigger Games raises $25M to grow its mobile gaming business | VentureBeat\nSkip to main content\nEvents\nVideo\nSpecial Issues\nJobs\nSubscribe\nVentureBeat Homepage\nGame Development\nView All\nProgramming\nOS and Hosting Platforms\nMetaverse\nView All\nVirtual Environments and Technologies\nVR Headsets and Gadgets\nVirtual Reality Games\nGaming Hardware\nView All\nChipsets & Processing Units\nHeadsets & Controllers\nGaming PCs and Displays\nConsoles\nGaming Business\nView All\nGame Publishing\nGame Monetization\nMergers and Acquisitions\nGames Releases and Special Events\nGaming Workplace\nLatest Games & Reviews\nView All\nPC/Console Games\nMobile Games\nGaming Events\nGame Culture\nSubscribe\nEvents\nVideo\nSpecial Issues\nJobs\nVentureBeat Homepage\nBigger Games raises $25M to grow its mobile gaming business\nRachel Kaser\n@rachelkaser\nJune 16, 2025 9:54 AM\nShare on Facebook\nShare on X\nShare on LinkedIn\nImage Credit: Bigger Games\nTurkish mobile games studio\nBigger Games\nannounced today that it has raised $25 million in its Series A funding round, which it will put towards growing its portfolio of games. Goodwater Capital led the round, with Arcadia Gaming Partners and existing investors Index Ventures and Play Ventures also participating. Coddy Johnson, former Activision president and current partner at Goodwater Capital, joins Bigger’s board of directors. The studio plans to use the funding to continue building its team and scaling its latest title, Kitchen Masters, for a global audience.\nThe Istanbul-based team launched in 2019 into the burgeoning Turkish gaming scene, and has specialized in casual puzzle games. Its most recent title, Kitchen Masters, is a match-3 title which Bigger reports shows strong early retention and engagement. With the Series A funding, Bigger plans to launch Kitchen Masters to other markets, including the U.S., alongside further refining the gameplay.\nGamesBeat spoke with Hakan Ulvan, Bigger Games CEO and co-founder, about the fundraise, and the studio’s vision for the future. He said, “Our belief is simple but bold: fun is a feeling, not a feature. Bigger Games exists to create emotionally impactful experiences: games where every swipe, tap, and decision feels meaningful. We obsess over emotional flow, the small bursts of mastery, surprise, and joy that keep players engaged. Our mission is to redefine fun for the mobile generation, and we do that by crafting intuitive games where every action matters and every moment delights.”\nBigger Games and its mobile market potential\nHakan said of the funding: “We weren’t just looking for capital, we wanted people who believe in the emotional power of games and the potential of Turkey as a global creative hub… Welcoming Arcadia’s founder, Akın Babayiğit, who also co-founded Tripledot Studios, which is now a top five independent mobile games studio by revenue globally after their recent acquisition of Applovin’s game studios portfolio, as an investor was a natural next step, as Akın has supported us since the very beginning, and we’re thrilled to make that relationship official now.”\nBabayiğit said in a statement, “Hakan and the Bigger team embody all the qualities that make the Turkish games ecosystem what it is today: Scary smart, super hungry, unwilling to settle or compromise, and incredibly creative and analytical. I have had the pleasure of being on this journey with Hakan from the beginning, and watching him evolve into the truly exceptional founder (and person) he is today. The sky is the limit for the whole team, and I can’t wait to see what they will achieve.\nRegarding the future, Hakan emphasized that Kitchen Masters is the studio’s main priority at the moment, alongside growing its marketing team over the next 12-24 months. “We’re also really excited about what this investment unlocks: the ability to test and learn what brings joy to players across different countries. As data nerds at heart, scaling our user acquisition efforts and learning from player behavior in more markets is something we truly thrive on. With the right product, the right people, and the right focus, we’re working to turn Kitchen Masters into not just a successful game, but a game that brings the joy at the core of playing.”\nJoin the GamesBeat community!\nEnjoy access to special events, private newsletters and more.\nJoin here\nGames\nBeat\nFollow us on Facebook\nFollow us on X\nFollow us on LinkedIn\nFollow us on RSS\nPress Releases\nContact Us\nAdvertise\nShare a News Tip\nContribute to DataDecisionMakers\nPrivacy Policy\nTerms of Service\nDo Not Sell My Personal Information\n© 2025\nVentureBeat\n. All rights reserved.\n×\nThe insights you need without the noise\nSubmit\nThanks for subscribing. Check out more\nVB newsletters here\n.\nAn error occured.",
    "content_type": "news",
    "processed": true,
    "fetch_date": "2025-06-17T08:51:05.656303",
    "summary_model": "gpt-3.5-turbo",
    "processed_date": "2025-06-17T08:51:46.098398",
    "audio_error": "This request exceeds your quota of 30078. You have 91 credits remaining, while 238 credits are required for this request."
  }
]